# Custom-Edit: Text-Guided Image Editing with Customized Diffusion Models

## 摘要
文本到图像的扩散模型可以根据用户提供的文本提示生成多样化、高保真的图像。最近的研究扩展了这些模型，支持基于文本的图像编辑。尽管文本引导为用户提供了一种直观的编辑界面，但它往往无法确保准确传达用户的概念。为了解决这一问题，我们提出了Custom-Edit方法，**其中我们（i）使用少量参考图像自定义扩散模型，然后（ii）执行文本引导的编辑。**我们的关键发现是，仅自定义与语言相关的参数并使用增强的提示，可以显著提高参考图像的相似度，同时保持源图像的相似度。此外，我们提供了每个自定义和编辑过程的详细方法。我们比较了流行的自定义方法，并在使用各种数据集的两个编辑方法上验证了我们的发现。

## 1. 引言
最近在深度生成模型方面的研究推动了图像编辑的快速进展。经过大规模数据库[23]训练的文本到图像模型[19, 22]允许对各种领域的图像进行直观的编辑[7, 15]。那么，这些模型能够在多大程度上支持精确的编辑指令？用户的独特概念，尤其是那些在大规模训练中未遇到过的概念，能否被用于编辑？如图1所示，使用从表现良好的标题生成模型[13]获取的提示进行编辑无法捕捉参考图像的外观。

我们提出了Custom-Edit，一种包含两步的方法：（i）使用少量参考图像来自定义模型[6, 12, 21]，然后（ii）利用有效的文本引导编辑方法[7, 15, 16]对图像进行编辑。虽然先前的自定义研究[6, 12, 21]主要处理随机生成图像（噪声→图像），我们的工作则专注于图像编辑（图像→图像）。如图1所示，自定义可以大幅提高参考图像外观的忠实度。本文表明，仅自定义与语言相关的参数并使用增强的提示，可以显著提升编辑图像的质量。此外，我们展示了每个自定义和编辑过程中的设计选择，并讨论了Custom-Edit中的源图像与参考图像之间的权衡。

## 2. 扩散模型
在本文中，我们使用了Stable Diffusion[19]，这是一种开源的文本到图像模型。扩散模型[5, 8, 24, 26]在变分自编码器（VAE）[11]的潜在空间中进行训练，以通过降采样图像来提高计算效率。该模型的训练目标是通过给定的文本条件$c$，从扰动后的潜在表示$ x_t $重建干净的潜在表示$x_0$，其中文本条件由CLIP文本编码器[18]进行嵌入。扩散模型通过以下目标函数进行训练：

$$
\sum_{t=1}^{T} \mathbb{E}_{x_0, \epsilon} [|| \epsilon - \epsilon_\theta(x_t, t, c) ||^2],(1)
$$

其中$\epsilon$为添加的噪声，$t$表示时间步长，指示扰动噪声级别，$\epsilon_\theta$为扩散模型，具有包含注意力块[27]的U-Net[20]架构。在训练过程中，文本嵌入被投影到交叉注意力层的键和值上，且文本编码器保持冻结状态，以保留其语言理解能力。Imagen[22]和eDiffi[1]的研究表明，冻结大型语言模型的丰富语言理解能力是提升性能的关键。

## 3. Custom-Edit
我们的目标是根据参考图像中的复杂视觉指令对图像进行编辑（见图1）。因此，我们提出了一个两步的方法：（i）在给定的参考图像上自定义模型（第3.1节），以及（ii）使用文本提示对图像进行编辑（第3.2节）。我们的方法如图2所示。

### 3.1 自定义
**可训练参数。** 我们仅优化交叉注意力的键和值以及‘[rare token]’，遵循Custom-Diffusion[12]的做法。正如我们在第4节中讨论的那样，我们的结果表明，**训练这些与语言相关的参数对于成功将参考概念迁移到源图像中至关重要**。此外，仅训练这些参数比Dreambooth[21]所需的存储空间更小。

**增强提示。** 我们通过最小化公式（1）来微调上述参数。我们通过增强文本输入$'[rare token][modifier][class noun]'$（例如，‘V*图案茶壶’）来改进Custom-Diffusion用于编辑。我们发现‘[modifier]’可以鼓励模型专注于学习参考的外观。

**数据集。** 为了在微调参考图像时保持语言理解能力，我们还通过对属于同一类别的多样化图像最小化先验保持损失[21]来实现这一目标。因此，我们使用CLIP检索[3]从LAION数据集[23]中检索200张图像及其标题，使用的文本查询为‘photo of a [修饰符] [类别名词]’。

### 3.2 基于文本的图像编辑
**Prompt-to-Prompt。** 我们使用Prompt-to-Prompt [7] (P2P)，这是最近引入的一种编辑框架，通过仅修改源提示对图像进行编辑。P2P提出了注意力注入，以保留源图像的结构。对于每个去噪步骤$t$，我们将源图像和编辑图像的注意力图分别表示为$M_t$和$M_t^*$。然后，P2P将新的注意力图Edit($M_t$, $M_t^*$, $t$)注入到模型$\epsilon_\theta$中。Edit是一个注意力图编辑操作，包括提示优化和词语替换。此外，P2P通过自动计算的掩码实现局部编辑。P2P计算与词语$w$相关的交叉注意力平均值$\overline{M}_{t,w}$和$\overline{M}_{t,w}^*$，并将其进行阈值处理以生成二值掩码$B(\overline{M}_t) \cup B(\overline{M}_{t}^*)$。在使用P2P进行编辑之前，我们利用Null-Text Inversion[16]来增强源图像的保留。详细信息请参见附录C。

**操作选择。** 由于参考图像数量有限，自定义的词语只支持有限种类的结构。这个限制启发我们提出了以下方案。首先，我们使用提示优化作为Edit函数。词语替换在自定义词语不适合交换注意力图时可能会失败。其次，我们使用掩码$B(\overline{M}_t)$而不是$B(\overline{M}_t) \cup B(\overline{M}_{t}^*)$，因为自定义词语可能生成错误的掩码。

**源图像与参考图像的权衡。** 图像编辑中的一个关键挑战是平衡编辑后图像的源相似性和参考相似性。我们用$\tau/T$表示强度，其中P2P从$t = T$到$t = \tau$注入自注意力。在P2P中，我们观察到控制这种权衡的关键因素是注入自注意力而非交叉注意力。较高的强度意味着源相似性更高，但参考相似性降低。在第4节中，我们还展示了使用SDEdit[15]的结果，该方法将图像从$t = 0$扩散到$t = \tau$，然后再去噪。与P2P不同，在SDEdit中，较高的强度意味着较高的参考相似性。

## 4. 实验
在本节中，我们旨在验证Custom-Edit的每个过程。具体来说，我们通过在自定义过程中使用Textual Inversion[6]和Dreambooth[21]来评估我们的设计选择，并比较它们在编辑过程中的源图像与参考图像之间的权衡。除了P2P，我们还使用SDEdit[15]进行实验。

**基线。** Textual Inversion通过最小化公式(1)，使用类名（例如“pot”）初始化一个新的文本嵌入$V^*$，以输入提示“V^*”进行学习。Dreambooth在冻结文本编码器的同时对扩散模型进行微调。对于输入提示“[稀有标记] [类名]”（例如“ktn teapot”），通过最小化公式(1)在给定的几张图像上进行微调。SDEdit是最简单的编辑方法，通过扩散和去噪图像来实现。

**数据集。** 我们在实验中使用了八个参考，包括两个宠物、五个物体和一件艺术品。对于每个参考，我们平均使用了五张源图像。

**度量。** 我们使用CLIP ViT-B/32[18]来衡量源图像和参考图像的相似性。我们使用的强度为P2P的[0.2, 0.4, 0.6, 0.8]和SDEdit的[0.5, 0.6, 0.7, 0.8]。对于每种强度和源图像，我们从不同的随机种子生成了两张P2P样本和三张SDEdit样本。

**推理细节。** 我们使用7.5的引导尺度和50步推理。我们使用BLIP2[13]获取所有源提示。更多细节见第B节。

### 4.1 定性结果
图3展示了选定的结果。Custom-Edit将参考的细节外观传递给源图像，同时保留整体结构。例如，Custom-Edit生成了从葡萄酒瓶子变形而来的水平延展的$V^*$木制壶（第一行）。在第二行中，Custom-Edit生成了带有龟壳纹理的$V^*$海龟毛绒玩具，戴着帽子。第三行的蓝松鸦变成了一个$V^*$陶瓷鸟，完美保留了马卡龙。在最后一行，$V^*$猫以参考集中不存在的姿势坐着。定性比较见第A.1节。

### 4.2 定量结果
图4显示了P2P和SDEdit的平均权衡曲线。我们改进的Custom-Diffusion实现了最佳权衡，而Textual Inversion虽然具有类似的源图像相似性，但参考相似性较低。Dreambooth的源图像相似性较高，但参考相似性较低，表明其在修改图像方面效果不佳。SDEdit的结果也显示了类似的趋势，支持了我们关于自定义与语言相关参数对于编辑的有效性的观点。值得注意的是，SDEdit显示的源图像相似性低于P2P，表明P2P及其操作选择在基于文本的编辑中更具优势。

## 5. 讨论
我们提出了Custom-Edit，这允许使用文本提示进行细粒度的编辑。我们展示了每个过程的设计选择，可以为未来的自定义和编辑工作提供帮助。此外，我们讨论了基于扩散的编辑中源图像与参考图像之间的权衡。

尽管Custom-Edit展示了各种成功的编辑结果，但仍存在一些失败案例，如第A.3节所述。Custom-Edit有时会编辑到不需要的区域，或者无法编辑复杂的背景。我们推测这可能是由于Stable Diffusion[7, 16]中不准确的注意力图和文本输入的可控性有限。潜在的解决方案是将Custom-Edit应用于具有更大文本编码器[1, 22]或扩展可控性[14, 28]的文本到图像模型。