# An Edit Friendly DDPM Noise Space: Inversion and Manipulations

## 摘要

去噪扩散概率模型（DDPMs）通过一系列白噪声样本生成图像。与GANs类似，这些噪声图可以视为与生成图像相关的潜在代码。然而，这一原生噪声空间没有便捷的结构，因此在编辑任务中难以处理。在此，**我们提出了一种替代的DDPM潜在噪声空间**，能够通过简单的方式实现广泛的编辑操作，并提出了一种反演方法来提取适合编辑的噪声图（无论是真实图像还是合成生成的图像）。与原生DDPM噪声空间不同，编辑友好的噪声图不遵循标准正态分布，并且在时间步长上不具有统计独立性。然而，它们能够完美重构任何所需的图像，并且对它们的简单变换可以转换为输出图像的有意义的操作（例如，移动、颜色编辑）。此外，在文本条件模型中，固定这些噪声图并改变文本提示可以在保持结构的同时修改语义。我们展示了这种特性如何通过多样化的DDPM采样方案实现基于文本的真实图像编辑（与流行的非多样化DDIM反演形成对比）。我们还展示了它如何能与现有的基于扩散的编辑方法结合使用，以提高它们的质量和多样性。代码和示例可在项目网页上获得。

## 引言

扩散模型已成为一种强大的生成框架，在图像合成领域达到了最先进的质量水平【2, 11, 21, 23, 24, 26】。最近的工作利用扩散模型完成了各种图像编辑和操控任务，包括文本引导编辑【1, 5, 9, 14, 28】、图像修复【17】和图像到图像转换【18, 25, 31】。这些方法的关键挑战之一是如何将它们应用于真实内容的编辑（与模型生成的图像相对）。这需要反转生成过程，即提取一系列噪声向量，使用这些向量可以通过反向扩散过程重构给定的图像。

尽管扩散模型的编辑功能取得了显著进展，反演仍被认为是一个主要挑战，特别是在去噪扩散概率模型（DDPM）的采样方案中【11】。许多最近的方法（例如【5, 9, 20, 22, 28, 30】）依赖于去噪扩散隐模型（DDIM）方案【27】的近似反演方法，DDIM是一种将单一初始噪声向量映射到生成图像的确定性采样过程。然而，该DDIM反演方法仅在使用大量扩散时间步时（例如1000步）才会精确，即使在这种情况下，文本引导编辑中的结果仍然常常不理想【9, 20】。为了应对这一问题，一些方法对扩散模型进行微调，基于给定的图像和文本提示进行调整【13, 14, 29, 33】。其他方法则通过不同方式介入生成过程，例如将从DDIM反演过程中得出的注意力图注入到文本引导的生成过程中【4, 9, 22, 28】。

在本文中，我们针对DDPM方案的反演问题提出了新方法。与DDIM不同，DDPM的生成过程涉及$T+1$个噪声图，每个噪声图与生成的输出图像维度相同。因此，噪声空间的总维度大于输出维度，存在无限多的噪声序列可以完美重构图像。尽管这一特性可能为反演过程提供灵活性，但并非所有一致的反演（即能导致完美重构的反演）都适合编辑应用。我们希望在文本条件模型的上下文中，固定噪声图并更改文本提示时，生成的图像没有伪影，并且语义对应于新文本，但结构与输入图像相似。什么样的一致反演满足这一特性？一个诱人的答案是噪声图应该是统计独立的，并具有标准正态分布，就像在常规采样中那样。这种方法在【31】中得到了应用。然而，正如我们在图2中展示的那样，原生的DDPM噪声空间实际上并不适合编辑。

在此，我们提出了一种替代的反演方法，它更适合用于编辑应用，从文本引导的操控到通过手绘颜色笔触的编辑操作。我们的方法将图像更强烈地“印刻”到噪声图上，这在固定噪声图并改变模型条件时有助于更好地保留图像结构。我们的方法通过使噪声图的方差高于原生噪声图来实现这一点。我们的反演不需要优化，且速度极快。然而，通过固定噪声图并更改文本条件（即不需要模型微调或在注意力图中进行干预），我们可以在相对较少的扩散步数内实现最先进的文本引导编辑效果。更重要的是，我们的DDPM反演也可以很容易地与现有依赖于DDIM近似反演的扩散编辑方法相结合。正如我们在图1中所展示的那样，这提高了它们对原始图像的保真度。此外，由于我们以随机方式寻找噪声向量，因此可以提供一组多样化的符合文本提示的编辑图像，这一特性在DDIM反演中是天然不具备的。

## 相关工作

### 2.1 扩散模型的反演

使用扩散模型编辑真实图像需要提取噪声向量，这些噪声向量在生成过程中可以用于生成该图像。绝大多数基于扩散的编辑工作使用了DDIM方案，这是一种将单一噪声图确定性地映射到生成图像的采样方法【5, 7, 9, 20, 22, 28, 30】。原始的DDIM论文【27】提出了一种该方案的高效近似反演方法。该方法在每个扩散时间步都会产生一个小误差，而当使用无分类器引导时，这些误差往往会积累成显著的偏差【10】。Mokady等人【20】通过固定每个时间步的漂移来提高重构质量。他们的两步方法首先使用DDIM反演计算一系列噪声向量，然后使用该序列优化每个时间步的输入空文本嵌入。Miyake等人【19】通过前向传播在没有优化的情况下实现了类似的重构精度，从而加速了编辑过程。Han等人【8】提出在空文本嵌入优化中加入正则化项以提高重构质量。EDICT【30】通过保持两个耦合的噪声向量，交替反演彼此，从而实现了真实图像的数学精确DDIM反演。该方法将扩散过程的计算时间加倍。CycleDiffusion【31】提出了一种DDPM反演方法，通过恢复一系列噪声向量，能够在DDPM采样过程中完美重构图像。然而，与我们的方法不同的是，他们提取的噪声图与DDPM的原生噪声空间的分布相同，导致编辑能力有限（见图2和图4）。

### 2.2 使用扩散模型进行图像编辑

DDPM采样方法并不常用于编辑真实图像。即便使用，通常也不会进行精确反演。例如，Ho等人【11】通过插值真实图像，Meng等人【18】通过用户草图或笔画对真实图像进行编辑（SDEdit）。两者都构建了一个噪声版本的真实图像，然后在编辑后应用反向扩散。然而，它们在生成图像的真实感与其对原始内容的忠实度之间存在固有的权衡。DiffuseIT【15】在没有明确反演的情况下执行基于参考图像或文本的图像转换。他们通过测量与原始图像的相似性损失来引导生成过程【6】。

一系列论文通过DDIM反演应用了基于文本驱动的图像到图像转换。Narek等人【28】和Cao等人【4】通过在扩散过程中操控空间特征及其自注意力实现了这一目标。Hertz等人【9】根据目标文本提示修改了原始图像的注意力图，并将其注入扩散过程。DiffEdit【5】基于源文本和目标文本提示自动生成了需要编辑的图像区域的掩码。此方法用于确保未编辑区域对原始图像的忠实度，以应对反演带来的重构质量较差问题。然而，当面对复杂提示时，该方法无法预测准确的掩码。

一些方法利用基于目标文本提示的模型优化。DiffusionCLIP【14】基于CLIP损失对模型进行微调，以实现目标文本。Imagic【13】首先优化目标文本嵌入，然后优化模型，以使用优化后的文本嵌入重构图像。UniTune【29】同样使用微调，在保持图像结构的同时，在全局风格变化和复杂的局部编辑中表现出色。

## 3. DDPM噪声空间

在本节中，我们将重点讨论适用于像素空间【11】和潜在空间【24】的DDPM采样方案。DDPM通过尝试反转扩散过程进行采样，该过程逐渐将干净图像$x_0$变为白噪声：

$$
x_t = \sqrt{1 - \beta_t} x_{t-1} + \sqrt{\beta_t} n_t, \quad t = 1, \dots, T\tag1
$$

其中，$\{n_t\}$是独立同分布的标准正态向量，$\{\beta_t\}$是某种方差调度。前向过程（1）可以等价地表示为：

$$
x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon_t,\tag2
$$

其中$\alpha_t = 1 - \beta_t$，$\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$，$\epsilon_t \sim N(0, I)$。需要注意的是，在表达式（2）中，向量$\{\epsilon_t\}$并不是独立的。这是因为每个$\epsilon_t$对应于噪声$n_1, \dots, n_t$的累积，因此$\epsilon_t$和$\epsilon_{t-1}$在所有$t$中高度相关。这一事实对训练过程无关紧要，因为训练过程不受跨时间步长$\epsilon_t$的联合分布影响，但对于我们接下来的讨论至关重要。

生成（反向）扩散过程从随机噪声向量$x_T \sim N(0, I)$开始，并通过递归迭代逐步对其去噪：

$$
x_{t-1} = \hat{\mu}_t(x_t) + \sigma_t z_t, \quad t = T, \dots, 1,\tag3
$$

其中$\{z_t\}$是独立同分布的标准正态向量，$\hat{\mu}_t(x_t)$是预测模型，$z_t$是噪声，$\sigma_t$是与方差调度相关的参数。

这些向量$\{x_T, z_T, \dots, z_1\}$唯一地确定了由过程（3）生成的图像$x_0$（但反之则不然）。因此，我们将这些噪声向量视为模型的潜在代码（见图3）。我们感兴趣的是反向过程，即给定一个真实图像$x_0$，我们希望提取噪声向量，如果在（3）中使用这些噪声向量，就可以生成$x_0$。我们将这些噪声向量称为与$x_0$一致的噪声向量。接下来，我们将介绍我们的反演方法，它适用于任意的$\eta \in (0, 1]$。

### 3.1 适合编辑的反演

需要注意的是，任何以$x_0$为真实图像的序列$x_0, \dots, x_T$都可以用于通过公式（3）提取一致的噪声图，具体为：

$$
z_t = \frac{x_{t-1} - \hat{\mu}_t(x_t)}{\sigma_t}, \quad t = T, \dots, 1。\tag5
$$

然而，除非这些辅助图像序列经过精心构建，否则它们很可能会远离网络$f_t(\cdot)$训练时的输入分布。在这种情况下，固定这些提取出来的噪声图$\{x_T, z_T, \dots, z_1\}$并改变文本条件，可能会导致较差的结果。

那么，应该如何构建用于公式（5）的辅助图像$x_1, \dots, x_T$呢？一种简单的方法是从与生成过程相似的分布中抽取这些图像。这种方法在【31】中得到了应用。具体来说，他们首先从$x_T \sim N(0, I)$进行采样。然后，对于每个时间步$t = T, \dots, 1$，他们使用$x_t$和真实的$x_0$从公式（2）中分离出$\epsilon_t$，再将这个$\epsilon_t$替换为公式（4）中的$f_t(x_t)$以计算$\hat{\mu}_t(x_t)$，并使用这个$\hat{\mu}_t(x_t)$通过公式（3）得到$x_{t-1}$。

该方法提取的噪声图与生成过程中的噪声分布相似。不幸的是，它们并不适合用于编辑全局结构。图4中展示了这一现象，图7中也展示了移动操作的上下文。其原因在于，DDPM的原生噪声空间本质上并不适合编辑。也就是说，即使我们获取了模型生成图像的“真实”噪声图，固定这些噪声图并改变文本提示，仍然无法保留图像的结构（见图2）。

有趣的是，我们观察到，直接从$x_0$而非公式（1）构建辅助序列$x_1, \dots, x_T$时，图像$x_0$更强烈地“印刻”在从公式（5）中提取的噪声图中。具体来说，我们建议将它们构建为：

$$
x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \tilde{\epsilon}_t, \quad t = 1, \dots, T,\tag6
$$

其中$\tilde{\epsilon}_t \sim N(0, I)$是统计独立的。需要注意的是，尽管公式（6）和（2）表面上相似，但它们描述的是本质上不同的随机过程。在公式（2）中，连续的$\epsilon_t$高度相关，而在公式（6）中，$\tilde{\epsilon}_t$是独立的。这意味着在我们的方法中，$x_t$和$x_{t-1}$通常比在公式（2）中相距更远，因此每个从公式（5）中提取的$z_t$的方差都比常规生成过程中的更大。我们的方法的伪代码在算法1中提供。

关于此反演方法，有几点需要说明。首先，它能够在机器精度范围内重构输入图像，前提是我们对数值误差积累进行了补偿（如算法1中的最后一行所示）。其次，任何扩散过程都可以直接使用该方法（例如，条件模型【11, 12, 24】、引导扩散【6】、无分类器引导【10】），只需使用适当的$\hat{\mu}_t(\cdot)$形式。最后，由于公式（6）的随机性，我们可以获得多种不同的反演。尽管每种反演都能实现完美的重构，但在编辑任务中使用时，它们会生成不同版本的编辑图像。这使得在例如基于文本的编辑任务中产生多样化的输出成为可能，而这一特性在DDIM反演方法中并不自然具备（见图1和附录材料）。

### 3.2 适合编辑的噪声空间的特性

我们现在来探讨适合编辑的噪声空间的特性，并将其与原生DDPM噪声空间进行比较。我们从一个二维的示例开始，如图5所示。这里，我们使用一个扩散模型，该模型旨在从$N((\begin{matrix} 10 \\ 10 \end{matrix}), I)$中采样。左上方的图展示了一个常规的DDPM过程，包含40个推理步骤。它从$x_T \sim N((\begin{matrix} 0 \\ 0 \end{matrix}), I)$（左下角的黑点）开始，生成了一个序列$\{x_t\}$（绿色点），最终到达$x_0$（右上角的黑点）。每个步骤都可以分解为确定性漂移$\hat{\mu}_t(x_t)$（蓝色箭头）和噪声向量$z_t$（红色箭头）。右上图展示了我们方法的潜在空间。具体来说，我们为某个给定的$x_0 \sim N((\begin{matrix} 10 \\ 10 \end{matrix}), I)$使用算法1计算了序列$\{x_t\}$和$\{z_t\}$。可以看出，在我们的方法中，噪声扰动$\{z_t\}$更大。这一特性来源于我们构建$x_t$的方式，相比公式（2），在我们的方法中，$x_t$和$x_{t-1}$通常相距更远，因此每个从公式（5）提取的$z_t$的方差比在常规生成过程中更大。仔细观察可以发现，连续噪声向量之间的夹角往往是钝角。换句话说，我们的噪声向量在连续的时间步长上是负相关的。从下方的两个图中可以看出这一点，图中展示了常规采样过程和我们方法中连续噪声向量之间的角度直方图。在前一种情况下，角度分布是均匀的，而在我们的方法中，角度分布在180度时有一个峰值。

同样的定性行为也存在于用于图像生成的扩散模型中。图6展示了从Imagenet训练的无条件扩散模型中使用100步采样时每像素的$z_t$方差和$z_t$与$z_{t-1}$之间的相关性。这里的统计量是基于从模型中抽取的10张图像计算的。与二维案例类似，我们的方法中的噪声向量具有更高的方差，并且在连续步骤之间表现出负相关。接下来，我们将展示这些较大方差的噪声向量如何更强烈地编码输入图像的结构，因此更适合用于编辑。

#### 图像移动

直观上，通过移动潜在代码的所有$T+1$张噪声图，应该可以实现对图像的移动。图7展示了对模型生成图像的潜在代码进行不同程度的移动所产生的效果。可以看到，移动原生潜在代码（用于生成图像的代码）会导致图像结构的完全丢失。相比之下，移动我们适合编辑的代码，结构仅有轻微的退化。定量评估见附录材料。

#### 颜色操作

我们的潜在空间还支持方便的颜色操作。具体来说，假设我们有一个输入图像$x_0$，一个二值掩码$B$，以及一个对应的彩色掩码$M$。我们首先使用公式（6）和（5）构建$\{x_1, \dots, x_T\}$并提取$\{z_1, \dots, z_T\}$，然后我们将噪声图修改为：

$$
z^{\text{edited}}_t = z_t + sB \odot (M - P(f_t(x_t))),
$$

其中$P(f_t(x_t))$源自公式（4），$s$是控制编辑强度的参数。我们在时间步范围$[T_1, T_2]$内进行这一修改。注意，括号内的项编码了期望颜色与每个时间步中预测的干净图像之间的差异。图8展示了这一过程的效果，并与SDEdit进行了比较，后者在输入图像的保真度与符合预期编辑效果之间存在固有的权衡。我们的方法可以在不修改纹理的情况下实现强烈的编辑效果（无论是在掩码内还是掩码外）。

## 4. 基于文本的图像编辑

我们的潜在空间可以用于基于文本的图像编辑。假设我们有一个真实图像$x_0$、描述该图像的文本提示$psrc$以及目标文本提示$ptar$。为了根据这些提示修改图像，我们在向去噪器注入$psrc$时提取适合编辑的噪声图$\{x_T, z_T, \dots, z_1\}$。然后，我们固定这些噪声图，并在向去噪器注入$ptar$时生成图像。我们从时间步$T - Tskip$开始运行生成过程，其中$Tskip$是一个控制图像与输入图像一致性的参数。图1、图2、图4和图9展示了使用这种方法进行的多种基于文本的编辑示例。可以看到，这种方法很好地修改了语义，同时保留了图像的结构。此外，它允许为任何给定的编辑生成多样化的输出（见图1和附录材料）。我们还进一步展示了如何将我们的反演与依赖于DDIM反演的方法结合使用（见图1和图11）。可以看到，这些方法通常无法保留细腻的纹理（如毛发、花朵或树叶），有时甚至无法保留对象的整体结构。通过集成我们的反演，结构和纹理得到了更好的保留。

## 5. 实验

我们在真实图像的基于文本的编辑任务上对我们的方法进行了定量和定性评估。我们分析了仅使用我们提取的潜在代码的效果（如第4节所述），并结合现有使用DDIM反演的方法进行了实验。在后一种情况下，我们使用我们提取的DDPM反演噪声图，将其注入反向过程中，并在此基础上进行例如对注意力图的操作。所有实验均使用了真实的输入图像以及源文本提示和目标文本提示。

#### 实现细节

我们使用了Stable Diffusion【24】，该扩散过程应用于一个预训练的图像自动编码器的潜在空间。图像尺寸为$512 \times 512 \times 3$，潜在空间的大小为$64 \times 64 \times 4$。我们的方法也适用于使用CLIP引导的无条件像素空间模型，然而我们发现Stable Diffusion可以产生更好的结果。两个超参数控制着输入图像的忠实度与目标提示的符合度之间的平衡：无分类器引导的强度【10】和在第4节中解释的$Tskip$。在我们的所有数值分析和本文中的所有结果中，我们使用的参数为：强度=15，$Tskip = 36$，$\eta = 1$，并进行了100步推理，除非另有说明。附录材料中提供了对强度和$Tskip$超参数影响的详细分析。

#### 数据集

我们使用了两个真实图像数据集：（i）来自【28】的“修改版ImageNet-R-TI2I”，并收集了来自互联网和其他数据集的附加示例，（ii）“修改版Zero-Shot I2I”，它包含了来自【22】的四类图像（猫、狗、马、斑马）以及从互联网上收集的图像。第一个数据集包含48张图像，每张图像有3-5个不同的目标文本提示，总共212对图像-文本对。第二个数据集每个类别有15张图像，每张图像对应一个目标文本提示，总共有60对图像-文本对。完整细节请见附录材料。

#### 评估指标

我们使用两种互补的指标对结果进行数值评估：LPIPS【32】量化结构保留的程度（越低越好），CLIP评分用于量化生成图像与文本提示的符合度（越高越好）。我们还评估了编辑图像所需的时间（以秒为单位）。关于多样性的进一步信息见附录材料。

#### 修改版ImageNet-R-TI2I数据集上的对比

我们将我们的结果与Plug-and-Play（PnP）【28】、EDICT【30】、空文本反演【20】和CycleDiffusion【31】进行对比。此外，我们还对比了我们的方法与Prompt-to-Prompt（P2P）【9】，既作为一种独立的技术，也在与我们的反演集成时进行对比。附录材料中提供了集成的详细信息。我们报告了CycleDiffusion的$\eta = 1$的结果，类似于我们的配置。$\eta = 0.1$的定量结果见附录材料。最后，我们还将我们的方法与普通的DDIM反演以及在特定时间步停止反演的DDIM反演进行了对比。所有方法均使用了其作者建议的默认参数，见附录材料。为了公平对比，我们对所有实验图像使用了相同的CycleDiffusion参数。这与其论文中对每张图像分别选择参数的做法不同。

如图9所示，我们的方法成功根据目标提示修改了真实图像。在所有情况下，我们的结果都表现出对输入图像的高度忠实度和对目标提示的符合度。EDICT的结果中出现了一些伪影，而CycleDiffusion生成的图像在目标文本提示的符合度上较差。PnP和空文本反演通常能够保留结构，但编辑一张图像通常需要超过2.5分钟。附录材料中还展示了普通的DDIM反演和P2P与我们反演集成的结果。图10展示了所有方法在修改版ImageNet-R-TI2I数据集上的CLIP-LPIPS损失图，其中，对于我们的反演、P2P与我们的反演以及CycleDiffusion，我们报告了使用三种不同参数的损失结果。可以看到，我们的方法在LPIPS和CLIP之间达到了良好的平衡。CycleDiffusion在应用强编辑时很难保留图像结构。将我们的反演集成到P2P中，改善了其在两个指标中的表现。附录材料中提供了更多细节。

#### 修改版Zero-Shot I2IT数据集上的对比

接下来，我们将我们的方法与使用DDIM反演的Zero-Shot图像到图像转换（Zero-Shot）【22】进行对比。该方法只在几个预定义的类别之间进行转换。我们遵循他们的设置，使用了50步扩散。在使用我们的反演时，我们将控制交叉注意力的超参数从默认值0.1降低到0.03。正如图11所示，虽然Zero-Shot的结果与目标文本一致，但通常较为模糊，缺乏细节。集成我们的反演后，输入图像的细节得以恢复。更多细节见附录材料。

## 6. 结论

我们提出了一种适用于DDPM的反演方法。我们的噪声图比常规采样中的噪声图更强烈地编码了图像结构，因此更适合用于图像编辑。我们展示了这些噪声图在基于文本的编辑中的优势，无论是独立使用还是与其他编辑方法结合使用。