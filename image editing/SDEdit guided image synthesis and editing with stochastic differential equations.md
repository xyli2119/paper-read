# SDEdit: guided image synthesis and editing with stochastic differential equations

## 摘要

引导图像合成使得普通用户可以通过最小的努力创建和编辑逼真的图像。关键挑战在于平衡用户输入（例如，手绘的彩色笔画）与合成图像的真实感。现有的基于GAN（生成对抗网络）的方法尝试通过使用条件GAN或GAN反演来实现这种平衡，但这些方法具有挑战性，通常需要为特定应用提供额外的训练数据或损失函数。为了解决这些问题，我们提出了一种新的图像合成和编辑方法，称为基于扩散模型生成先验的随机微分编辑（SDEdit）。该方法通过随机微分方程（SDE）迭代去噪合成逼真的图像。给定带有用户引导输入的图像，SDEdit首先向输入图像添加噪声，然后通过SDE先验去噪以增强其真实感。SDEdit 不需要任务特定的训练或反演，能够自然地实现真实感与忠实性的平衡。根据一项人类感知研究，SDEdit 在多个任务（包括基于笔画的图像合成和编辑以及图像合成）中，在逼真度和整体满意度评分上均比最先进的基于GAN的方法高出多达98.09%和91.72%。

## 1 引言

现代生成模型可以从随机噪声中生成逼真的图像（Karras 等，2019；Song 等，2021），成为视觉内容创作的重要工具。引导图像合成和编辑尤其引人关注，其中用户指定一般的引导（例如粗略的彩色笔画），生成模型则学习填充细节（见图1）。对于引导图像合成，有两个自然的要求：合成的图像应既真实又忠实于用户引导输入，从而使具有或不具艺术技能的人都能够从不同细节级别生成逼真的图像。

现有方法通常通过两种方法来实现这种平衡。第一类方法利用条件GAN（Isola 等，2017；Zhu 等，2017），通过直接学习从原始图像到编辑图像的映射。然而，对于每一个新的编辑任务，这些方法都需要数据收集和模型重新训练，而这两者都可能昂贵且耗时。第二类方法利用GAN反演（Zhu 等，2016；Brock 等，2017；Abdal 等，2019；Gu 等，2020；Wu 等，2021；Abdal 等，2020），其中预训练的GAN用于将输入图像反演为潜在表示，随后对其进行修改以生成编辑后的图像。此过程涉及为不同的图像编辑任务手动设计损失函数和优化过程。此外，有时可能无法找到忠实于输入的潜在代码（Bau 等，2019b）。

为了在避免上述挑战的同时平衡真实感和忠实性，我们提出了SDEdit，一种利用生成随机微分方程（SDE；Song 等，2021）的引导图像合成和编辑框架。类似于扩散模型（Sohl-Dickstein 等，2015；Ho 等，2020），基于SDE的生成模型通过迭代去噪将初始的高斯噪声向量平滑地转换为逼真的图像样本，并在无条件图像合成性能上达到或超越GAN（Dhariwal & Nichol，2021）。SDEdit的核心思想是“劫持”基于SDE的生成模型的生成过程，如图2所示。给定具有用户引导输入的图像（如笔画绘画或带笔画编辑的图像），我们可以添加适量的噪声来平滑不需要的伪影和失真（如笔画像素处的不自然细节），同时保留用户引导输入的整体结构。然后，我们使用此带噪输入初始化SDE，并逐步去除噪声，以获得既真实又忠实于用户引导输入的去噪结果（见图2）。

与条件GAN不同，SDEdit不需要为每个新任务收集训练图像或用户注释；与GAN反演不同，SDEdit不需要设计额外的训练或特定任务的损失函数。SDEdit仅使用一个预训练的基于SDE的生成模型，该模型在未标记数据上训练：给定操控RGB像素形式的用户引导，SDEdit向引导添加高斯噪声，然后运行反向SDE以合成图像。SDEdit自然地在真实感和忠实性之间找到权衡：当我们添加更多高斯噪声并运行SDE更长时间时，合成的图像更真实但忠实性较差。我们可以利用这一现象来找到真实感和忠实性之间的最佳平衡。

我们在三个应用中展示了SDEdit：基于笔画的图像合成、基于笔画的图像编辑以及图像合成。我们表明，SDEdit能够从具有不同忠实性水平的引导中生成真实且忠实的图像。在基于笔画的图像合成实验中，SDEdit在逼真度得分上比最先进的基于GAN的方法高出多达98.09%，在人类评判的整体满意度得分（衡量真实感和忠实性）上高出多达91.72%。在图像合成实验中，SDEdit在忠实性得分上表现更好，在用户研究的整体满意度得分上比基线方法高出多达8373%。我们的代码和模型将在发布时公开。

## 2 背景：使用随机微分方程（SDEs）进行图像合成

随机微分方程（SDE）通过在动力学中引入随机噪声，推广了常微分方程（ODE）。SDE的解是一个随时间变化的随机变量（即随机过程），我们将其表示为 $x(t) \in \mathbb{R}^d$，其中 $t \in [0,1]$ 表示时间。在图像合成中（Song 等，2021），我们假设 $x(0) \sim p_0 = p_{\text{data}}$ 表示从数据分布中抽取的样本，并且通过高斯扩散的方式，前向SDE生成 $t \in (0,1]$ 时的 $x(t)$。给定 $x(0)$，$x(t)$ 服从如下的高斯分布：

$$
x(t) = \alpha(t) x(0) + \beta(t) z \quad z \sim \mathcal{N}(0, I)\tag1
$$

其中 $\alpha(t) : [0,1] \rightarrow [0,\infty)$ 和 $\beta(t) : [0,1] \rightarrow [0,\infty)$ 是描述噪声 $z$ 强度的标量函数，$x(0)$ 是数据的强度。$x(t)$ 的概率密度函数表示为 $p_t$。

通常考虑两类SDE：方差爆炸SDE（VE-SDE）具有 $\alpha(t) = 1$，且 $\beta(1)$ 是一个大常数，这样 $p_1$ 接近 $\mathcal{N}(0, \sigma^2(1) I)$；而方差保持SDE（VP-SDE）满足 $\alpha^2(t) + \beta^2(t) = 1$ 对于所有 $t$，且 $\beta(t) \rightarrow 0$ 当 $t \rightarrow 1$，使得 $p_1$ 等于 $\mathcal{N}(0, I)$。随着 $t$ 从 $0$ 变为 $1$，这两种SDE都会将数据分布转换为随机高斯噪声。为了简洁起见，我们在本文的其余部分基于VE-SDE讨论细节，并在附录C中讨论VP-SDE的过程。尽管形式稍有不同，并且在不同的图像领域表现不同，但它们共享相同的数学直觉。

### 使用 VE-SDE 进行图像合成

在这些定义下，我们可以将图像合成问题表述为从噪声观测 $x(t)$ 中逐渐去除噪声以恢复 $x(0)$。这可以通过从 $t = 1$ 到 $t = 0$ 的反向SDE（Anderson, 1982；Song 等，2021）来实现，基于关于噪声扰动的得分函数 $\nabla_x \log p_t(x)$ 的知识。例如，VE-SDE 的采样过程由以下（反向）SDE定义：

$$
dx(t) = \frac{d[\sigma^2(t)]}{dt} \nabla_x \log p_t(x) \, dt + \frac{d[\sigma^2(t)]}{dt} \, dw\tag2
$$

其中，$w$ 是当时间从 $t = 1$ 向 $t = 0$ 流动时的 Wiener 过程。如果我们将初始条件设为 $x(1) \sim p_1 = \mathcal{N}(0, \sigma^2(1)I)$，则 $x(0)$ 的解将服从 $p_{\text{data}}$。在实践中，可以通过去噪得分匹配（Vincent，2011）来学习噪声扰动的得分函数。将学习到的得分模型表示为 $s_\theta(x(t), t)$，对于时间 $t$ 的学习目标为：

$$
L_t = \mathbb{E}_{x(0) \sim p_{\text{data}}, z \sim \mathcal{N}(0, I)} \left[\left\|s_\theta(x(t), t) - \frac{z}{\sigma(t)}\right\|^2_2\right]\tag3
$$

其中，$p_{\text{data}}$ 是数据分布，$x(t)$ 定义如方程(1)。总体的训练目标是对每个时间 $t$ 的学习目标 $L_t$ 的加权和，关于加权程序的不同讨论可以参考 Ho 等（2020）；Song 等（2020；2021）。

使用参数化得分模型 $s_\theta(x(t), t)$ 来逼近 $\nabla_x \log p_t(x)$，可以通过 Euler-Maruyama 方法来逼近SDE的解；从 $t + \Delta t$ 到 $t$ 的更新规则为：

$$
x(t) = x(t + \Delta t) + (\sigma^2(t) - \sigma^2(t + \Delta t)) s_\theta(x(t), t) + \sqrt{\sigma^2(t) - \sigma^2(t + \Delta t)} z\tag4
$$

其中 $z \sim \mathcal{N}(0, I)$。我们可以选择从 1 到 0 的特定时间间隔离散化，初始化 $x(0) \sim \mathcal{N}(0, \sigma^2(1)I)$，并通过方程(4)迭代生成图像 $x(0)$。

## 3 使用SDEdit进行引导图像合成和编辑

在本节中，我们介绍SDEdit并描述如何通过一个在未标注图像上预训练的SDE模型执行引导图像合成和编辑。

### 设置
用户提供一个全分辨率的图像 $x(g)$，以操控RGB像素的形式作为“引导”。引导可能包含不同层次的细节；高层次的引导仅包含粗略的彩色笔画，中层次的引导是在真实图像上添加彩色笔画，而低层次的引导是在目标图像上添加图像块。我们在图1中展示了这些引导，这些引导可以由非专家轻松提供。我们的目标是生成具有以下两个要求的全分辨率图像：
- **真实感**：图像应该看起来逼真（例如由人类或神经网络评估）。
- **忠实性**：图像应与引导 $x(g)$ 相似（例如通过L2距离衡量）。

我们注意到，真实感和忠实性并不总是正相关的，因为可能存在逼真的图像却不忠实（例如，随机生成的逼真图像），也可能存在忠实却不真实的图像（例如，引导本身）。与常规的逆问题不同，我们不假设对测量函数的了解（即，从真实图像到用户创建的RGB像素引导的映射是未知的），因此用于解决基于得分模型的逆问题的技术（Dhariwal & Nichol，2021；Kawar等，2021）以及需要配对数据集的方法（Isola等，2017；Zhu等，2017）在这里不适用。

### 过程
我们的方法SDEdit利用了反向SDE可以不仅从 $t_0 = 1$ 开始，还可以从任意中间时间 $t_0 \in (0,1)$ 开始的事实——这是先前基于SDE的生成模型未研究的方法。我们需要找到从引导开始的适当初始化，从该初始化我们可以通过求解反向SDE生成理想的、逼真且忠实的图像。对于任何给定的引导 $x(g)$，我们定义SDEdit过程如下：

$$
\text{Sample} \, x^{(g)}(t_0) \sim \mathcal{N}(x^{(g)}, \sigma^2(t_0)I)，然后通过迭代方程(4)生成 x(0)。
$$

我们用 $SDEdit(x^{(g)};t_0)$ 来表示上述过程。实际上，SDEdit选择一个特定的时间 $t_0$，将标准差为 $\sigma(t_0)$ 的高斯噪声添加到引导 $x(g)$，然后在 $t=0$ 处求解对应的反向SDE以生成合成的 $x(0)$。

除了SDE求解器采取的离散化步骤外，SDEdit的关键超参数是 $t_0$，即我们开始在反向SDE中执行图像合成过程的时间。在接下来的部分中，我们描述了一个真实感-忠实性权衡，该权衡允许我们选择合理的 $t_0$ 值。

### 真实感-忠实性权衡
我们注意到，对于正确训练的SDE模型，选择不同的 $t_0$ 会出现真实感和忠实性的权衡。为了说明这一点，我们集中在LSUN数据集上，并使用高层次的笔画绘画作为引导来进行基于笔画的图像生成。实验细节在附录D.2中提供。我们考虑了对于同一输入的不同 $t_0 \in [0,1]$ 选择。为了量化真实感，我们采用了用于比较图像分布的神经方法，例如核嵌入得分（KID；Binkowski等，2018）。如果合成图像与真实图像之间的KID较低，则合成图像逼真。对于忠实性，我们测量合成图像与引导 $x(g)$ 之间的L2距离平方。根据图3，我们观察到，随着 $t_0$ 增加，真实感增加但忠实性下降。

真实感-忠实性权衡可以从另一个角度解释。如果引导远离任何逼真的图像，那么为了生成逼真的图像，我们必须容忍一定程度的偏离引导（即非忠实性）。这一点在以下命题中得到了说明：

**命题1**：假设对于所有 $x \in X$ 和 $t \in [0,1]$，$\|s_\theta(x, t)\|_2^2 \leq C$。则对于所有 $\delta \in (0,1)$，以至少 $1 - \delta$ 的概率，

$$
\|x^{(g)} - SDEdit(x^{(g)}; t_0, \theta)\|_2^2 \leq \sigma^2(t_0)(C \sigma^2(t_0) + d + 2\sqrt{-d \cdot \log \delta} - 2 \log \delta)\tag5
$$

其中，$d$ 是 $x^{(g)}$ 的维数。证明见附录A。从高层次来看，引导与合成图像之间的差异可以分解为得分的输出和随机高斯噪声的影响；两者都会随着 $t_0$ 的增加而增大，因此差异变大。上述命题表明，若要以高概率生成逼真的图像，必须有足够大的 $t_0$。反之，如果 $t_0$ 太大，引导的忠实性会恶化，SDEdit将生成随机的逼真图像（极端情况下是无条件图像合成）。

### $t_0$ 的选择
我们注意到，引导的质量可能会影响合成图像的整体质量。对于合理的引导，我们发现 $t_0 \in [0.3, 0.6]$ 表现良好。然而，如果引导是仅包含白色像素的图像，那么即使是最接近模型分布的“逼真”样本也可能差距较大，因此我们必须通过选择较大的 $t_0$ 来牺牲忠实性以获得更好的真实感。在交互式设置中（用户绘制草图引导），我们可以初始化 $t_0 \in [0.3, 0.6]$，使用SDEdit生成候选图像，并询问用户样本是否应该更忠实或更逼真；通过用户的反馈，我们可以通过二分搜索找到合理的 $t_0$。在大规模的非交互式设置中（给定一组生成的引导），我们可以在随机选择的图像上执行类似的二分搜索以获得 $t_0$，然后在同一任务中的所有引导中固定 $t_0$。尽管不同的引导可能有不同的最优 $t_0$，但我们在实验中观察到，对于同一任务中的所有合理引导，共享的 $t_0$ 表现良好。

### 详细算法和扩展
我们在算法1中给出了基于VE-SDE的一般算法。由于篇幅限制，我们在附录C中描述了VP-SDE的详细算法。本质上，该算法是用于求解 $SDEdit(x(g); t_0)$ 的Euler-Maruyama方法。在某些情况下，如果我们希望保持合成图像的某些部分与引导一致，我们还可以引入一个额外的通道来屏蔽我们不希望编辑的图像部分。这是对主文中提到的SDEdit过程的一个轻微修改，我们在附录C.2中讨论了详细内容。

**算法 1 使用 SDEdit (VE-SDE) 进行引导图像合成和编辑**

### 输入：
- $x^{(g)}$ （引导）
- $t_0$ （SDE 超参数）
- $N$ （总去噪步数）

### 过程：
1. 初始化时间步长：$\Delta t \leftarrow \frac{t_0}{N}$
2. 生成噪声：$z \sim \mathcal{N}(0, I)$
3. 初始化图像：$x \leftarrow x + \sigma(t_0)z$
4. 对 $n$ 从 $N$ 到 $1$，执行以下步骤：
    1. 计算时间步长：$t \leftarrow \frac{t_0 n}{N}$
    2. 生成噪声：$z \sim \mathcal{N}(0, I)$
    3. 计算更新量：$\epsilon \leftarrow \sqrt{\sigma^2(t) - \sigma^2(t - \Delta t)}$
    4. 更新图像：$x \leftarrow x + \epsilon^2 s_\theta(x, t) + \epsilon z$
5. 返回 $x$

## 4 相关工作

### 条件GAN
用于图像编辑的条件GAN（Isola等，2017；Zhu等，2017；Jo & Park，2019；Liu等，2021）学习基于用户输入直接生成图像，并在各种任务上表现出成功，包括图像合成与编辑（Portenier等，2018；Chen & Koltun，2017；Dekel等，2018；Wang等，2018；Park等，2019；Zhu等，2020b；Jo & Park，2019；Liu等，2021），图像修复（Pathak等，2016；Iizuka等，2017；Yang等，2017；Liu等，2018），照片上色（Zhang等，2016；Larsson等，2016；Zhang等，2017；He等，2018），语义图像纹理与几何合成（Zhou等，2018；Guérin等，2017；Xian等，2018）。它们在使用用户草图或颜色进行图像编辑时也表现出强大的性能（Jo & Park，2019；Liu等，2021；Sangkloy等，2017）。然而，条件模型必须在原始图像和编辑后图像上进行训练，因此需要为新的编辑任务收集数据并重新训练模型。因此，将这些方法应用于实时图像操控仍然具有挑战性，因为每个新应用都需要训练一个新模型。与条件GAN不同，SDEdit只需要在原始图像上进行训练。因此，它可以直接在测试时应用于各种编辑任务，如图1所示。

### GAN反演与编辑
另一种主流的图像编辑方法是GAN反演（Zhu等，2016；Brock等，2017），其中输入首先投影到无条件GAN的潜在空间中，然后从修改后的潜在代码中生成新的图像。在这一方向上已经提出了多种方法，包括为每个图像微调网络权重（Bau等，2019a；Pan等，2020；Roich等，2021），选择更好或多个层次进行投影与编辑（Abdal等，2019；2020；Gu等，2020；Wu等，2021），设计更好的编码器（Richardson等，2021；Tov等，2021），建模图像损坏与变换（Anirudh等，2020；Huh等，2020），以及发现有意义的潜在方向（Shen等，2020；Goetschalckx等，2019；Jahanian等，2020；Härkönen等，2020）。然而，这些方法需要为不同的任务定义不同的损失函数。它们还需要进行GAN反演，而这对于各种数据集来说可能效率低下且不准确（Huh等，2020；Karras等，2020b；Bau等，2019b；Xu等，2021）。

### 其他生成模型
最近在训练非归一化概率模型方面取得了进展，例如基于得分的生成模型（Song & Ermon，2019；2020；Song等，2021；Ho等，2020；Song等，2020；Jolicoeur-Martineau等，2021）和基于能量的模型（Ackley等，1985；Gao等，2017；Du & Mordatch，2019；Xie等，2018；2016；Song & Kingma，2021），这些模型在图像样本质量上达到了与GAN相当的水平。然而，在这一方向上的大多数早期工作主要集中于无条件图像生成和密度估计，图像编辑与合成的先进技术仍然主要由基于GAN的方法主导。在这项工作中，我们专注于最近出现的基于随机微分方程（SDE）的生成建模，并研究其在可控图像编辑和合成任务中的应用。一项同时进行的工作（Choi等，2021）使用扩散模型执行条件图像合成，其中条件可以表示为底层真实图像的已知函数。

## 5 实验

在本节中，我们展示了SDEdit在基于笔画的图像合成与编辑以及图像合成任务中，能够超越最先进的基于GAN的模型。SDEdit和基线模型都使用了公开的预训练检查点。基于开源SDE检查点的可用性，我们在LSUN数据集上使用了VP-SDE，在CelebA-HQ数据集上使用了VE-SDE。

### 评估指标
我们基于真实感和忠实性对编辑结果进行评估。为了量化真实感，我们使用生成图像与目标逼真图像数据集之间的核嵌入得分（KID）（详细信息见附录D.2），并通过亚马逊机械土耳其（MTurk）进行不同方法之间的成对人类评估。为了量化忠实性，我们报告引导图像与编辑输出图像之间的L2距离，并对所有像素求和并归一化到[0,1]范围内。对于某些实验，我们还考虑LPIPS（Zhang等，2018）和MTurk人类评估。为了量化总体人类满意度得分（真实感+忠实性），我们利用MTurk人类评估进行SDEdit与基线模型之间的成对比较（见附录F）。

### 5.1 基于笔画的图像合成
给定输入笔画绘画，我们的目标是在没有配对数据的情况下生成逼真且忠实的图像。我们使用人类用户创建的笔画绘画引导（见图5）。同时，我们还提出了一种算法，基于源图像自动模拟用户笔画绘画（见图4），这使我们能够对SDEdit进行大规模定量评估。更多细节见附录D.2。

#### 基线模型
为了进行比较，我们选择了三种最先进的基于GAN的图像编辑与合成方法作为我们的基线模型。我们的第一个基线模型是StyleGAN2-ADA中的图像投影方法（Karras等，2020a），其中在StyleGANs的W+空间中通过最小化感知损失进行反演。我们的第二个基线是In-domain GAN（Zhu等，2020a），其中通过在编码器之上运行优化步骤来完成反演。我们考虑了In-domain GAN反演技术的两个版本：第一个版本（记为In-domain GAN-1）仅使用编码器来最大化反演速度，而第二个版本（记为In-domain GAN-2）运行额外的优化步骤以最大化反演精度。我们的第三个基线是e4e（Tov等，2021），其编码器目标明确设计为在感知质量和可编辑性之间取得平衡，通过鼓励将图像反演到预训练的StyleGAN模型的W空间附近。

#### 结果
我们在图4中展示了定性比较结果。我们观察到，所有基线模型都难以基于笔画绘画输入生成逼真的图像，而SDEdit能够成功生成保留输入笔画绘画语义的逼真图像。如图5所示，SDEdit还能够为相同的输入合成多样化的图像。我们在表1中展示了使用用户创建的笔画引导的定量比较结果，在表2中展示了使用算法模拟的笔画引导的定量比较结果。我们报告了L2距离用于忠实性比较，并利用MTurk（见附录F）或KID分数用于真实感比较。为了量化总体人类满意度得分（忠实性+真实感），我们要求不同的一组MTurk工作者进行基于忠实性和真实感的3000次成对比较。我们观察到，SDEdit在所有评估指标上都优于GAN基线模型，在真实感评分上超出基线模型80%以上，在总体满意度评分上超出基线模型75%以上。我们在附录C中提供了更多实验细节，在附录E中提供了更多结果。

### 5.2 灵活的图像编辑
在本节中，我们展示了SDEdit在图像编辑任务上能够超越现有的基于GAN的模型。我们重点关注LSUN（卧室、教堂）和CelebA-HQ数据集，实验设置的更多细节见附录D。

#### 基于笔画的图像编辑
给定带有笔画编辑的图像，我们希望基于用户编辑生成逼真且忠实的图像。我们考虑了与前一个实验相同的基于GAN的基线模型（Zhu等，2020a；Karras等，2020a；Tov等，2021）。如图6所示，基线生成的结果往往引入了不必要的修改，偶尔使笔画外的区域变得模糊。相比之下，SDEdit能够生成既逼真又忠实于输入的图像编辑，同时避免了不必要的修改。我们在附录E中提供了更多结果。

#### 图像合成
我们重点关注在CelebA-HQ数据集（Karras等，2017）上的图像合成。给定从数据集中随机采样的图像，我们要求用户指定他们希望通过像素块从其他参考图像复制以及希望修改的像素块（见图7）。我们将我们的方法与传统的融合算法（Burt & Adelson，1987；Pérez等，2003）以及先前考虑的基于GAN的基线模型进行比较。我们在图7中进行了定性比较。对于定量比较，我们报告了L2距离用于量化忠实性。为了量化真实感，我们要求MTurk工作者进行1500次基线模型与SDEdit的成对比较。为了量化用户满意度得分（忠实性+真实感），我们要求不同的工作者进行另外1500次与SDEdit的成对比较。为了量化不必要的更改（例如身份的改变），我们按照Bau等（2020）的方法计算了屏蔽LPIPS（Zhang等，2018）。如表3所示，我们观察到SDEdit能够生成既忠实又逼真的图像，其LPIPS得分比基线模型好得多，在总体满意度评分上优于基线模型83.73%，在真实感上优于基线模型75.60%。尽管我们的真实感评分略低于e4e，但SDEdit生成的图像更忠实且整体上更令人满意。我们在附录D中提供了更多实验细节。

## 6 结论

我们提出了随机微分编辑（SDEdit），这是一种通过随机微分方程（SDE）进行生成建模的引导图像编辑与合成方法，能够在真实感和忠实性之间实现平衡。与通过GAN反演的图像编辑技术不同，我们的方法不需要针对特定任务的优化算法来重构输入图像，特别适用于GAN反演损失难以设计或优化的数据集或任务。与条件GAN不同，我们的方法不需要为“引导”图像收集新的数据集或重新训练模型，这两者可能既昂贵又耗时。我们证明了SDEdit在基于笔画的图像合成、基于笔画的图像编辑和图像合成任务中，能够在无需特定任务训练的情况下优于现有的基于GAN的方法。