# Focus on Your Instruction: Fine-grained and Multi-instruction Image Editing by Attention Modulation

## 摘要

最近，基于扩散的方法，如InstructPix2Pix（IP2P），已经在基于指令的图像编辑中取得了有效的成果，仅需要用户提供自然语言指令。然而，这些方法往往会不经意地改变非目标区域，并且在多指令编辑中表现不佳，导致结果质量下降。为了解决这些问题，我们提出了**Focus on Your Instruction**（FoI），该方法旨在确保在多个指令中实现精确且和谐的编辑，无需额外的训练或测试时的优化。在FoI中，我们主要强调两个方面：(1) 精确提取每个指令的感兴趣区域，(2) 引导去噪过程集中在这些感兴趣区域内。对于第一个目标，我们从指令与图像之间的交叉注意力中识别了IP2P的隐式对齐能力，并开发了一种有效的掩码提取方法。对于第二个目标，我们引入了一个交叉注意力调制模块，用于大致隔离目标编辑区域和无关区域。此外，我们还引入了一种基于掩码的解耦采样策略，以进一步确保清晰的区域隔离。实验结果表明，FoI在定量和定性评估中都超越了现有方法，尤其在多指令编辑任务中表现出色。代码可在[https://github.com/guoqincode/Focus-on-Your-Instruction](https://github.com/guoqincode/Focus-on-Your-Instruction)获取。

## 1. 引言
大规模文本到图像（T2I）扩散模型【5, 12, 33, 41, 43, 45–47, 63】在图像生成中的多样性和逼真度方面取得了显著的进展，吸引了广泛关注。这些先进的T2I模型在大量图像-文本数据集【48】上训练，擅长各种生成任务。然而，它们在图像编辑中的直接应用仍然有限，往往缺乏在图像中控制特定对象或属性的精确性。

在编辑图像时，视觉创作者通常首先确定需要编辑的区域，然后专注于修改这些区域。对于多次编辑，确保最终结果的协调性至关重要。尽管最近基于文本的图像编辑【7, 10, 17, 24, 30, 32, 33, 38, 53, 56, 66】取得了显著进展，但精确编辑目标区域而不影响无关区域仍然是一个重要挑战。这些方法通常难以准确定位编辑区域，导致非目标区域的意外修改，进而导致次优结果。此外，它们通常难以同时执行多方向编辑，进一步限制了在复杂编辑任务中的应用。

IP2P【7】提供了一种直观且保真度高的基于指令的图像编辑方法，无需对输入和输出图像进行详细描述。然而，如图1所示，IP2P容易出现过度编辑的问题，最近的研究【21, 31】也指出了这一点。在我们对IP2P的分析中，我们揭示了它通过在成对的合成数据集上训练而开发出的强大的隐式对齐能力。如图2所示，在初始去噪步骤的交叉注意力图中，我们可以观察到关键词与图像中空间位置的精确对齐。这种有效的对齐甚至扩展到形容词和动词。这与Stable Diffusion【2, 9, 17, 51】等模型中的注意力图演变形成了鲜明对比。然而，如图3所示，虽然IP2P能够有效定位诸如“帽子”之类的物品，但其他指令词可能会意外影响无关区域，导致非预期的编辑。根据我们的了解，目前尚无方法利用IP2P的强大对齐能力来增强其编辑能力。

为了解决当前图像编辑方法的局限性，并与视觉创作者的编辑范式保持一致，我们引入了**Focus on Your Instruction**（FoI），该方法基于IP2P框架开发，专为精确且和谐的多指令编辑而设计，尤其值得注意的是，它无需额外的训练或测试时优化。首先，我们利用IP2P的隐式对齐能力来识别每个指令的感兴趣区域。接着，我们引入交叉条件注意力调制，通过使用空指令的交叉注意力来调制交叉注意力的计算，使每个指令集中于其对应的区域，并隐式减少不同指令之间的干扰。最后，我们提出了一种基于掩码的解耦采样策略，旨在准确区分编辑区域和非编辑区域，分离整体编辑方向和保留原始图像方向，从而增强模型在超参数选择中的鲁棒性。实验结果表明，FoI在定量和定性评估中均优于现有方法，尤其在多指令编辑任务中表现出色。

我们的贡献可以总结如下：
- 我们引入了FoI，这是一种利用IP2P的对齐能力进行精确且和谐的多指令编辑的方法，无需额外的训练或测试时优化。
- 我们提出了交叉条件注意力调制，确保每个指令专注于其对应的区域，从而减少干扰。该方法使用空指令的交叉注意力来调制带指令的交叉注意力计算。
- 我们开发了基于掩码的解耦采样策略，用于隔离编辑区域并区分编辑和保留方向。
- 我们在实验中展示了FoI的优异表现，特别是在多指令编辑任务中，定量和定性评估均优于现有方法。



## 2.相关工作 

### 文本引导的图像编辑。

早期的方法主要依赖于生成对抗网络（GANs）[13, 15, 29, 39, 62]，在特定领域如人脸和花朵上表现出色，但通用性有限。最近，基于扩散模型的方法[19, 49]在图像生成和编辑方面展现了前所未有的能力[5, 33, 41, 43, 45, 47, 63]。SDEdit[30]利用这些模型通过噪声添加和去噪两步过程来与提示对齐。Imagic[24]为每张图片微调扩散模型，专注于对象变体的生成。Prompt2Prompt(P2P)[17]和PnP[53]探索了注意力和特征注入以提高图像编辑性能。相比于P2P，PnP可以直接编辑真实图像。为了使P2P适应真实的图像编辑，Null-Text Inversion (NTI)[32]提出更新空文本嵌入以实现精确重建和编辑[18]。Blended Diffusion[3, 4]使用用户设计的掩码和提示实现了局部编辑。IP2P[7]通过直接应用指令简化了图像编辑，避免了对详细描述或掩码的需求。这种方法不仅规避了基于逆向方法中的重建缺陷[10, 17, 50]，还避免了长时间的测试时优化[32, 38, 53, 56]，提高了图像保真度。 

### 定位目标编辑区域。

精确的编辑区域定位对于防止意外改变图像至关重要。Text2Live[6]利用CLIP[42]优化加性图像层。FEAT[20]和CoralStyleCLIP[44]利用StyleGAN的潜在代码进行领域特定的局部编辑。Diffedit[10]和Watch Your Steps[31]通过对比不同的噪声预测生成掩码。InstructEdit[58]和OIR[64]使用文本条件分割模型[25, 28]识别指定编辑的对象，但在细粒度编辑方面遇到困难。LPM[40]根据交叉注意力值聚类自注意力图，主要集中在对象级别的形状变化。然而，大多数开放域视觉编辑工作在细节编辑和保持原始图像保真度方面面临挑战。例如，在指令“给她戴上迪士尼发带”中，目的是简单地添加发带，但典型的方法往往会改变身份特征或其他图像区域。通过利用IP2P的隐式定位能力，我们的方法能够针对每个指令准确地定位最相关的区域，比之前的方法实现更细粒度的编辑，同时只增加了最小的计算开销。 

### 多指令图像编辑。

一个关键挑战在于有效地指导模型针对每个指令的具体编辑区域，同时最小化指令之间的干扰以确保和谐的多指令结果。近期的工作如EMILIE[21]关注迭代多指令编辑，但忽略了IP2P的过度编辑问题，主要解决连续编辑中图像质量下降的问题。现有的基于指令的方法[7, 11, 14, 65, 67]往往难以处理多指令任务。当代的多对象编辑方法[57, 64]使用基于逆向的技术[50]，主要集中在对象级别的替换。这些方法由于需要复杂的优化而难以应对细粒度编辑，并且往往耗时。相比之下，我们的方法避免了额外的训练或测试时优化，并且可以轻松地与现有的基于指令的模型集成。



## 3.前提知识 

### 指令Pix2Pix。

给定一张图像 $I$，IP2P 根据给定的编辑指令 $T$ 对其进行编辑。IP2P 在使用 P2P [17] 和 GPT-3 [8] 合成的数据集上进行了监督训练。数据集中的每一项都包括原始图像 $I$、编辑指令 $T$ 以及相应的编辑结果 $I_e$。IP2P 是建立在 Stable Diffusion 框架 [45] 上的，采用了一个带有编码器 $E$ 和解码器 $D$ 的 VQ-VAE [54] 来增强效率和质量。在训练过程中，向 $z = E(I_e)$ 添加噪声 $\epsilon \sim N(0,1)$ 以创建噪声潜变量 $z_t$，噪声级别由随机时间步 $t \in T$ 设定。去噪器 $\epsilon_\theta$ 最初带有 Stable Diffusion 的权重 [45]，通过微调以最小化 $E_{I_e,I,\epsilon,t} \lVert \epsilon - \epsilon_\theta(z_t,t,I,T) \rVert_2^2$。在训练过程中，通过设置 $I = \emptyset_I$ 或 $T = \emptyset_T$ 来间歇地忽略条件 [27]。基础的 IP2P 分数估计如下：
$$
\tilde{\epsilon}_\theta(z_t, t, I, T) = \epsilon_\theta(z_t,t,\emptyset_I,\emptyset_T) + \\s_I (\epsilon_\theta(z_t,t,I,\emptyset_T) - \epsilon_\theta(z_t,t,\emptyset_I,\emptyset_T)) \\+ s_T (\epsilon_\theta(z_t,t,I,T) - \epsilon_\theta(z_t,t,I,\emptyset_T)),(1)
$$


研究 [7, 16, 31] 强调了平衡图像引导 $s_I$ 和文本引导 $s_T$ 的重要性。增加 $s_I$ 可以保留图像细节但减少指令的影响；而增加 $s_T$ 则存在过度编辑的风险。因此，$\epsilon_\theta(z_t,t,I,\emptyset_T)$ 估计用于图像保留的分数，而 $\epsilon_\theta(z_t,t,I,T)$ 用于应用编辑。

###  指令Pix2Pix 中的交叉注意力。

IP2P 通过交叉注意力机制 [55] 将文本特征整合到图像编辑中。此过程在每次去噪步骤 $t$ 为输入指令中的每一个标记（使用 CLIP [42] 的标记器标记化）生成交叉注意力图 $A_t \in \mathbb{R}^{r \times r \times N}$，其中 $r \in \{64,32,16,8\}$ [2, 9, 17]。由于 IP2P 将原始图像整合到其 U-Net 的输入通道中，其注意力机制的行为与 Stable Diffusion [45] 相比展现出一些区别。我们用 $A_{t,ins}$ 表示 $\epsilon_\theta(z_t,t,I,T)$ 中的交叉注意力图。

## 4.方法 

给定输入图像 $I$ 和复合指令 $T$，后者由 $k$ 个子指令 {$T_1, T_2, ..., T_k$} 组成，我们的目标是编辑 $I$，使其（1）精确执行 $T$ 中的每个子指令，以及（2）作为一个整体和谐地执行 $T$。我们认为这里的核心挑战是如何精确地将指令指向它们对应的兴趣区域。为了解决这一挑战，我们提出了 FoI，其总体框架如图 4 所示。在本节中，我们首先讨论如何为每个子指令找到精确的兴趣区域（第 4.1 节）。然后，我们将介绍如何通过交叉注意力调制（第 4.2 节）和解耦采样策略（第 4.3 节）来引导去噪过程朝向每个指令各自的兴趣区域的方向进行。

### 4.1 基于指令提取的掩码

受到大规模扩散模型分割能力的启发 [22, 52, 59, 60]，我们对IP2P的分析揭示了其在早期去噪步骤中的精确定位能力，这从图2中的交叉注意力图中可以看出。如图3所示，IP2P能够快速识别如“帽子”这样的物体应放置的位置。我们利用这种强大的定位能力，从IP2P的交叉注意力图中提取出每个指令的关注区域。

先前的研究 [2, 9, 17, 61] 表明，分辨率为16×16的注意力图能够捕捉到最详细的语义信息。因此，我们使用分辨率为 $r = 16$ 的注意力图来提取掩码。

在每个子指令 $T_i$ 中，我们识别出一个关键词 $e_i$，该关键词表示要编辑的目标对象、要添加的对象，或从上下文推断出的对象。首先，我们对相应的交叉注意力图 $A_t[e_i] \in \mathbb{R}^{r \times r}$ 应用高斯滤波器 [9]。这一步确保地图中的每个补丁都是原始地图中相邻补丁的线性组合。然后，我们使用直接有效的算法来提取掩码，通过迭代增强交叉注意力图 $A_t[e_i]$。该算法通过一系列操作运行，共重复 $\gamma$ 次。在每个循环中，图被平方，然后通过最小-最大归一化（min-max scaling）到 [0, 1] 范围。这种迭代方法旨在逐步增强目标区域与周围区域之间的对比度，如下方公式所述：

$$

A_t[e_i] = \text{norm} \cdot \text{norm} \cdot \dots \cdot \text{norm}(A_t[e_i]^2 \dots 2^2) \quad (\gamma \text{次})
$$

其中，$\text{norm}$ 表示最小-最大归一化过程，将地图中的值缩放到 [0, 1] 范围。完成 $\gamma$ 次迭代后，我们应用阈值 $\tau$ 来计算掩码 $M_{e_i} = 1(A_t[e_i] \geq \tau)$。该掩码表示第 $i$ 个子指令的关注区域，其维度为 $\in \mathbb{R}^{r \times r}$。图4展示了我们方法在为每个子指令提取掩码时的有效结果。

### 4.2 跨条件注意力调制

为了进行精细编辑，将每个指令限制在其掩码内是至关重要的。我们引入了跨条件注意力调制。该方法利用无指令的交叉注意力图来调制有指令的交叉注意力计算，从而减少指令对无关区域的影响，并在存在多个指令时减少它们之间的干扰。具体来说，我们在计算 $\epsilon_\theta(z_t, t, I, T)$ 时保留了掩码区域的注意力，而将外部区域的注意力替换为 $\epsilon_\theta(z_t, t, I, \varnothing_T)$ 的注意力。修改后的交叉注意力函数定义如下：

$$
A'_t, \text{ins} = \frac{\text{softmax}(X + \Delta X) \odot M + Y \odot (1 - M)}{\sqrt{d}}
$$

这里，$d$ 表示潜在投影维度。各个项的定义如下：

$$
X = Q_{I, T} K_{I, T}^T,
$$

$$
Y = Q_{I, \varnothing_T} K_{I, \varnothing_T}^T,
$$

$$
\Delta X = \alpha \odot \xi(t) \odot (\max(Q_{I, T} K_{I, T}^T) - Q_{I, T} K_{I, T}^T)
$$

其中，$Q_{I, T}$ 和 $K_{I, T}$ 分别是 $\epsilon_\theta(z_t, t, I, T)$ 中的查询和键，而 $Q_{I, \varnothing_T}$ 和 $K_{I, \varnothing_T}$ 分别是 $\epsilon_\theta(z_t, t, I, \varnothing_T)$ 中的查询和键。注意力掩码 $M$ 通过首先将每个关键词的掩码 $M_{e_i}$ 广播到其相应的子指令 $T_i$ 中来构建。随后，这些广播后的掩码会被连接到所有子指令中，结果使得 $M$ 的初始维度为 $R^{(r \times r) \times N}$。然后，这个掩码会在每个交叉注意力层中自适应地进行插值。

我们利用 $\Delta X$ 来微妙地增强掩码内的注意力值。这使得我们能够精确控制不同子指令的相对强度，从而实现对每个子指令强度的细粒度控制。通过选择性地调整系数向量 $\alpha$ 中的值，我们可以细化每个子指令的强度。通过这种方法，我们能够在掩码内定向注意力，并灵活调整每个子指令的相对强度，确保在编辑过程中效果集中且可控。

在方程 (6) 中，与时间步相关的权重项为：

$$
\xi(t) = 0.05 \cdot t^4
$$

时间步 $t \in [0, 1]$ 已被归一化。

在掩码提取之后，我们在剩余的所有去噪步骤中应用跨条件注意力调制。图4的底部展示了在应用跨条件注意力调制前后的交叉注意力图。可以看出，与调制前相比，每个子指令在其各自的关注区域内变得更加集中。

### 4.3 掩码引导的解耦采样

虽然在交叉注意力层级限制关注区域是有用的，但由于交叉注意力中的语义丰富层分辨率较低 [9, 55, 61]，这对于精细编辑来说是不够的。此外，解耦方程 (1) 中的不同噪声估计是具有挑战性的，这导致在任意选择 $s_I$ 和 $s_T$ 时缺乏鲁棒性。因此，我们建议修改噪声估计，以在采样过程中将编辑区域与无关区域隔离开来，并解耦编辑方向和保持原始图像的方向。为此，我们首先结合所有子指令对应的掩码，得到 $M_{\text{union}}$：

$$
M_{\text{union}} = \text{Upsample} \left( \sum_i M_{e_i} \right)
$$

其中，Upsample 表示将掩码上采样以匹配潜在空间的分辨率。接下来，新的得分估计公式如下：

$$
\tilde{\epsilon_\theta}(z_t, t, I, T) = \epsilon_\theta(z_t, t, \varnothing_I, \varnothing_T) + s_I \left( \epsilon_\theta(z_t, t, I, \varnothing_T) - 
\epsilon_\theta(z_t, t, \varnothing_I, \varnothing_T) \right) + s_T \left( \epsilon_\theta(z_t, t, I, T) - \epsilon_\theta(z_t, t, I, \varnothing_T) \right) \odot M_{\text{union}}
$$
我们将使用上述得分估计的采样称为解耦采样。在实践中，我们在最初的 75% 步骤中使用解耦采样，而在剩下的 25% 中切换到标准的 IP2P 采样。

## 5. 实验
### 5.1 实验设置
**数据集**. 对于单指令编辑，我们从IP2P数据集 [7] 中筛选出5,000张局部编辑类型的图像，使用GPT-4 [34] 进行过滤，每张图像都带有特定的对象编辑标签。对于多指令编辑，我们收集了100张真实图像。对于每张图像，使用GPT-4V(ision) [1, 34, 35] 创建2-4个指令，并附有原始和目标描述，标记要编辑的对象。

**评估指标**. 我们使用CLIP图像相似性 [42] 和Dinov2图像相似性 [36] 来测量编辑后图像与原始图像之间的余弦相似度。CLIP文本-图像方向相似性 [13] 用于评估图像的变化与其标题变化的对应程度。此外，PickScore [26] 基于学习的人类偏好评估图像保真度。

**基准模型**. 我们与最新的图像编辑方法进行对比，包括Diffedit [10]、NTI+P2P [32]、IP2P [7]、MagicBrush [65] 和InstructDiffusion (InsDiff) [14]。Diffedit通过原始和目标提示的噪声预测差异来识别编辑区域。NTI+P2P扩展了P2P [17]，实现了对真实图像的编辑，代表了基于反转的图像编辑方法。IP2P是基于指令的编辑方法的基础模型。MagicBrush在其自己构建的高质量数据集上微调了IP2P。InsDiff使用了与IP2P相同的模型结构，但在多个数据集上训练了一个通用模型。与之密切相关的WatchYourSteps [31] 因缺乏可用的实现，无法进行直接比较。

**实现细节**. 在我们所有的实验中，我们使用了预训练的IP2P模型 [7]，并冻结了权重。我们使用了Euler祖先采样器 [23]，总共进行100步去噪操作。除非另有说明，默认设置为 $s_I = 1.5$ 和 $s_T = 7.5$。掩码提取如第4.1节所述，仅在第一个去噪步骤中执行。阈值 $\tau$ 从 [0.4, 0.7] 范围内随机采样，超参数 $\gamma$ 设置为3。根据之前的工作 [31]，初始噪声通过向原始图像添加80%的噪声生成，因此实际去噪步骤为80步。

### 5.2 主要结果
**定性评估**. 我们在图5中展示了一些定性实验结果。从实验中我们观察到以下几点：首先，Diffedit 和 NTI+P2P 经常忽略复杂编辑场景中的子任务。例如，在图5(d) 中，塔的编辑被忽略了；在图5(e) 中，WALL-E仍然在水中而不是沙漠中；在图5(f) 中，草地没有变成沙滩。这些模型还导致了编辑区域内的不必要修改，例如对蒙娜丽莎面部特征的过度修改（图5(a)），错误的面部编辑（图5(b)），以及对图5(f)中狗的过度修改。其次，基于指令的编辑方法如IP2P、MagicBrush和InsDiff倾向于过度编辑。这可以从IP2P和MagicBrush超出意图的头带区域修改蒙娜丽莎的身份看出（图5(a)）。在图5(b)中，它们将整个图像变为紫色，而在图5(c)中，它们将背景改为蓝色。过度编辑也可以在图5(d)和(e)中由MagicBrush和InsDiff看出，而IP2P在图5(d)中遗漏了塔的编辑。这三种方法在图5(e)中对WALL-E进行了过度编辑，导致与原始图像的显著差异。IP2P和MagicBrush未能在图5(f)中恰当地为狗添加太阳镜，而InsDiff创建了一个不完整的戴着太阳镜的狗头。尽管指令只要求将草地变为沙滩，IP2P、MagicBrush和InsDiff还修改了背景。

与Diffedit相比，我们的方法以更高的精度和细节提取了关注区域的掩码，确保了更高质量和更精细的编辑。这进一步证明了IP2P在增强精细编辑时的隐性定位能力。

我们的方法提供了更细致的编辑能力，并且不影响无关区域，在多指令场景中尤其超越了基准模型。

**定量评估**. 如表1所示，在单指令和多指令评估中，我们在CLIP图像相似性、Dinov2图像相似性和PickScore中都取得了最先进的结果，表明我们的方法在图像的保真度和编辑效果上与人类感知最一致。值得注意的是，我们的方法在多指令编辑任务中的表现显著优于基准模型，证明了我们的方法在面对复杂编辑指令时的优越性。

在CLIP方向相似性指标中，我们的方法得分低于MagicBrush [65] 和InsDiff [14]，因为它们倾向于过度编辑，使输入图像朝着指令方向发生较大变化，而我们的方法专注于必要的编辑并尽量减少对无关区域的影响。CLIP [42] 本身在感知细微变化方面存在困难 [37]，这也证明了我们进行了更精细的编辑。过度编辑会导致CLIP图像相似性和Dinov2图像相似性降低，而CLIP方向相似性增加。我们的方法在保持原始图像细节和执行编辑指令之间取得了平衡。

**人类偏好研究**. 对于单指令和多指令编辑，我们使用每类20张图像进行人类偏好研究，比较我们的FoI方法与Diffedit、NTI+P2P、IP2P、MagicBrush和InsDiff。该研究包括60名参与者。在指令对齐方面，参与者被要求选择最符合编辑指令效果的方法。在图像对齐方面，他们选择最能保留原始图像细节（即无关区域没有发生变化）的方法。如表2所示，在单指令和多指令编辑中，我们的FoI方法相比基准方法更受青睐，尤其是在多指令编辑场景中，超过80%的参与者认为FoI的编辑质量和保真度优于基准模型。这凸显了FoI在精确和高质量编辑任务中的优越性。

更多结果请参见附录B。

### 5.3 消融研究

**掩码提取步骤**. 如图6(a)所示，在更长的时间步长中搜索掩码并不会增强结果；相反，这会在更多的去噪步骤中导致无意的无关区域编辑，并且还会降低在预期编辑区域内的有效性。

**跨条件注意力调制**. 如图6(b)所示，在不同的去噪步骤中停止交叉注意力调制会产生不同的效果。过早终止可能导致指令影响无关区域，尤其是在多指令场景中，这还可能扰乱其他指令。值得注意的是，子指令的有效性随着执行步骤的增加而增强。

**解耦采样**. 如图6(c)所示，即使应用了交叉注意力调制，使用宽泛的形容词仍然可能导致无关区域的轻微修改。解耦采样方法缓解了注意力调制中颗粒度不足的问题，有效地将编辑区域与无关区域分离开来。然而，在所有步骤中使用解耦采样可能导致次优结果。例如，在图6(c)的第80步，“Make it sunset” 指令产生了比第60步更碎片化的效果。此外，如图6(d)所示，我们设置 $s_I = 1.5$ 并逐步增加 $s_T$，与之前的方法 [7, 16, 21, 31] 需要精确调整 $s_I$ 和 $s_T$ 之间的平衡不同，我们的方法在保持平衡方面具有更大的鲁棒性。

定量评估和消融研究的分析将在附录A中详细说明。

## 6. 结论
我们提出了FoI，一种无需调参的方法，使预训练的IP2P模型能够执行精确的单指令编辑和多指令编辑。我们发现了IP2P模型的隐式定位能力，并提取了与每个指令对应的掩码。此外，我们利用这些掩码进行跨条件注意力调制，将指令限制在其各自的掩码内，同时减少不同指令之间的干扰。最后，我们引入了解耦采样，旨在将编辑区域与无关区域隔离开来，并解耦编辑方向与保持原始图像方向。我们的方法在定性和定量实验中都表现出色。

**局限性**. 我们的方法也存在一些局限性。虽然较小的交叉注意力图富含语义内容，但它们在一定程度上限制了我们超精细编辑的能力。此外，我们方法的有效性在很大程度上依赖于预训练的IP2P模型的能力。