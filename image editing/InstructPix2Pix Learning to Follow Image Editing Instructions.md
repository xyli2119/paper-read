# InstructPix2Pix: Learning to Follow Image Editing Instructions

## 摘要
我们提出了一种基于人类指令进行图像编辑的方法：给定一张输入图像和一条书面指令，模型根据这些指令对图像进行编辑。为了获取该问题的训练数据，我们结合了两个大型预训练模型的知识——语言模型（GPT-3）和文本到图像模型（Stable Diffusion），生成了大量的图像编辑示例数据集。我们的条件扩散模型InstructPix2Pix在我们生成的数据上进行训练，并能够在推理时泛化到真实图像和用户编写的指令。由于它在前向传播中执行编辑操作，不需要针对每个示例进行微调或反转，因此我们的模型能够在几秒钟内快速编辑图像。我们展示了对多种输入图像和书面指令的编辑效果，结果令人信服。

## 1. 引言
我们提出了一种方法，能够教导生成模型遵循人类编写的指令进行图像编辑。由于此任务的训练数据难以大规模获取，我们提出了一种生成配对数据集的方法，该方法结合了多个在不同模态上预训练的大型模型：一个大型语言模型（GPT-3 [7]）和一个文本到图像模型（Stable Diffusion [52]）。这两个模型捕捉了关于语言和图像的互补知识，可以结合起来为跨模态任务生成配对训练数据。

利用我们生成的配对数据，我们训练了一个条件扩散模型，该模型在给定输入图像和编辑指令的情况下，生成经过编辑的图像。我们的模型直接在前向传播过程中执行图像编辑，不需要任何额外的示例图像、输入/输出图像的完整描述或每个示例的微调。尽管模型完全在**合成示例（即生成的书面指令和生成的图像）上训练**，但它能够实现零样本泛化，适用于任意真实图像和自然的人类编写指令。我们的模型支持直观的图像编辑，可以根据人类指令进行多种编辑操作：替换对象、改变图像风格、修改场景、艺术媒介等。选定的示例可以在图1中找到。

## 2. 相关工作
### 大型预训练模型的组合
最近的研究表明，大型预训练模型可以组合起来解决单一模型无法独立完成的多模态任务，如图像描述生成和视觉问答（这些任务需要结合大型语言模型和文本到图像模型的知识）。组合预训练模型的技术包括在新任务上的联合微调[4, 34, 41, 68]，通过提示进行沟通[63,70]，组合基于能量模型的概率分布[11, 38]，通过反馈指导一个模型[62]，以及迭代优化[35]。我们的方法与先前的工作类似，利用了两个预训练模型——GPT-3[7]和Stable Diffusion[52]——的互补能力，但不同之处在于我们使用这些模型生成配对的多模态训练数据。

### 基于扩散的生成模型
扩散模型的最新进展[60]使得图像合成达到了最先进的水平[10,18,19,54,56,61]，并且还出现了其他模态的生成模型，如视频[21,59]、音频[31]、文本[36]和网络参数[46]。最近的文本到图像扩散模型[42,49,52,55]已经展示了从任意文本描述生成逼真图像的能力。

### 图像编辑的生成模型
传统的图像编辑模型通常针对单一编辑任务，如风格迁移[15, 16]或图像域之间的转换[22, 24, 37, 43, 72]。许多编辑方法通过反转[1–3, 12]或编码[8, 51, 64]图像到潜在空间（如StyleGAN[26, 27]），通过操纵潜在向量来进行编辑。最近的模型利用CLIP[48]嵌入向量通过文本引导图像编辑[5, 9, 14, 29, 32, 42, 45, 71]。我们与其中的一种方法Text2Live[6]进行了比较，该方法优化了一个加性图像层，以最大化CLIP相似度目标。

最近的研究使用预训练的文本到图像扩散模型进行图像编辑[5,17,28,39,49]。虽然一些文本到图像模型本身具备编辑图像的能力（例如，DALLE-2可以生成图像的变体、填充区域并操纵CLIP嵌入向量[49]），但使用这些模型进行有针对性的编辑并不容易，因为大多数情况下，它们无法保证相似的文本提示会生成相似的图像。Hertz等人的最新研究[17]通过Prompt-to-Prompt方法解决了这个问题，该方法可以同化生成的图像，使其对应于相似的文本提示，从而可以对生成的图像进行局部编辑。我们使用这种方法生成训练数据。为了编辑非生成的（即真实的）图像，SDEdit[39]使用预训练模型对输入图像进行加噪和去噪，并结合新的目标提示。我们将SDEdit作为基线进行比较。其他最新的工作通过给定标题和用户绘制的遮罩进行局部填充[5,49]，生成从小规模图像集合中学习到的特定对象或概念的新图像[13,53]，或通过反转（和微调）单个图像并使用新文本描述重新生成[28]。与这些方法不同的是，我们的模型只需要一张图像和一条编辑指令（即，不需要对图像进行完整描述），并且能够在前向传播中直接执行编辑，无需用户绘制遮罩、额外的图像或针对每个示例的反转或微调。

### 学习遵循指令
我们的方法与现有的基于文本的图像编辑工作[6,13,17,28,39,53]不同，它通过指令告诉模型要执行的操作，而不是提供输入/输出图像的文本标签、标题或描述。遵循编辑指令的一个关键优势在于，用户只需用自然语言文本告诉模型具体要做什么，而无需提供额外的信息，如示例图像或输入和输出图像之间保持不变的视觉内容的描述。指令表达清晰、精确且易于书写，使用户能够轻松隔离需要更改的特定对象或视觉属性。我们跟随书面图像编辑指令的目标受到了最近研究中教导大型语言模型更好地遵循人类指令执行语言任务的启发[40,44,69]。

### 使用生成模型生成训练数据
深度模型通常需要大量的训练数据。互联网数据集通常是合适的，但可能不存在适合监督的形式，例如特定模态的配对数据。随着生成模型的不断进步，越来越多的研究关注使用它们作为下游任务的廉价且丰富的训练数据来源[33, 47, 50, 58, 65, 66]。在本文中，我们使用两种不同的现成生成模型（语言模型和文本到图像模型）来为我们的编辑模型生成训练数据。

## 3. 方法
我们将基于指令的图像编辑视为一个监督学习问题：（1）首先，我们生成一个配对的训练数据集，包含文本编辑指令和编辑前/后的图像（第3.1节，图2a-c）；然后（2）我们在生成的数据集上训练一个图像编辑扩散模型（第3.2节，图2d）。尽管模型在生成的图像和编辑指令上进行训练，但它能够泛化到使用任意人类编写的指令对真实图像进行编辑。图2展示了我们方法的概述。

### 3.1 生成多模态训练数据集

我们结合了两个在不同模态上运行的大规模预训练模型的能力——一个大型语言模型[7]和一个文本到图像模型[52]——来生成一个包含文本编辑指令及其对应的编辑前后图像的多模态训练数据集。在接下来的两节中，我们详细描述了这一过程的两个步骤。在第3.1.1节中，我们描述了微调GPT-3 [7]生成一组文本编辑指令的过程：给定一个描述图像的提示，生成描述要进行的更改的文本指令，以及描述更改后图像的提示（图2a）。然后，在第3.1.2节中，我们描述了如何使用文本到图像模型[52]将这两个文本提示（即编辑前和编辑后）转换为一对对应的图像（图2b）。

#### 3.1.1 生成指令和配对标题
我们首先完全在文本领域操作，利用一个大型语言模型接受图像标题，并生成编辑指令以及编辑后的文本标题。例如，如图2a所示，提供输入标题“一个女孩骑马的照片”，我们的语言模型可以生成一个合理的编辑指令“让她骑一条龙”，以及一个适当修改的输出标题“一个女孩骑龙的照片”。在文本领域操作使我们能够生成大量多样化的编辑，同时保持图像变化与文本指令之间的一致性。

我们的模型通过在一个相对较小的人工编写的**编辑三元组数据集上微调GPT-3进行训练：（1）输入标题，（2）编辑指令，（3）输出标题。**为了生成微调数据集，我们从LAION-Aesthetics V2 6.5+ [57] 数据集中抽取了700个输入标题，并手动编写了指令和输出标题。有关我们编写的指令和输出标题的示例，请参见表1a。使用这些数据，我们对GPT-3 Davinci模型进行了单轮次**微调**，使用默认的训练参数。

受益于GPT-3的广泛知识和泛化能力，我们的微调模型能够生成具有创造性且合乎逻辑的指令和标题。有关GPT-3生成数据的示例，请参见表1b。我们的数据集是通过使用该训练模型生成大量编辑和输出标题而创建的，其中输入标题来自LAION-Aesthetics（排除具有重复标题或重复图像URL的样本）。我们选择LAION数据集是因为它规模大，内容多样（包括对专有名词和流行文化的引用），且涵盖了多种媒介（照片、绘画、数字艺术）。LAION的一个潜在缺点是它相当嘈杂，包含一些无意义或描述不清的标题——不过，我们发现通过数据集筛选（第3.1.2节）和无分类器引导（第3.2.1节）相结合，可以减轻数据集噪声的影响。我们生成的最终指令和标题语料库包含454,445个示例。

#### 3.1.2 从配对标题生成配对图像
接下来，我们使用预训练的文本到图像模型将一对标题（指编辑前后的图像）转换为一对图像。将一对标题转换为一对对应图像的一个挑战是，文本到图像模型无法保证图像的一致性，即使在条件提示有极小变化的情况下。例如，两个非常相似的提示：“一只猫的照片”和“一只黑猫的照片”可能会生成完全不同的猫的图像。这对于我们的目的来说是不合适的，因为我们打算将这些配对数据作为训练模型编辑图像的监督数据（而不是生成不同的随机图像）。因此，我们使用了Prompt-to-Prompt [17]，这是一种旨在鼓励文本到图像扩散模型生成多个相似图像的最新方法。该方法通过在某些去噪步骤中借用交叉注意力权重来实现。图3展示了使用和不使用Prompt-to-Prompt生成的图像对比。

虽然这种方法极大地帮助了生成图像的同化，不同的编辑可能需要在图像空间中进行不同程度的变化。例如，较大幅度的变化（如改变大规模的图像结构，移动物体，或替换成形状不同的物体）可能需要较少的图像对相似性。幸运的是，Prompt-to-Prompt具有一个参数，可以控制两幅图像之间的相似度：即共享注意力权重的去噪步骤比例$p$。然而，仅根据标题和编辑文本确定$p$的最优值是困难的。因此，我们为每对标题生成100对图像样本，每个样本使用随机的$p \sim U(0,1.0)$，并使用基于CLIP的度量来过滤这些样本：即由Gal等人[14]引入的CLIP空间中的方向相似性。这一度量衡量了两幅图像在CLIP空间中的变化与两幅图像标题之间变化的一致性。执行这种过滤不仅有助于最大化图像对的多样性和质量，还使我们的数据生成对Prompt-to-Prompt和Stable Diffusion的失败更具鲁棒性。

### 3.2 InstructPix2Pix
我们使用生成的训练数据来训练一个基于指令的图像编辑条件扩散模型。我们的模型基于Stable Diffusion，这是一种大规模的文本到图像潜在扩散模型。

扩散模型[60]通过一系列去噪自编码器学习生成数据样本，这些自编码器估计数据分布的得分[23]（指向更高密度数据的方向）。潜在扩散模型[52]通过在预训练的变分自编码器[30]的潜在空间中操作，提高了扩散模型的效率和质量。对于图像$x$，扩散过程向编码后的潜变量$z=E(x)$中添加噪声，产生噪声潜变量$z_t$，其中噪声级别随着时间步长$t \in T$增加。我们学习一个网络，该网络根据图像条件$c_I$和文本指令条件$c_T$，预测添加到噪声潜变量$z_t$中的噪声。我们最小化以下潜在扩散目标：

$$
L = \mathbb{E}_{E(x)} \mathbb{E}_{(c_I, c_T) \sim \mathcal{N}(0, 1), t} \left( \| z_t - \epsilon_\theta(z_t | c_I, c_T) \|^2 \right)
$$

Wang等人[67]表明，对于图像翻译任务，微调一个大型图像扩散模型比从头训练模型表现更优，尤其是当配对训练数据有限时。因此，我们使用一个预训练的Stable Diffusion检查点来初始化我们的模型权重，利用其强大的文本到图像生成能力。为了支持图像条件输入，我们在第一个卷积层中添加了额外的输入通道，将$z_t$与$E(c_I)$拼接在一起。扩散模型的所有可用权重都从预训练检查点初始化，针对新添加的输入通道的权重则初始化为零。我们复用了原本用于标题的文本条件机制，以接收文本编辑指令$c_T$作为输入。更多训练细节在附录材料中提供。

### 3.2.1 无分类器指导的双条件扩散
无分类器扩散指导[20]是一种用于权衡扩散模型生成样本质量和多样性的方法。它通常用于类别条件和文本条件的图像生成，以提高生成图像的视觉质量并使采样的图像更好地符合其条件。无分类器指导有效地将概率质量向一个隐式分类器$p_{\theta}(c|z_t)$对条件$c$赋予高似然的地方偏移。无分类器指导的实现涉及联合训练扩散模型的条件和无条件去噪，并在推理时结合这两种得分估计。无条件去噪的训练通过在训练的某些频率下将条件设为固定的空值$c = \emptyset$来完成。在推理时，使用指导尺度$s ≥ 1$，修改后的得分估计$\widetilde{e_\theta}(z_t , c)$向条件得分${e_\theta}(z_t , c)$的方向外推，并远离无条件得分$e_\theta(z_t,\emptyset)$：

$$
\widetilde{e_\theta}(z_t | c) = e_\theta(z_t,\emptyset) + s ·(e_\theta(z_t , c) - e_\theta(z_t,\emptyset)),(2)
$$

对于我们的任务，得分网络$e_\theta(z_t , c_I, c_T)$有两个条件输入：输入图像$c_I$和文本指令$c_T$。我们发现，在两个条件输入上使用无分类器指导是有益的。Liu等人[38]展示了一个条件扩散模型可以组合来自多个不同条件值的得分估计。我们将相同的概念应用于我们的模型，具有两个独立的条件输入。在训练中，我们随机将5%的示例设定为只有$c_I = \emptyset$，5%的示例设定为只有$c_T = \emptyset$，以及5%的示例同时设定为$c_I = \emptyset$和$c_T = \emptyset$。因此，我们的模型能够针对一个或两个条件输入进行条件或无条件去噪。我们引入了两个指导尺度$s_I$和$s_T$，它们可以调节生成样本与输入图像和编辑指令的对应强度。我们的修改得分估计如下：

$$
\widetilde{e_\theta}(z_t , c_I, c_T) = \epsilon_\theta(z_t,\empty,\empty) + s_I (e_\theta(z_t , c_I,\empty) - e_\theta(z_t,\empty,\empty)) + s_T (e_\theta(z_t , c_I, c_T) - e_\theta(z_t , c_I,\empty)) ,(3)
$$

在图4中，我们展示了这两个参数对生成样本的影响。关于我们无分类器指导方法的详细信息，请参见附录B。

## 4. 结果
我们展示了基于指令的图像编辑结果，涵盖了多种类型的编辑和指令措辞，适用于各种真实照片和艺术作品。请参阅图1、图5、图6、图7、图11、图12、图15、图16、图17、图18和图19中的选定结果。我们的模型成功完成了许多具有挑战性的编辑任务，包括替换物体、改变季节和天气、替换背景、修改材料属性、转换艺术媒介等各种编辑操作。我们定性地将我们的方法与最近的几项工作进行比较，包括SDEdit[39]和Text2Live[6]。我们的模型遵循如何编辑图像的指令，而先前的工作（包括这些基线方法）则需要图像的描述（或编辑层）。因此，我们为它们提供了“编辑后”的文本描述，而不是编辑指令。我们还使用两个度量标准对我们的模型与SDEdit进行定量比较，这些度量标准衡量图像一致性和编辑质量，进一步描述见第4.1节。最后，我们展示了生成训练数据的大小和质量如何影响模型性能的消融实验，见第4.2节。

### 4.1. 基线比较
我们提供了与SDEdit[39]和Text2Live[6]的定性比较，并与SDEdit进行了定量比较。SDEdit[39]是一种使用预训练扩散模型编辑图像的技术，输入一个部分加噪的图像并进行去噪，以生成一个新的编辑图像。我们与SDEdit的公开Stable Diffusion实现进行了比较。Text2Live[6]是一种通过生成颜色+不透明度增强层并根据文本提示进行图像编辑的技术。我们与作者发布的公共实现进行比较。

在图9中，我们对两种方法进行了定性比较。我们注意到，虽然SDEdit在内容保持大致不变且仅改变风格的情况下效果良好，但在需要较大改动时，它难以保持图像主体的一致性和隔离单个对象。此外，它需要对所需图像进行完整的描述，而不是编辑指令。另一方面，虽然Text2Live能够在涉及加性层的编辑中产生令人信服的结果，但它的设计限制了其可处理的编辑类别。

在图8中展示了与SDEdit的定量比较。我们绘制了两个度量之间的权衡：CLIP图像嵌入的余弦相似度（编辑后的图像与输入图像的匹配程度）和Galet al.[14]引入的方向性CLIP相似度（文本标题的变化与图像变化的匹配程度）。这些是相互竞争的度量——增加输出图像与期望编辑的匹配程度会减少它们与输入图像的相似性（一致性）。尽管如此，我们发现，与SDEdit相比，我们的方法在相同的方向性相似度值下，具有显著更高的图像一致性（CLIP图像相似度）。

### 4.2. 消融实验
在图10中，我们提供了关于数据集大小选择和第3.1节中描述的数据集筛选方法的定量消融实验。我们发现，减少数据集的大小通常会降低执行较大（即更显著的）图像编辑的能力，转而只能进行微小或风格上的图像调整（因此，保持较高的图像相似性分数，但方向性得分较低）。从我们的数据生成中移除CLIP筛选会产生不同的效果：总体上图像与输入图像的一致性降低。

我们还在图4中提供了两个无分类器指导尺度对生成样本影响的分析。增加$s_T$会使图像应用更强的编辑（即输出与指令的匹配度更高），增加$s_I$可以帮助保持输入图像的空间结构（即输出与输入图像的匹配度更高）。我们发现$s_T$在5到10范围内，$s_I$在1到15范围内通常能产生最佳结果。在实践中，本文中的结果表明，为每个示例调整指导权重以获得一致性与编辑强度之间的最佳平衡是有益的。

## 5. 讨论
我们展示了一种方法，将两个大型预训练模型（一个大型语言模型和一个文本到图像模型）结合起来，生成用于训练扩散模型的数据集，使其能够遵循书面图像编辑指令。虽然我们的方法能够对图像进行多种多样的引人注目的编辑，包括风格、媒介和其他上下文的变化，但仍存在一些局限性。

我们的模型受限于生成数据集的视觉质量，因此也受限于用于生成图像的扩散模型（在这种情况下为Stable Diffusion[52]）。此外，我们的方法在泛化到新的编辑和正确关联视觉变化与文本指令的能力，受限于用于微调GPT-3[7]的人类编写指令，GPT-3生成指令和修改标题的能力，以及Prompt-to-Prompt[17]修改生成图像的能力。特别是，我们的模型在计数物体数量和空间推理方面表现不佳（例如，“将它移到图像左边”、“交换它们的位置”或“在桌子上放两个杯子，椅子上放一个杯子”），就像Stable Diffusion和Prompt-to-Prompt一样。故障示例见图13。此外，我们的方法基于的数据和预训练模型存在众所周知的偏见，因此我们的编辑图像可能会继承这些偏见或引入其他偏见（见图14）。

除了减轻上述局限性外，我们的工作还提出了一些问题，例如：如何遵循空间推理的指令，如何将指令与其他条件模态（如用户交互）结合，以及如何评估基于指令的图像编辑。结合人类反馈以改进模型是未来工作的另一个重要领域，人类参与的强化学习等策略可以用于提高模型与人类意图之间的对齐度。