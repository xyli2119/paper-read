# Prompt-to-Prompt Image Editing with Cross Attention Control

## Abstract

近年来，大规模的文本驱动合成模型因其在生成与给定文本提示相符的高多样性图像方面表现出色而备受关注。这种基于文本的合成方法特别吸引那些习惯于通过语言描述其意图的人们。因此，将文本驱动的图像合成扩展到文本驱动的图像编辑是自然而然的。然而，对于这些生成模型来说，编辑是一项具有挑战性的任务，因为编辑技术的固有特性是要保留原始图像的大部分内容，而在基于文本的模型中，即使是文本提示中的微小修改也常常会导致完全不同的结果。最先进的方法通过要求用户提供一个空间掩码来定位编辑区域，从而忽略了被掩码区域内的原始结构和内容。在本文中，我们提出了一种直观的“从提示到提示”的编辑框架，其中编辑仅由文本控制。为此，我们深入分析了一个文本条件模型，发现交叉注意力层是控制图像空间布局与提示中每个词之间关系的关键。基于这一观察，我们展示了几种仅通过编辑文本提示来监控图像合成的应用。这些应用包括通过替换词语进行局部编辑、通过添加说明进行全局编辑，甚至能够精细地控制词语在图像中反映的程度。我们在多样化的图像和提示上展示了我们的结果，展现了高质量的合成效果以及对编辑后提示的高度忠实。

## 1 引言

近年来，大规模语言-图像（LLI）模型，如Imagen [38]、DALL·E 2 [33] 和 Parti [48] 展现了非凡的生成语义和组合能力，并在研究界和公众中引起了前所未有的关注。这些LLI模型在极其庞大的语言-图像数据集上进行了训练，并使用了最先进的图像生成模型，包括自回归模型和扩散模型。然而，这些模型并不提供简单的编辑方式，并且通常无法对给定图像的特定语义区域进行有效控制。特别是，甚至文本提示中的细微变化也可能导致完全不同的输出图像。

为了解决这个问题，基于LLI的方法[28, 4, 33]要求用户明确遮盖图像的一部分进行修复，并推动编辑后的图像仅在遮盖区域内发生变化，同时与原始图像的背景相匹配。这种方法提供了令人满意的结果，然而，遮盖过程繁琐，阻碍了快速直观的文本驱动编辑。此外，遮盖图像内容会移除重要的结构信息，这些信息在修复过程中被完全忽略。因此，一些编辑功能超出了修复的范围，例如修改特定对象的纹理。

在本文中，我们介绍了一种直观且强大的文本编辑方法，通过提示到提示的操作，在预训练的文本条件扩散模型中进行语义图像编辑。为此，我们深入研究了交叉注意力层，探讨其作为控制生成图像的语义能力的潜力。

具体而言，我们考虑了内部的交叉注意力图，这些是绑定从提示文本中提取的像素和标记的高维张量。我们发现这些图包含丰富的语义关系，这些关系对生成的图像产生了重要影响。

我们的核心思想是，**通过在扩散过程中注入交叉注意力图，可以编辑图像，从而控制哪些像素在扩散的各个步骤中关注提示文本的哪些标记**。为了将我们的方法应用于各种创造性的编辑应用中，我们展示了几种通过简单且语义化的界面来控制交叉注意力图的方法（见图1）。**第一种方法**是改变提示中的单个标记的值（例如，将“dog”改为“cat”），同时固定交叉注意力图，以保持场景的构图。**第二种方法**是通过在提示中添加新词并冻结对先前标记的注意力，同时允许新的注意力流向新标记，来对图像进行全局编辑，例如更改风格。**第三种方法**是放大或减弱生成图像中某个词的语义效果。

我们的方法构成了一种直观的图像编辑界面，仅通过编辑文本提示来实现，因此称之为“提示到提示”（Prompt-to-Prompt）。这种方法能够完成许多其他方式难以实现的编辑任务，并且不需要模型训练、微调、额外数据或优化。在整个分析过程中，我们发现了对生成过程的更多控制，识别出编辑后的提示与源图像之间的保真度之间的权衡。**我们甚至展示了通过使用现有的逆向过程将我们的方法应用于真实图像的可能性**。我们的实验和大量结果表明，我们的方法能够在极其多样化的图像上实现无缝且直观的基于文本的编辑。

## 2 相关工作

图像编辑是计算机图形学中最基本的任务之一，涵盖了通过使用辅助输入（如标签、涂鸦、掩码或参考图像）修改输入图像的过程。一种特别直观的图像编辑方式是通过用户提供的文本提示。最近，利用生成对抗网络（GANs）[15, 8, 19–21]在文本驱动的图像操作上取得了显著进展。GANs以其高质量生成而闻名，并与CLIP [32]结合使用，CLIP由数百万对文本图像对训练而成，具有语义丰富的联合图像-文本表示。这些结合了这些组件的开创性工作[29, 14, 46, 2]是革命性的，因为它们不需要额外的人工操作，仅使用文本即可生成高度逼真的操作。Bau等人[7]进一步展示了如何使用用户提供的掩码来定位基于文本的编辑，并将更改限制在特定的空间区域。然而，尽管基于GAN的图像编辑方法在高度策划的数据集（如人脸）上取得了成功[27]，但它们在大型且多样化的数据集上表现不佳。

为了获得更具表现力的生成能力，Crowson等人[9]使用在多样化数据上训练的VQ-GAN [12]作为主干。其他一些工作[5, 22]利用了最新的扩散模型[17, 39, 41, 17, 40, 36]，这些模型在高度多样化的数据集上实现了最先进的生成质量，常常超过GANs [10]。Kim等人[22]展示了如何进行全局更改，而Avrahami等人[5]成功地使用用户提供的掩码进行局部操作。

尽管大多数只需要文本（即不需要掩码）的工作仅限于全局编辑[9, 23]，但Bar-Tal等人[6]提出了一种基于文本的局部编辑技术，不使用任何掩码，展示了令人印象深刻的结果。然而，他们的技术主要允许更改纹理，但不能修改复杂的结构，如将自行车改为汽车。此外，与我们的方法不同的是，他们的方法需要为每个输入训练一个网络。

众多工作[11, 16, 42, 25, 26, 30, 31, 34, 49, 9, 13, 36]极大地推动了基于纯文本的图像生成，即文本到图像合成。最近出现了几种大规模文本-图像模型，如Imagen [38]、DALL-E2 [33] 和 Parti [48]，展示了前所未有的语义生成能力。然而，**这些模型并未提供对生成图像的控制，特别是仅使用文本指导。更改与图像相关的原始提示中的一个词通常会导致完全不同的结果**。例如，在“dog”前添加形容词“white”常常会改变狗的形状。为了解决这个问题，一些工作[28, 4]假设用户提供一个掩码来限制更改的应用区域。

与以往的工作不同，**我们的方法只需要文本输入，通过使用生成模型内部层的空间信息**。这为用户提供了一种更加直观的编辑体验，仅通过修改文本提示即可修改局部或全局细节。

## 3 方法

设 $I$ 为使用文本提示 $P$ 和随机种子 $s$ 通过文本引导扩散模型[38]生成的图像。我们的目标是仅通过编辑后的提示 $P^*$ 来引导编辑输入图像，生成编辑后的图像 $I^*$。例如，考虑从提示“我的新自行车”生成的图像，假设用户希望编辑自行车的颜色、材质，甚至将其替换为滑板车，同时保留原始图像的外观和结构。对用户来说，一种直观的界面是直接通过进一步描述自行车的外观或将其替换为另一个词来更改文本提示。

与以往的工作不同，我们希望**避免依赖任何用户定义的掩码来辅助或指示编辑应发生的位置**。一种简单但不成功的尝试是固定内部随机性，并使用编辑后的文本提示重新生成图像。不幸的是，如图2所示，这会导致一个具有不同结构和构图的完全不同的图像。

我们的关键观察是，生成图像的结构和外观不仅取决于随机种子，还取决于像素与文本嵌入在扩散过程中的交互。通过修改在交叉注意力层中发生的像素与文本的交互，我们提供了“提示到提示”的图像编辑功能。更具体地说，**注入输入图像 $I$ 的交叉注意力图使我们能够保留原始的构图和结构**。在第3.1节中，我们回顾了交叉注意力的使用方式，并在第3.2节中描述了如何利用交叉注意力进行编辑。关于扩散模型的更多背景信息，请参见附录A。

### 3.1 文本条件扩散模型中的交叉注意力

我们使用**Imagen [38]** 文本引导的合成模型作为主干。由于图像的构图和几何形状主要在64 × 64分辨率下确定，我们**只对文本到图像的扩散模型进行适配**，保持超分辨率过程不变。回顾一下，每个扩散步骤 $t$ 包括使用U型网络 [37] 从噪声图像 $z_t$ 和文本嵌入 $\psi(P)$ 预测噪声 $ϵ$。在最后一步，这个过程生成了图像 $I = z_0$。最重要的是，两种模态之间的交互发生在噪声预测过程中，**此时视觉和文本特征的嵌入通过交叉注意力层融合，生成每个文本标记的空间注意力图。**

**图3：方法概述。** 上部：视觉和文本嵌入通过生成每个文本标记的空间注意力图的交叉注意力层进行融合。下部：我们通过使用源图像的注意力图来控制生成图像的空间布局和几何形状。这使得可以仅通过编辑文本提示来实现各种编辑任务。**当在提示中替换一个词时，我们注入源图像的注意力图 $M_t$，覆盖目标图像的注意力图 $M_t^*$，以保留空间布局。而在添加新短语的情况下，我们只注入与提示中未更改部分对应的注意力图。通过重新加权相应的注意力图，可以增强或减弱一个词的语义效果。**

更正式地，如图3（上）所示，噪声图像的深度空间特征 $\phi(z_t)$ 被投影到查询矩阵 $Q = l_{Q}(\phi(z_t))$，而文本嵌入被投影到键矩阵 $K = l_K(\psi(P))$ 和值矩阵 $V = l_V(\psi(P))$，**通过学习的线性投影 **$l_Q, l_K, l_V$。然后，注意力图被定义为
$$
M = \text{Softmax}\left(\frac{QK^T}{\sqrt{d}}\right),(1)
$$

其中，$M_{ij}$ 定义了第 $j$ 个标记在第 $i$ 个像素上的值的权重，$d$ 是键和值查询的潜在投影维度。最后，交叉注意力输出被定义为 $\phi'(z_t) = MV$，然后用来更新空间特征 $\phi(z_t)$。

直观上，交叉注意力输出 $MV$ 是值 $V$ 的加权平均，其中权重为注意力图 $M$，这些权重与 $Q$ 和 $K$ 之间的相似度相关。实际上，为了增加其表达能力，使用了多头注意力 [44] 并行执行，然后将结果连接并通过学习的线性层获得最终输出。

Imagen [38]，类似于GLIDE [28]，在每个扩散步骤的噪声预测中基于文本提示进行条件处理（见附录A.2），通过两种类型的注意力层：i) 交叉注意力层。ii) 混合注意力层，通过简单地将文本嵌入序列连接到每个自注意力层的键-值对，既作为自注意力又作为交叉注意力。在本文的其余部分中，我们将两者统称为交叉注意力，因为我们的方法仅干预混合注意力中的交叉注意力部分。也就是说，**在混合注意力模块中，只有最后的通道，即与文本标记相关的部分，被修改**。

### 3.2 控制交叉注意力

**图4：文本条件扩散图像生成的交叉注意力图。** 上排显示了用于合成左侧图像的提示中每个词的平均注意力掩码。下排显示了在不同扩散步骤中与“bear”和“bird”两个词相关的注意力图。

我们回到关键观察——生成图像的空间布局和几何形状依赖于交叉注意力图。图4展示了像素和文本之间的这种交互关系，其中绘制了平均注意力图。可以看到，**像素更倾向于被描述它们的词语所吸引**，例如，熊的像素与“熊”这个词相关。请注意，平均化处理仅用于可视化目的，在我们的方法中，每个注意力头的注意力图都是独立的。有趣的是，我们可以看到**图像的结构在扩散过程的早期步骤中就已经确定。**

由于注意力反映了整体的构图，我们可以将通过原始提示 $P$ 生成的注意力图 $M$ 注入到使用修改后的提示 $P^*$ 进行的第二次生成中。这使得生成的编辑图像 $I^*$ 不仅根据编辑后的提示进行了修改，还保留了输入图像 $I$ 的结构。这个例子是基于注意力的一系列操作中的一个特定实例，导致了不同类型的直观编辑。因此，我们首先提出一个通用框架，接着描述具体的编辑操作。

设 $DM(z_t, P, t, s)$ 为扩散过程单步 $t$ 的计算，它输出噪声图像 $z_{t-1}$ 和注意力图 $M_t$（如果未使用则省略）。我们将 $DM(z_t, P, t, s)\{M \leftarrow M_c\}$ 表示为用给定的注意力图 $M_c$ 覆盖原注意力图 $M$ 的扩散步骤，但保留来自提供的提示的值 $V$。我们还用 $M_t^*$ 表示使用编辑后的提示 $P^*$ 生成的注意力图。最后，我们定义 $Edit(M_t, M_t^*, t)$ 为一个通用的编辑函数，在生成过程中接收原始图像和编辑后图像的第 $t$ 步注意力图作为输入。

我们用于控制图像生成的通用算法包括同时对两个提示进行迭代扩散过程，其中在每一步根据所需的编辑任务应用基于注意力的操作。需要注意的是，为了使上述方法有效，我们必须固定内部随机性。这是因为扩散模型的特性，即使对于相同的提示，不同的随机种子也会产生截然不同的输出。形式化地，我们的通用算法如下：

**算法 1: 提示到提示的图像编辑**

1. 输入：源提示 $P$，目标提示 $P^*$，随机种子 $s$。
2. 输出：源图像 $x_{\text{src}}$ 和编辑后的图像 $x_{\text{dst}}$。
3. $z_T \sim N(0, I)$ 为具有随机种子 $s$ 的单位高斯随机变量；
4. $z_T^* \leftarrow z_T$；
5. 对于 $t = T, T-1, \ldots, 1$，执行
   6. $z_{t-1}, M_t \leftarrow DM(z_t, P, t, s)$；
   7. $M_t^* \leftarrow DM(z_t^*, P^*, t, s)$；
   8. $cM_t \leftarrow Edit(M_t, M_t^*, t)$；
   9. $z_{t-1}^* \leftarrow DM(z_t^*, P^*, t, s)\{M \leftarrow cM_t\}$；
10. 结束
11. 返回 $(z_0, z_0^*)$

请注意，我们还可以将通过提示 $P$ 和随机种子 $s$ 生成的图像 $I$ 定义为附加输入。然而，算法仍将保持不变。有关编辑真实图像的内容，请参见第4节。另外，请注意，我们可以通过在扩散前向函数内部应用编辑函数来跳过第7行中的前向调用。此外，可以在同一批次中（即并行）对 $z_{t-1}$ 和 $z_t^*$ 应用扩散步骤，因此与原始扩散模型的推理相比，只有一个步骤的额外开销。

现在我们将转向处理具体的编辑操作，补充 $Edit(M_t, M_t^*, t)$ 函数的定义。概述见图3（下）。

#### 词语替换

在这种情况下，用户将原始提示中的标记替换为其他标记，例如，将 $P =$ “a big red bicycle” 替换为 $P^* =$ “a big red car”。**主要挑战在于既要保留原始构图**，又要处理新提示的内容。为此，我们将源图像的注意力图注入到使用修改后提示生成的过程中。然而，所提出的注意力注入可能会过度约束几何形状，尤其是在涉及较大结构修改（如“car”到“bicycle”）时。我们通过建议一种更柔和的注意力约束来解决这个问题：

$$
Edit(M_t, M_t^*, t) := \begin{cases} 
M_t^* & \text{if } t < \tau \\
M_t & \text{otherwise}
\end{cases}
$$

其中，**$\tau$ 是一个时间戳参数，决定了注入将应用到哪个步骤。请注意，构图是在扩散过程的早期步骤中确定的**。因此，通过限制注入步骤的数量，我们可以在引导新生成图像的构图的同时，为适应新提示提供必要的几何自由度。图示见第4节。我们算法的**另一种自然放松是为提示中的不同标记分配不同数量的注入时间戳**。如果两个词使用不同数量的标记表示，则可以根据需要使用对齐函数（如下一段所述）复制或平均这些图。

#### 添加新短语

在另一种情况下，用户在提示中添加新的标记，例如，将 $P =$ “a castle next to a river” 替换为 $P^* =$ “children drawing of a castle next to a river”。为了保留共同的细节，我们仅在两个提示中的共同标记上应用注意力注入。形式上，**我们使用一个对齐函数** $A$，该函数接收目标提示 $P^*$ 中的标记索引，并输出在 $P$ 中对应的标记索引，如果没有匹配项则输出 `None`。然后，编辑函数由以下公式给出：

$$
(Edit(M_t, M_t^*, t))_{i,j} := \begin{cases} 
(M_t^*)_{i,j} & \text{if } A(j) = \text{None} \\
(M_t)_{i,A(j)} & \text{otherwise}
\end{cases}
$$

回顾一下，索引 $i$ 对应于像素值，索引 $j$ 对应于文本标记。同样，我们可以设置一个时间戳 $\tau$ 来控制应用注入的扩散步骤的数量。这种编辑方式可以实现多种“提示到提示”的功能，如样式化、对象属性的指定或全局操作，如第4节中所示。

#### 注意力重新加权

最后，用户可能希望增强或减弱每个标记对生成图像的影响程度。例如，考虑提示 $P =$ “a fluffy red ball”，假设我们希望使球变得更加或较少蓬松。为了实现这种操作，我们使用参数 $c \in [-2, 2]$ 缩放分配给标记 $j^*$ 的注意力图，从而产生更强或更弱的效果。其余的注意力图保持不变。即：

$$
(Edit(M_t, M_t^*, t))_{i,j} := \begin{cases} 
c \cdot (M_t)_{i,j} & \text{if } j = j^* \\
(M_t)_{i,j} & \text{otherwise}
\end{cases}
$$

如第4节所述，参数 $c$ 允许对引发的效果进行精细且直观的控制。

## 4 应用

我们在第3节中描述的方法通过控制用户提供的提示中每个词对应的空间布局，实现了直观的仅文本编辑。在本节中，我们展示了使用该技术的几个应用。

### 仅文本的局部编辑

我们首先展示了通过修改用户提供的提示进行局部编辑，而不需要任何用户提供的掩码。在图2中，我们展示了一个使用提示“lemon cake”生成图像的示例。我们的方法允许在将词语“lemon”替换为“pumpkin”时保留空间布局、几何形状和语义（上排）。请注意，背景得到了很好的保留，包括左上角的柠檬变成了南瓜。另一方面，仅将提示“pumpkin cake”直接输入合成模型，即使在确定性设置（如DDIM [40]）中使用相同的随机种子，结果也会产生完全不同的几何结构（第3排）。我们的方法甚至在处理像“pasta cake”这样的挑战性提示时也能成功（第2排）——生成的蛋糕由意面层组成，顶部有番茄酱。另一个示例见图5，在该示例中，我们并未注入整个提示的注意力，**而仅注入了特定词语“butterfly”的注意力。这使得我们能够保留原始蝴蝶的同时更改其他内容**。附录中提供了更多结果（图13）。

**图5：对象保留。** 通过仅注入从左上角图像中获取的“butterfly”一词的注意力权重，我们可以在替换其背景的同时保留单个对象的结构和外观。注意，蝴蝶以非常合理的方式落在所有物体之上。

**图6：在不同数量的扩散步骤中注入注意力。** 在顶部，我们展示了源图像和提示。在每一行中，我们通过替换文本中的单个词语并在扩散步骤中注入源图像的交叉注意力图来修改图像内容，注入比例从0%（左侧）到100%（右侧）不等。请注意，一方面，如果不使用我们的方法，无法保证保留源图像的任何内容。**另一方面，在所有扩散步骤中注入交叉注意力可能会过度约束几何形状，导致对文本提示的低保真度，例如，车（第3行）在完全注入交叉注意力后变成了一辆自行车**。

如图6所示，我们的方法不仅限于修改纹理，还可以执行结构性修改，例如将“bicycle”更改为“car”。为了分析我们的注意力注入效果，在左栏中我们展示了没有交叉注意力注入的结果，单词的更改导致了完全不同的结果。自左向右，我们展示了随着扩散步骤的增加，注入注意力后生成的图像。注意，在更多的扩散步骤中应用交叉注意力注入时，图像对原始图像的保真度越高。然而，通过在所有扩散步骤中应用注入并不一定能达到最佳效果。因此，通过更改注入步骤的数量，我们可以为用户提供更好的原始图像保真度控制。

**图7：通过提示优化进行编辑。** 通过扩展初始提示的描述，我们可以对汽车进行局部编辑（上排）或全局修改（下排）。

除了替换一个词语，用户还可以希望为生成的图像添加新的说明。在这种情况下，我们保留原始提示的注意力图，同时允许生成器处理新添加的词语。例如，见图7（上），我们在“car”中添加了“crushed”，结果在原始图像的基础上生成了更多的细节，同时背景仍然保留。更多示例见附录（图14）。

### 全局编辑

保留图像构图不仅对局部编辑有价值，对于全局编辑也是一个重要的方面。在这种情况下，编辑应影响图像的所有部分，但仍应保留原始构图，例如对象的位置和身份。如图7（下）所示，我们在添加“snow”或更改光线时保留了图像内容。更多示例见图8，包括将草图转换为逼真的图像以及引入艺术风格。

**图8：图像风格化。** 通过在提示中添加风格描述并注入源注意力图，我们可以生成在新风格下保留原始图像结构的各种图像。

### 使用注意力重新加权的渐变控制

虽然通过编辑提示控制图像非常有效，但我们发现这仍然不能完全控制生成的图像。考虑提示“snowy mountain”。用户可能希望控制山上的积雪量。然而，通过文本描述所需的积雪量是相当困难的。相反，我们建议使用渐变控制 [24]，用户可以控制特定词语引发的效果的大小，如图9所示。正如第3节所述，我们通过重新缩放指定词语的注意力来实现这种控制。附录中提供了更多结果（图15）。

**图9：带有渐变控制的基于文本的图像编辑。** 通过减少（上排）或增加（下排）指定词语的交叉注意力（用箭头标记），我们可以控制其对生成图像的影响程度。

### 真实图像编辑

**编辑真实图像需要找到一个初始噪声向量，该向量在输入扩散过程中能够生成给定的输入图像。这个过程被称为反演，最近在GAN领域引起了相当大的关注，例如[51, 1, 3, 35, 50, 43, 45, 47]，但对于文本引导的扩散模型尚未得到充分解决。**

在下文中，我们展示了基于扩散模型的常见反演技术对真实图像进行初步编辑的结果。首先，一种较为简单的方法是向输入图像添加高斯噪声，然后执行预定义数量的扩散步骤。由于这种方法会导致显著的失真，我们采用了一种改进的反演方法[10, 40]，该方法基于确定性的DDIM模型，而不是DDPM模型。我们在反方向上执行扩散过程，即从 $x_0$ 到 $x_T$ 而不是从 $x_T$ 到 $x_0$，其中 $x_0$ 被设定为给定的真实图像。

**图10：真实图像编辑。** 左侧显示了使用DDIM [40] 采样的反演结果。我们在给定的真实图像和文本提示上初始化反向扩散过程。这会生成一个潜在噪声，当输入扩散过程时，会产生与输入图像近似的结果。随后，在右侧，我们应用“提示到提示”技术对图像进行编辑。

如图10所示，这种反演过程通常会产生令人满意的结果。然而，在许多其他情况下，反演的准确性并不充分，如图11所示。这部分是由于失真与可编辑性之间的权衡[43]，我们发现，降低无分类器引导[18]参数（即减少提示影响）可以改善重建效果，但会限制我们执行显著操作的能力。

**图11：反演失败案例。** 基于当前DDIM的真实图像反演可能会导致不理想的重建结果。

为缓解这一限制，我们建议**使用直接从注意力图中提取的掩码来恢复原始图像的未编辑区域**。请注意，这里的掩码是无需用户指导生成的。如图12所示，即使使用简单的DDPM反演方案（添加噪声然后去噪），这种方法也能很好地工作。注意，在各种编辑操作下，猫的身份得到了很好的保留，而掩码仅由提示本身生成。

**图12：基于掩码的编辑。** 当反演失真较大时，我们使用注意力图保留图像中未编辑的部分。这不需要任何用户提供的掩码，因为我们通过自己方法从模型中提取空间信息。注意，在编辑过程后，猫的身份得到了保留。

## 5 结论

在本研究中，我们揭示了文本到图像扩散模型中的交叉注意力层的强大能力。我们展示了这些高维层次具备可解释的空间图表示，在将文本提示中的词语与合成图像的空间布局联系起来方面起到了关键作用。基于这一观察，我们展示了如何通过对提示的各种操作直接控制合成图像中的属性，从而为包括局部和全局编辑在内的各种应用铺平了道路。这项工作是为用户提供简单直观的图像编辑手段的第一步，利用文本语义的强大力量。它使用户能够在语义文本空间中导航，每一步之后都会展示增量变化，而不是在每次文本操作后从头开始生成所需图像。

尽管我们已经通过更改文本提示展示了语义控制的能力，我们的技术仍然存在一些限制，需要在后续工作中加以解决。**首先，目前的反演过程在一些测试图像上会导致可见的失真**。此外，反演过程需要用户提出合适的提示，对于复杂的构图，这可能是一个挑战。需要注意的是，**针对文本引导扩散模型的反演挑战与我们的工作是正交的**，这将在未来进行深入研究。其次，当前的注意力图分辨率较低，因为交叉注意力位于网络的瓶颈处。这限制了我们进行更精确的局部编辑的能力。为了解决这一问题，我们建议在高分辨率层中也引入交叉注意力。我们将这项工作留待未来，因为这需要分析训练过程，超出了我们当前的研究范围。**最后，我们认识到当前的方法无法用于在图像中跨空间移动现有对象，这种控制也将留待未来的研究中进行探讨。**

## A 背景

### A.1 扩散模型

扩散去噪概率模型（DDPM）[39, 17] 是一种生成潜变量模型，其目标是建模一个分布 $p_\theta(x_0)$，该分布近似于数据分布 $q(x_0)$ 并易于采样。DDPMs 在 $x_0$ 空间中建模了从数据到噪声的“前向过程”。这个过程是一个从 $x_0$ 开始的马尔可夫链，我们逐渐向数据中添加噪声，以生成潜变量 $x_1, \ldots, x_T \in X$。因此，潜变量序列遵循 $q(x_1, \ldots, x_t \mid x_0) = \prod_{i=1}^t q(x_t \mid x_{t-1})$，其中前向过程中的一步被定义为高斯转移 $q(x_t \mid x_{t-1}) := N(x_t;\sqrt{1 - \beta_t}x_{t-1}, \beta_tI)$，其参数化由一个时间表 $\beta_0, \ldots, \beta_T \in (0, 1)$ 给出。当 $T$ 足够大时，最后的噪声向量 $x_T$ 几乎遵循各向同性高斯分布。

前向过程的一个有趣特性是，可以直接将潜变量 $x_t$ 表示为噪声和 $x_0$ 的以下线性组合，而无需采样中间的潜变量：

$$
x_t = \sqrt{\alpha_t}x_0 + \sqrt{1 - \alpha_t}w, \quad w \sim N(0, I),
$$

其中，$\alpha_t := \prod_{i=1}^t(1 - \beta_i)$。

为了从分布 $q(x_0)$ 采样，我们定义了从各向同性高斯噪声 $x_T$ 到数据的对偶“反向过程” $p(x_{t-1} \mid x_t)$，通过采样后验分布 $q(x_{t-1} \mid x_t)$ 来实现。由于难以处理的反向过程 $q(x_{t-1} \mid x_t)$ 依赖于未知的数据分布 $q(x_0)$，我们使用一个参数化的高斯转移网络 $p_\theta(x_{t-1} \mid x_t) := N(x_{t-1} \mid \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))$ 来近似它。$\mu_\theta(x_t, t)$ 可以通过使用公式2预测添加到 $x_0$ 的噪声 $\epsilon_\theta(x_t, t)$ 来替代 [17]。

在这个定义下，我们使用贝叶斯定理来近似：

$$
\mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \alpha_t}} \epsilon_\theta(x_t, t) \right)。
$$
一旦我们有了训练好的 $\epsilon_\theta(x_t, t)$，我们可以使用以下采样方法：

$$
x_{t-1} = \mu_\theta(x_t, t) + \sigma_t z, \quad z \sim N(0, I)。
$$

我们可以控制每个采样阶段的 $\sigma_t$，在DDIMs [40] 中，采样过程可以通过在所有步骤中使用 $\sigma_t = 0$ 来实现确定性。反向过程最终可以通过解决以下优化问题进行训练：

$$
\min_\theta L(\theta) := \min_\theta \mathbb{E}_{x_0 \sim q(x_0), w \sim N(0, I), t} \left\| w - \epsilon_\theta(x_t, t) \right\|_2^2,
$$

通过最大化变分下界，训练参数 $\theta$ 以拟合 $q(x_0)$。

### A.2 Imagen中的交叉注意力

Imagen [38] 由三个文本条件扩散模型组成：一个64 × 64的文本到图像模型，以及两个超分辨率模型——64 × 64 → 256 × 256 和 256 × 256 → 1024 × 1024。这些模型通过U型网络预测噪声 $\epsilon_\theta(z_t, c, t)$，$t$ 的范围从 $T$ 到 $1$，其中 $z_t$ 是潜向量，$c$ 是文本嵌入。我们强调这三个模型之间的区别：

- 64 × 64：从随机噪声开始，使用与[10]中相同的U-Net。该模型通过在分辨率为[16, 8]的交叉注意力层和分辨率为[32, 16, 8]的混合注意力层对文本嵌入进行条件处理，分别用于U-Net中的下采样和上采样过程。
- 64 × 64 → 256 × 256：基于简单上采样的64 × 64图像进行条件处理。使用了一个高效版本的U-Net，其中在瓶颈层（分辨率为32）包含混合注意力层。
- 256 × 256 → 1024 × 1024：基于简单上采样的256 × 256图像进行条件处理。使用了一个高效版本的U-Net，其中在瓶颈层（分辨率为64）仅包含交叉注意力层。

# 想法

- 图二中很明显的可以看出替换名词带来的影响是将图片中所有相关的元素都进行替换，但如果限定词有其他限定条件可能会出现将被限定的目标也替换的情况。