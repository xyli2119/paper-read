# Null-text Inversion for Editing Real Images using Guided Diffusion Models

## 摘要

近年来，文本引导的扩散模型提供了强大的图像生成能力。目前，人们正在大力研究如何仅使用文本来修改这些图像，从而提供直观且多功能的编辑方式。要使用这些最先进的工具编辑真实图像，**首先必须将图像与有意义的文本提示反演到预训练模型的域中**。在本文中，我们介绍了一种**精确的反演技术**，从而实现对图像的直观文本编辑。我们提出的反演技术包含两个关键的创新组件：(i) 针对扩散模型的关键反演。当前方法旨在将随机噪声样本映射到单个输入图像，**而我们为每个时间戳使用单个关键噪声向量并围绕它进行优化**。我们证明了直接反演本身并不充分，但它为我们的优化提供了良好的基础。(ii) 无文本优化。我们仅修改用于无分类器引导的无条件文本嵌入，而不是输入的文本嵌入。这允许保持模型权重和条件嵌入不变，从而能够应用基于提示的编辑，避免了对模型权重的繁琐调整。我们的无文本反演**基于公开的Stable Diffusion模型**，经过对各种图像和提示编辑的广泛评估，展示了对真实图像的高保真度编辑。

## 1. 引言

文本引导扩散模型在图像合成方面的进展因其出色的真实感和多样性而备受关注。大规模模型[27,30,32] 激发了众多用户的想象力，赋予了图像生成前所未有的创作自由。这自然引发了对如何利用这些强大模型进行图像编辑的持续研究。最近，基于文本的直观编辑在合成图像上得到了展示，使用户可以仅通过文本轻松操作图像[16]。

**然而，使用这些最先进的工具对真实图像进行文本引导编辑需要对给定图像和文本提示进行反演**。也就是说，需要找到一个初始噪声向量，使其在提示和扩散过程中输入时生成输入图像，同时保留模型的编辑能力。最近，反演过程在GANs [7,41] 中引起了相当大的关注，但对于文本引导的扩散模型，尚未得到充分解决。尽管已经为无条件扩散模型提出了一种有效的DDIM反演方案[13,35]，但当**应用对有意义编辑至关重要的无分类器引导[18]时**，该方案在文本引导的扩散模型中显得不足。

在本文中，我们介绍了一种有效的反演方案，能够实现接近完美的重建，同时保留原始模型丰富的文本引导编辑能力（见图1）。我们的方法基于对引导扩散模型的两个关键方面的分析：**无分类器引导和DDIM反演。**

**图1. 真实图像编辑的无文本反演。** 我们的方法将真实图像（最左列）和相关标题作为输入。图像通过DDIM扩散模型反演，生成扩散轨迹（左侧第二列）。一旦反演完成，**我们使用初始轨迹作为无文本优化的支点**，准确重建输入图像（左侧第三列）。然后，我们可以仅通过修改输入标题，使用Prompt-to-Prompt [16] 编辑技术对反演的图像进行编辑。

在广泛使用的**无分类器引导中**，每个扩散步骤都会执行两次预测：一次是无条件预测，一次是带有文本条件的预测。然后对这些预测进行外推，以增强文本引导的效果。虽然所有的研究都集中在条件预测上，但我们认识到无条件部分所产生的巨大影响。因此，我们对无条件部分使用的嵌入进行了优化，以反演输入图像和提示。我们将其称为**无文本优化**，因为我们用优化后的嵌入替换了空文本字符串的嵌入。

DDIM反演包括按照相反的顺序执行DDIM采样。虽然每一步都会引入轻微的误差，但在无条件情况下效果很好。然而，实际上，当应用于文本引导的合成时，**由于无分类器引导放大了累积误差**，导致该方法失效。我们发现它仍然可以为反演提供一个有前景的起点。受GAN文献的启发，**我们使用从初始DDIM反演获得的一系列带噪声的潜在代码作为支点[29]，然后围绕这个支点进行优化，以得到改进和更准确的反演。**我们将这种高度有效的优化称为**扩散关键反演**，这与现有的尝试将所有可能的噪声向量映射到单个图像的工作形成对比。

据我们所知，我们的方法是第一个能够在真实图像上启用Prompt-to-Prompt [16] 文本编辑技术的方法。此外，与最近的方法[19, 39]不同，我们不调整模型权重，因此避免了损害已训练模型的先验知识或为每个图像复制整个模型。通过全面的消融研究和比较，我们展示了我们关键组件对实现高保真度重建真实图像的贡献，同时提供了有意义和直观的编辑能力。我们的代码基于公开的Stable Diffusion模型构建，详情请访问我们项目页面：https://null-textinversion.github.io/。

## 2. 相关工作

大规模扩散模型，如Imagen [32]、DALL-E 2 [27] 和 Stable Diffusion [30]，最近在基于纯文本生成图像（即文本到图像合成）任务上提升了标准。利用扩散模型[17, 17, 30, 34–36]的强大架构，这些模型可以通过简单地输入相应的文本生成几乎任何图像，从而改变了艺术应用的格局。

然而，合成非常特定或个人化的对象（这些对象在训练数据中并不常见）一直是个挑战。这需要一种反演过程，**使得能够使用文本引导的扩散模型重新生成输入图像中显示的对象。反演在GANs中得到了广泛研究[7, 10, 22, 41, 42, 44]，包括基于潜变量的优化[1, 2]、编码器[28, 38]，以及特征空间编码器[40]和模型微调[3, 29]。**受此启发，Gal等人[15]提出了一种用于扩散模型的文本反演方案，该方案允许通过3−5张图像重新生成用户提供的概念。同时，Ruiz等人[31]通过模型微调解决了相同任务。然而，这些工作在准确再现未编辑部分的同时对给定的真实图像进行编辑时存在困难。

自然地，最近的研究尝试将文本引导的扩散模型应用于单图像编辑的基本挑战，旨在利用其丰富多样的语义知识。Meng等人[23]向输入图像添加噪声，然后从预定义的步骤执行文本引导的去噪过程。但他们在准确保留输入图像细节方面存在困难。为了解决这个问题，一些研究[4, 5, 25]假设用户提供一个掩码，以限制更改应用的区域，从而实现有意义的编辑和背景保留。

然而，要求用户提供精确的掩码是繁琐的。此外，掩码图像内容会移除重要信息，这些信息在修复过程中大多被忽略。一些仅基于文本的编辑方法局限于全局编辑[12, 20, 21]，而Bar-Tal等人[6]提出了一种无需掩码的基于文本的局部编辑技术。他们的技术允许高质量的纹理编辑，但不能修改复杂的结构，因为他们仅使用CLIP [26] 作为引导，而不是生成性扩散模型。

Hertz等人[16]提出了一种直观的编辑技术，称为**提示到提示**（Prompt-to-Prompt），通过修改文本提示来操纵局部或全局细节，适用于使用文本引导的扩散模型。通过注入内部的交叉注意力图，他们保留了空间布局和几何结构，使得在修改提示时能够重新生成图像。然而，由于没有反演技术，他们的方法仅限于合成图像。Sheynin等人[33]提出了一种无需反演的局部编辑模型训练方法，但其表现力和质量相比于当前的大规模扩散模型要逊色一些。同时，DiffEdit [9] 使用DDIM反演进行图像编辑，但通过自动生成掩码来避免失真问题，从而保留背景。

与我们的工作并行，Imagic [19] 和 UniTune [39] 使用强大的Imagen模型[32]展示了令人印象深刻的编辑结果。然而，它们都需要对模型进行限制性微调。此外，Imagic 每次编辑都需要重新微调，而 UniTune 对每张图像都需要参数搜索。我们的方法能够在真实图像上应用Prompt-to-Prompt [16] 的仅文本直观编辑。我们不需要任何微调，并且使用公开的Stable Diffusion [30] 模型实现了高质量的局部和全局修改。

## 3. 方法

设 $I$ 为真实图像。我们的目标是仅使用文本引导来编辑 $I$，从而得到编辑后的图像 $I^*$。我们采用Prompt-to-Prompt [16] 中定义的设置，其中编辑由源提示 $P$ 和编辑后的提示 $P^*$ 引导。这要求用户提供一个源提示。**然而，我们发现使用现成的图像描述模型[24] 自动生成源提示效果很好（见第4节）**。例如，如图2所示，给定一张图像和源提示“一个穿着……的婴儿”，我们通过提供编辑后的提示“一个穿着……的机器人”将婴儿替换为机器人。

**这种编辑操作首先需要将 $I$ 反演到模型的输出域。**也就是说，主要挑战是通过将源提示 $P$ 输入模型，忠实地重建 $I$，同时仍保留基于文本的直观编辑能力。

我们的方法基于两个主要观察结果。首先，当应用无分类器引导时，DDIM反演会产生不令人满意的重建结果，但它为优化提供了一个良好的起点，使我们能够高效地实现高保真度反演。**其次，优化无分类器引导中使用的无条件嵌入（null embedding），可以在不调整模型和条件嵌入的情况下实现精确的重建，从而保留所需的编辑能力**。

接下来，我们提供一个简短的背景介绍，并在第3.2节和第3.3节详细描述我们的方法。图3中提供了一个总体概述。

### 3.1. 背景与基础

文本引导的扩散模型旨在将随机噪声向量 $z_t$ 和文本条件 $P$ 映射到输出图像 $z_0$，该图像对应于给定的条件提示。为了进行序列去噪，网络 $\epsilon_\theta$ 被训练来预测人工噪声，其目标函数如下：

$$
\min_\theta \mathbb{E}_{z_0, \epsilon \sim N(0,I), t \sim \text{Uniform}(1,T)} \|\epsilon - \epsilon_\theta(z_t, t, C)\|_2^2。
$$

其中，$C = \psi(P)$ 是文本条件的嵌入，$z_t$ 是加噪后的样本，噪声根据时间戳 $t$ 添加到采样数据 $z_0$ 上。在推理过程中，给定噪声向量 $z_T$，噪声通过我们训练的网络在 $T$ 步中逐步预测并去除。

为了精确重建给定的真实图像，我们采用确定性的DDIM采样[35]：

$$
z_{t-1} = \sqrt{\frac{\alpha_{t-1}}{\alpha_t}} z_t + \left(\sqrt{\frac{1}{\alpha_{t-1}} - 1} - \sqrt{\frac{1}{\alpha_t} - 1}\right) \cdot \epsilon_\theta(z_t, t, C)。
$$

关于 $\alpha_t$ 的定义和更多细节，请参见附录E。扩散模型通常在图像像素空间中运行，其中 $z_0$ 是真实图像的一个样本。在我们的案例中，我们使用流行且公开的Stable Diffusion模型[30]，其中扩散前向过程应用于潜在图像编码 $z_0 = E(x_0)$，并在扩散反向过程结束时通过图像解码器恢复图像 $x_0 = D(z_0)$。

**无分类器引导**。**文本引导生成中的一个关键挑战是放大由条件文本引发的效果**。为此，Ho等人[18]提出了无分类器引导技术，其中预测也以无条件方式进行，然后与条件预测进行外推。更正式地，令 $\emptyset = \psi("")$ 为空文本的嵌入，$w$ 为引导比例参数，则无分类器引导的预测定义为：
$$
\tilde{\epsilon}_\theta(z_t, t, C, \emptyset) = w \cdot \epsilon_\theta(z_t, t, C) + (1 - w) \cdot \epsilon_\theta(z_t, t, \emptyset)。
$$

例如，$w = 7.5$ 是Stable Diffusion的默认参数。

**DDIM反演**。一种简单的反演技术被建议用于DDIM采样[13, 35]，基于小步长情况下ODE过程可以逆转的假设：

$$
z_{t+1} = \sqrt{\frac{\alpha_{t+1}}{\alpha_t}} z_t + \left(\sqrt{\frac{1}{\alpha_{t+1}} - 1} - \sqrt{\frac{1}{\alpha_t} - 1}\right) \cdot \epsilon_\theta(z_t, t, C)。
$$

换句话说，扩散过程以相反的方向执行，即 $z_0 \rightarrow z_T$ 而不是 $z_T \rightarrow z_0$，其中 $z_0$ 被设置为给定真实图像的编码。

### **3.2. 关键反演**

最近的反演工作在优化的每次迭代中使用随机噪声向量，旨在将每个噪声向量映射到单个图像上。我们观察到这效率不高，因为推理只需要一个噪声向量。受GAN文献[29]的启发，我们寻求执行一种更“局部”的优化，理想情况下仅使用单个噪声向量。特别是，我们的目标是围绕一个关键的噪声向量进行优化，该向量是一个良好的近似值，从而允许更高效的反演。

我们从研究DDIM反演开始。实际上，每一步都引入了轻微的误差。对于无条件扩散模型，累积误差可以忽略不计，DDIM反演是成功的。然而，请注意，使用Stable Diffusion模型[30] 进行有意义的编辑需要应用无分类器引导，并且需要较大的引导比例 $w > 1$。我们观察到，这样的引导比例放大了累积误差。因此，使用无分类器引导执行DDIM反演不仅会导致视觉伪影，而且获得的噪声向量可能会偏离高斯分布。后者会降低编辑能力，即使用特定噪声向量进行编辑的能力。

我们认识到，使用引导比例 $w = 1$ 的DDIM反演可以粗略地近似原始图像，具有很高的可编辑性，但并不准确。更具体地说，反向DDIM在图像编码 $z_0$ 和高斯噪声向量 $z_T^*$ 之间生成了一个 $T$ 步的轨迹。同样，较大的引导比例对于编辑至关重要。因此，我们专注于将 $z_T^*$ 输入带有无分类器引导的扩散过程中（$w > 1$）。这虽然产生了高可编辑性，但重建不准确，因为中间潜在代码偏离了轨迹，如图3所示。

**图3. 无文本反演概述。** 上部：关键反演。我们首先对输入图像应用初始DDIM反演，估计扩散轨迹 $\{z_t^*\}_{t=0}^T$。从最后的潜在向量 $z_T^*$ 开始扩散过程会导致不理想的重建结果，因为潜在代码逐渐偏离原始轨迹。**我们使用初始轨迹作为优化的支点，将扩散的反向轨迹 $\{\bar{z}_t\}_{t=1}^T$ 拉近到原始图像编码 $z_0^*$。**下部：针对时间戳 $t$ 的无文本优化。回顾无分类器引导，其由两次预测组成——一次使用文本条件嵌入，另一次无条件使用无文本嵌入 $\emptyset$（左下角）。然后，通过引导比例 $w$ 进行外推（中间）。我们仅优化无条件嵌入 $\emptyset_t$，通过在预测的潜在代码 $z_{t-1}$ 和支点 $z_{t-1}^*$ 之间使用重建MSE损失（红色部分）。

关于DDIM反演中不同引导比例值的分析见附录B（图9）。

鉴于其高可编辑性，我们将这个初始的**DDIM反演（$w = 1$）称为我们的关键轨迹**，并使用标准引导比例 $w > 1$ 进行优化。也就是说，我们的优化最大化了与原始图像的相似性，同时保持了执行有意义编辑的能力。实际上，我们在扩散过程中的每个时间戳 $t = T \rightarrow t = 1$ 上单独执行优化，目标是尽可能接近初始轨迹 $z_T^*, \dots, z_0^*$：

$$
\min \| z_{t-1}^* - z_{t-1} \|_2^2,
$$

其中 $z_{t-1}$ 是优化的中间结果。

由于我们的关键DDIM反演提供了一个较好的起点，与使用随机噪声向量相比，这种优化效率更高，如第4节所示。请注意，对于每个 $t < T$，优化应从上一步（$t + 1$）优化的终点开始，否则我们优化后的轨迹在推理时无法保持。因此，在步骤 $t$ 优化后，我们计算当前的带噪潜在变量 $\bar{z}_t$，然后在下一步优化中使用它，以确保新的轨迹最终接近 $z_0$（更多细节见公式(3)）。

### 3.3. 无文本优化

为了成功将真实图像反演到模型域中，最近的工作优化了文本编码[15]、网络权重[31, 39]，或两者兼顾[19]。为每张图像微调模型权重涉及复制整个模型，这在内存消耗方面极为低效。此外，除非对每次编辑都进行微调，否则必然会损害模型的学习先验，从而影响编辑的语义。直接优化文本嵌入会导致不可解释的表示，因为优化后的标记不一定与现有词语匹配。因此，直观的提示到提示编辑变得更加困难。

相反，我们利用了无分类器引导的关键特性——结果受到无条件预测的高度影响。因此，我们将默认的无文本嵌入替换为优化后的嵌入，这被称为无文本优化。也就是说，对于每个输入图像，我们仅优化无条件嵌入 $\emptyset$，并以无文本嵌入作为初始值。模型和条件文本嵌入保持不变。

这种方法实现了高质量的重建，同时仍然允许通过优化后的无条件嵌入进行直观的Prompt-to-Prompt [16] 编辑。此外，经过一次反演过程后，优化后的无条件嵌入可以用于输入图像的多次编辑操作。由于无文本优化本质上比微调整个模型的表现力要低，因此它需要更高效的关键反演方案。

我们将优化单个无条件嵌入 $\emptyset$ 称为**全局无文本优化**。在实验过程中（如图4所示），我们观察到针对每个时间戳 $t$ 优化不同的“无文本嵌入” $\emptyset_t$ 显著提高了重建质量，这与我们的关键反演非常契合。因此，我们对每个时间戳使用无条件嵌入 $\{\emptyset_t\}_{t=1}^T$，并将 $\emptyset_t$ 初始化为前一步的嵌入 $\emptyset_{t+1}$。

**图 4. 消融研究**。顶部：我们将完整算法（绿色线）的性能与不同变体进行比较，评估通过优化迭代次数和运行时间（分钟）衡量的重建质量，使用 PSNR 得分作为指标。底部：我们在右侧展示了完整算法在 200 次迭代后的反演结果，并与其他基线方法进行视觉比较。所有迭代的结果请参见附录 B（图 13 和图 14）。

结合这两个组件，我们的完整算法如算法1所示。

**算法 1: 无文本反演**

1. **输入**: 源提示嵌入 $C = \psi(P)$ 和输入图像 $I$。
2. **输出**: 噪声向量 $z_T$ 和优化后的嵌入 $\{\emptyset_t\}_{t=1}^T$。
3. 设置引导比例 $w = 1$；
4. 使用DDIM反演在 $I$ 上计算中间结果 $z_T^*, \dots, z_0^*$；
5. 设置引导比例 $w = 7.5$；
6. 初始化 $z_T \leftarrow z_T^*$，$\emptyset_T \leftarrow \psi("")$；
7. 对于 $t = T, T - 1, \dots, 1$，执行以下操作：
   8. 对于 $j = 0, \dots, N - 1$，执行以下操作：
      9. $\emptyset_t \leftarrow \emptyset_t - \eta \nabla_{\emptyset} \| z_{t-1}^* - z_{t-1}(\bar{z}_t, \emptyset_t, C) \|_2^2$；
   10. 结束循环；
   11. 设置 $\bar{z}_{t-1} \leftarrow z_{t-1}(\bar{z}_t, \emptyset_t, C)$，$\emptyset_{t-1} \leftarrow \emptyset_t$；
12. 结束循环；
13. 返回 $z_T$ 和 $\{\emptyset_t\}_{t=1}^T$。

$w = 1$ 的DDIM反演输出一系列带噪潜在代码 $z_T^*, \dots, z_0^*$，其中 $z_0^* = z_0$。我们初始化 $\bar{z}_T = z_t$，并在时间戳 $t = T, \dots, 1$ 上以默认的引导比例 $w = 7.5$ 进行以下优化，每个时间戳迭代 $N$ 次：
$$
\min_{\emptyset_t} \| z_{t-1}^* - z_{t-1}(\bar{z}_t, \emptyset_t, C) \|_2^2。
$$

为了简化表示，$z_{t-1}(\bar{z}_t, \emptyset_t, C)$ 表示使用 $\bar{z}_t$、无条件嵌入 $\emptyset_t$ 和条件嵌入 $C$ 应用DDIM采样步骤。在每一步结束时，我们更新：

$$
\bar{z}_{t-1} = z_{t-1}(\bar{z}_t, \emptyset_t, C)。
$$

我们发现，早停可以减少时间消耗，在单个A100 GPU上耗时约1分钟。

最后，我们可以使用噪声 $\bar{z}_T = z_T^*$ 和优化后的无条件嵌入 $\{\emptyset_t\}_{t=1}^T$ 对真实输入图像进行编辑。有关更多实现细节，请参见附录D。

## 4. 消融研究

在本节中，我们验证了主要组件的贡献，通过消融研究全面分析了设计选择的有效性。我们重点关注输入图像的保真度，这是图像编辑的一个重要评估指标。在第5节中，我们展示了我们的方法在实现高质量和有意义的操作方面的表现。

### 实验设置

评估结果如图4所示。我们使用了从COCO验证集[8]中随机选择的100对图像和标题对。然后，我们对每对图像-标题应用我们的方法，使用默认的Stable Diffusion超参数，每个扩散步骤的迭代次数从 $N = 1, \dots, 20$ 递增（见算法1）。重建质量通过平均PSNR进行测量。接下来，我们分析算法的不同变体。

### DDIM反演

我们将DDIM反演视为算法的下限，因为它是我们优化的起点，在应用无分类器引导时会产生不理想的重建结果（见第3.2节）。

### VQAE

作为上限，我们考虑使用Stable Diffusion模型中VQ自动编码器[14]（VQAE）进行的重建。虽然潜在代码或VQ解码器可以根据输入图像进一步优化，但这超出了我们的范围，因为这只适用于特定模型[30]，而我们的目标是通用算法。因此，我们的优化将其编码 $z_0$ 视为真值，因为在大多数情况下，获得的重建是非常准确的。

### 我们的方法

如图4所示，我们的方法在总共500次迭代后（$N = 10$）收敛到接近VQAE上限的最优重建，即使在250次迭代后（在A100 GPU上耗时约1分钟），我们也能实现高质量的反演。

### 随机支点

我们通过将基于DDIM的轨迹替换为潜在代码的随机轨迹来验证DDIM初始化的重要性，两个轨迹共享相同的起点 $z_0$（即输入图像编码）。换句话说，我们对每张图像随机采样一个高斯噪声 $\sim N(0, I)$，并使用扩散调度器将其加噪到相应的编码 $z_0$ 上，时间戳从 $t = 1$ 到 $t = T$。如图4所示，DDIM初始化对于快速收敛至关重要，因为当支点随机时，初始误差显著增加。

### 对不同输入标题的鲁棒性

由于我们需要输入标题，因此很自然地会问我们的方法是否对所选标题高度敏感。我们通过为每张图像从数据集中随机抽取一个标题，将这一问题推至极限。即使标题不匹配，优化也会收敛到VQAE的最优重构。因此，我们得出结论，反演对输入标题具有鲁棒性。显然，随机选择标题不适合基于文本的编辑，但提供任何合理且可编辑的提示都可以使用，包括使用现成的图像描述模型[24,37]。这在附录B（图12）中得到了说明。我们使用多个标题对一幅图像进行反演，展示了编辑部分应包含在源标题中，以生成用于编辑的语义注意力图。例如，要编辑衬衫上的图案，源标题应包括“带有图案的衬衫”或类似短语。

### 全局无文本嵌入

我们将为所有时间戳优化单个嵌入 $\emptyset$ 称为全局嵌入。可以看出，这种优化难以收敛，因为它的表现力比我们最终使用的每时间戳嵌入 $\{\emptyset_t\}_{t=1}^T$ 低。更多实现细节见附录D。

### 文本反演

我们将我们的方法与类似于Gal等人[15]提出的方法的文本反演进行比较。我们使用随机噪声样本而不是关键反演优化文本嵌入 $C = \psi(P)$。也就是说，我们为每次优化步骤随机采样不同的高斯噪声，并根据扩散调度器将其添加到 $z_0$ 上获得 $z_t$。直观地说，该目标旨在将所有噪声向量映射到单个图像，而我们的关键反演则专注于单个噪声向量轨迹。优化目标定义为：

$$
\min_C \mathbb{E}_{z_0, \epsilon \sim N(0, I), t} \|\epsilon - \epsilon_\theta(z_t, t, C)\|_2^2。
$$

请注意，Gal等人[15]的目标是再生成特定对象，而不是实现准确的反演。如图4所示，收敛速度远比我们的方法慢，且重建质量较差。

### 使用支点的文本反演

我们观察到，使用关键反演结合文本反演显著提高了重建质量，达到了与我们方法相当的重建效果。这进一步展示了使用支点进行优化的强大之处。然而，我们发现与无文本优化相比，编辑能力有所下降。特别是，如附录B（图15）所示，注意力图的精确度较差，降低了Prompt-to-Prompt编辑的性能。

### 无关键反演的无文本优化

我们发现，使用随机噪声向量而不是关键反演来优化无条件无文本嵌入会完全破坏无文本优化。结果甚至比DDIM反演基线还差，如附录B（图13和图14）所示。我们推测无文本优化的表现力不如模型微调，因此依赖于高效的关键反演，因为它难以将所有噪声向量映射到单个图像。

## 5. 结果

真实图像编辑展示在图1、图2和图5中，展示了我们的方法不仅达到了显著的重建质量，还保留了高度的可编辑性。特别是，我们使用了Prompt-to-Prompt [16]的直观方法，证明了此前仅限于合成图像的编辑能力现在可以通过我们的反演技术应用于真实图像。

如图2所示，我们的方法有效地修改了纹理（例如“花卉衬衫”）和结构化对象（例如从“婴儿”到“机器人”）。由于我们支持Prompt-to-Prompt的局部编辑并实现了高保真度的重建，原始身份得到了很好的保留，即使是在处理婴儿脸这种具有挑战性的情况下。图2还说明，我们的方法只需要一次反演过程即可执行多次编辑操作。通过一次反演过程，我们可以修改头发颜色、眼镜、表情、背景和光线，甚至可以替换物体或为人脸添加小丑妆（底部几行）。使用Prompt-to-Prompt，我们还可以在真实图像上减弱或增强特定词语的影响，如图5所示。更多示例请参见附录C。

我们的高保真重建的视觉结果展示在图1、图8和附录C（图16）中，支持了第4节中的量化测量结果。

### 5.1. 对比

我们的方法侧重于仅使用文本进行直观编辑，因此我们将其与其他仅使用文本的编辑方法进行比较：(1) VQGAN+CLIP [11]，(2) Text2Live [6]，和(3) SDEedit [23]。我们使用了Bar-Tal等人[6]使用的图像，并从网上收集了包含更多结构化对象（如人类和动物）的照片。总共使用了100个图像、输入标题和编辑标题样本。

我们还将我们的方法与以下基于掩码的方法进行比较：(4) Glide [25]，(5) Blended-Diffusion [5]，以及(6) Stable Diffusion Inpaint [30]。后者通过使用目标提示对掩码区域进行修补，从而对扩散模型进行微调，允许简单的掩码区域编辑。

最后，我们考虑并行工作的(7) Imagic [19]，它对每次编辑操作都进行了模型微调，并为Imagen模型[32]设计。由于没有可用的实现，我们未将其与并行工作的Unitune [39] 和 DiffEdit [9] 进行比较。

**定性对比**。如图6所示，VQGAN+CLIP [11] 生成的结果大多不够真实。Text2LIVE [6] 在处理纹理修改时表现不错，但在操作更结构化的对象时失败，例如放置船只（第3排）。这两者都由于使用了CLIP [26]，其缺乏生成能力而受限。SDEdit [23] 中，带噪图像被输入扩散过程中的中间步骤，因此它在重建原始细节时表现不佳。这导致在涉及细节的情况下出现严重的伪影，例如在人脸中身份漂移（第1排），以及背景未得到良好保留（第2排）。相反，我们的方法成功保留了原始细节，同时允许从简单纹理到替换结构化对象的广泛且有意义的编辑。

图7展示了与基于掩码的方法的对比，显示这些方法在保留掩码区域内的细节方面存在困难。这是由于掩码程序会移除重要的结构信息，因此某些功能超出了修补的能力范围。

对Imagic [19] 的比较展示在附录B（图17）中，该方法在不同的设置下运行——每次编辑操作都需要模型微调。我们首先为Stable Diffusion使用了非官方的Imagic实现，并展示了不同插值参数 $\alpha = 0.6, 0.7, 0.8, 0.9$ 的结果。该参数用于在目标文本嵌入和优化后的嵌入之间进行插值[19]。此外，Imagic作者使用Imagen模型在相同图像上应用了他们的方法，使用的参数为 $\alpha = 0.93, 0.86, 1.08$。如图所示，Imagic 在编辑方面表现出高度的意义，特别是当使用Imagen模型时。然而，Imagic 在保留原始细节方面表现不佳，例如婴儿的身份（第1排）或背景中的杯子（第2排）。此外，我们观察到Imagic 对插值参数 $\alpha$ 的敏感性较强，较高值降低了图像的保真度，较低值则降低了文本引导的保真度，而单一值无法应用于所有示例。最后，Imagic 的推理时间较长，详见附录C（表2）。

**量化对比**。由于文本编辑真实图像的地面真实数据不可用，量化评估仍然是一个开放的挑战。与[6, 16]类似，我们在表1中展示了一项用户研究。50名参与者对每个基线总共48张图像进行了评分。参与者通过Prolific (prolific.co) 招募。我们随机展示了由VQGAN+CLIP、Text2LIVE、SDEdit和我们的方法生成的图像，并让参与者选择更好地应用所请求编辑且保留大部分原始细节的方法。附录F（图18）中提供了截图。正如表1所示，大多数参与者更喜欢我们的方法。

与Imagic的量化比较展示在附录B（图11）中，使用非官方的Stable Diffusion实现。根据这些指标，我们的方法在LPIPS感知距离方面得分更高，表明其对输入图像的保真度更好。

### 5.2. 评估其他编辑技术

大多数展示的结果都使用了Prompt-to-Prompt [16] 编辑技术来应用我们的方法。然而，我们证明了我们的方法不仅限于特定的编辑方法，还通过改进SDEdit [23] 编辑技术展示了其通用性。

如图8（顶部）所示，我们使用LPIPS感知距离[43]（数值越低越好）来衡量对原始图像的保真度，并使用CLIP相似度[26]（数值越高越好）来衡量对目标文本的保真度，样本数为100。我们使用不同的SDEdit参数 $t_0$（标记在曲线上），即我们从不同的 $t = t_0 \cdot T$ 开始扩散过程，使用相应加噪的输入图像。该参数控制了输入图像保真度（低 $t_0$）与文本对齐度（高 $t_0$）之间的权衡。我们比较了标准SDEdit与先应用我们的反演后进行SDEdit，并用我们优化后的嵌入替换无文本嵌入的结果。如图所示，我们的反演显著提高了对输入图像的保真度。

这在图8（底部）中得到了视觉上的展示。由于参数 $t_0$ 控制了重建与可编辑性之间的权衡，我们为每种方法（SDEdit有无反演）使用了不同的参数，以便两者都达到相同的CLIP分数。如图所示，使用我们的方法时，婴儿的真实身份得到了很好的保留。

## 6. 限制

尽管我们的方法在大多数场景中表现良好，但仍然存在一些局限性。最显著的一个是推理时间。我们的方法需要大约一分钟的GPU时间来反演单个图像。之后，可以进行无限次编辑操作，每次仅需十秒。这对于实时应用程序来说仍然不够。此外，其他限制来自于使用Stable Diffusion [30] 和Prompt-to-Prompt 编辑[16]。首先，VQ自动编码器在某些情况下会产生伪影，尤其是在处理人脸时。由于这特定于Stable Diffusion，因此我们认为VQ解码器的优化不在本文范围内，我们的目标是一个通用框架。其次，我们观察到，Stable Diffusion生成的注意力图比Imagen [32] 的注意力图准确度低，即词语可能与错误的区域相关，这表明其基于文本的编辑能力较弱。最后，复杂的结构修改对于Prompt-to-Prompt来说难以实现，例如将坐着的狗变成站立的狗[19]。我们的反演方法与具体模型和编辑技术是正交的，我们相信这些问题将在不久的将来得到改进。

## 7. 结论

我们提出了一种方法，将真实图像及其对应的标题反演到文本引导的扩散模型的潜在空间，同时保留其强大的编辑能力。我们的两步方法首先使用DDIM反演计算一系列带噪代码，这些代码大致近似原始图像（基于给定的标题），然后使用该序列作为固定支点来优化输入的无文本嵌入。其精细的优化弥补了由无分类器引导组件导致的不可避免的重建误差。一旦图像-标题对被准确嵌入到模型的输出域中，就可以在推理时即时应用Prompt-to-Prompt编辑。

通过向文本引导的扩散模型引入两个新的技术概念——关键反演和无文本优化，我们成功缩小了重建与可编辑性之间的差距。我们的方法提供了一种出乎意料的简单且紧凑的方式来重建任意图像，避免了计算量大的模型微调。我们相信，无文本反演为基于文本的直观图像编辑的现实应用场景铺平了道路。