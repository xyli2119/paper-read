# A Latent Space of Stochastic Diffusion Models for Zero-Shot Image Editing and Guidance

## 摘要
扩散模型通过迭代去噪生成图像。最近的研究表明，通过使去噪过程变为确定性，可以将真实图像编码为相同大小的潜在代码，这些代码可用于图像编辑。**本文探讨了即使在去噪过程保持随机性的情况下定义潜在空间的可能性**。回顾一下，在随机扩散模型中，每一步去噪都会添加高斯噪声，我们可以将所有噪声连接起来形成一个潜在代码。这导致了一个比原始图像高得多维度的潜在空间。我们展示了这种随机扩散模型的潜在空间可以像确定性扩散模型的潜在空间一样应用于两个场景。首先，我们提出了**CycleDiffusion**，一种使用随机扩散模型进行零样本和无配对图像编辑的方法，其性能优于确定性扩散模型的对应方法。其次，我们展示了在确定性和随机扩散模型的潜在空间中实现统一的、即插即用的指导。

## 1. 引言
扩散概率模型（DPMs）[30, 9] 在生成建模中取得了前所未有的成果，并且在诸如 DALL·E-2 [24] 等文本生成图像模型中起到了关键作用。在 DPMs 中，图像通过迭代去噪生成。在最初的形式化中，去噪过程是随机的，每一步去噪都会添加高斯噪声。这种随机形式使得 DPMs 与之前的模型，如变分自编码器（VAEs）[15]、生成对抗网络（GANs）[6] 和归一化流（normalizing flows）[14] 不同，在这些模型中，生成过程可以反转以从真实图像中获得潜在代码。

不同于随机形式，以前的研究[31, 29] 表明，每个随机DPM都有一个基于常微分方程（ODE）的确定性对应模型，并具有相同的输出分布。基于ODE的确定性DPM的一个重要优点是去噪过程可以被反转。也就是说，给定一个图像，可以获得一个与图像大小相同的潜在代码，该代码可以通过去噪重构图像。这个特性使得确定性DPM成为图像编辑的有力候选者[32]，在这种情况下，可以使用一个模型（或条件）将图像编码为潜在代码，并使用另一个模型（或条件）解码以获得新图像。

在本文中，我们展示了：（1）随机DPMs也可以有潜在空间，（2）真实图像可以被编码到这个潜在空间中，（3）这个潜在空间可以与确定性DPMs的潜在空间以相同方式使用。

为了定义潜在空间，回顾一下，在随机DPMs中，每一步去噪都会添加高斯噪声，我们可以将所有噪声连接起来形成一个潜在代码。这导致了一个比原始图像高得多维度的潜在空间。为了进行编码，我们提出了DPM编码器（DPM-Encoder），**这是一个用于将真实图像编码到随机DPM潜在空间的算法**。DPM-Encoder基于这样一个事实：向真实图像添加噪声的过程（即前向过程）是预定义的。因此，我们可以从前向过程采样连续的噪声图像，并通过定义计算去噪过程中使用的噪声。

我们将展示随机DPMs的潜在空间可以在两种情况下与确定性DPMs的潜在空间以相同方式使用。首先，以前的研究[32, 7] 已经表明确定性DPMs可用于图像编辑。给定两个使用相同噪声调度训练的确定性DPMs，一个可以使用源域模型将源图像编码为潜在代码，并使用目标域模型解码以获得目标图像[32]。给定一个确定性的文本到图像的扩散模型，一个可以在源文本条件下将图像编码为潜在代码，并在目标文本条件下解码以获得新图像[7]。基于这些工作，我们提出了CycleDiffusion，这是一种将相同思想从确定性DPMs扩展到随机DPMs的方法。我们的实验表明，CycleDiffusion在无配对图像编辑（第4.1节）和零样本图像编辑（第4.2节）中优于之前的方法。

其次，我们展示了确定性和随机DPMs都可以通过基于能量的模型[18, 20, 35]以即插即用的方式进行引导。这样的即插即用引导此前被提出用于从预训练的GANs中采样受控分布。值得注意的是，不同于分类器引导[5]，即插即用引导不需要对噪声图像上的引导模型进行微调。我们的实验表明，即插即用引导可以从确定性和随机DPMs中采样受控且多样的图像（第4.3节）。

## 2. 相关工作
### 图像生成的扩散模型
扩散概率模型（DPMs）[30, 9] 是一类通过迭代去噪生成图像的生成模型，已经成为诸如 DALL·E-2 [24] 和 Stable Diffusion [25] 等文本生成图像合成的基础。扩散模型训练完成后，可以使用不同的去噪过程生成图像，理论上会产生相同的边际输出分布。去噪过程可以分为两种类型：随机和确定性。在随机DPMs中[30, 9]，每一步去噪都会添加高斯噪声，而在确定性DPMs中[31, 29]，去噪过程是从潜在代码到图像的确定性映射。确定性DPMs的一个优点是，基于其ODE形式化，去噪过程可以被反转[31, 29]。反转后的潜在代码可以用于诸如真实图像编辑等应用[32]。在本文中，我们展示了可以为随机DPMs定义潜在空间，该空间可以与确定性DPMs的潜在空间以相同方式使用。

### 使用扩散模型进行图像编辑
最近的研究表明，扩散模型具备图像编辑的能力[17, 32, 3, 7]。其中，[32, 7, 13] 基于固定随机种子可以生成变化最小的图像这一观察。然而，为了编辑真实图像，他们需要采用基于ODE的确定性DPMs，将图像反转到潜在空间，这不适用于随机DPMs。据我们所知，我们的工作是首次展示随机DPMs也可以用于图像编辑。此外，[7] 展示了在基于Transformer的文本生成图像扩散模型中，固定交叉注意力图可以进一步保留图像结构；我们展示了这同样可以应用于随机DPMs。

### 扩散模型的引导
扩散模型预训练后，可以通过额外输入进行引导，以生成具有特定属性的图像，如类别标签[5]、文本[19] 和受损图像[12]。引导方法如分类器引导[5] 和CLIP引导[19, 16] 需要在所有噪声级别上对分类器和CLIP进行训练或微调，这意味着引导不是即插即用的。分类器无引导（CFG）[10] 不需要微调分类器或CLIP，但仅适用于已经是条件模型的情况。在这项工作中，我们展示了潜在空间形式化允许确定性和随机DPMs以即插即用的方式进行引导，与之前基于GANs的工作相同[18, 20, 35]。

## 3. 方法
### 3.1. 扩散模型
扩散概率模型（DPMs）[9, 30] 通过迭代去噪生成图像。提出了两种类型的扩散模型：随机DPMs和确定性DPMs。随机DPMs [9, 30, 29] 通过马尔可夫链结构生成图像。给定均值估计器 $\mu_T$（通常由UNet参数化），图像 $x := x_0$ 通过以下过程生成：
$$
x_T \sim N(0,I), \\
x_{t-1} = \mu_T(x_t,t) + \sigma_t \odot \epsilon_t, \\
\epsilon_t \sim N(0,I), t = T,\dots,1. \tag{1}
$$

确定性DPMs [29, 31, 11] 通过ODE形式化生成图像，这消除了去噪过程中的随机性。给定去噪方向的估计器 $\mu_T$（通常由UNet参数化），确定性DPMs通过以下方式生成 $x := x_0$：
$$
x_T \sim N(0,I), \\
x_{t-1} = \mu_T(x_t,t), t = T,\dots,1. \tag{2}
$$

### 3.2. 扩散模型的高斯潜在空间
给定 $x_T$，确定性DPM从潜在代码 $z$ 到图像 $x$ 的映射是确定的，即可以视为一个从潜在代码 $z$ 到图像 $x$ 的确定性映射 $G: \mathbb{R}^d \rightarrow X$。特别地，基于公式（2），潜在代码 $z$ 和映射 $G$ 可以定义为：
$$
z := x_T \sim N(0,I), \\
x_{t-1} = \mu_T(x_t,t), t = T,\dots,1. \tag{3}
$$

在本文中，我们展示了随机DPMs也可以被公式化为从潜在代码 $z$ 到图像 $x$ 的确定性映射。特别地，基于公式（1），我们将潜在代码 $z$ 和映射 $G$ 定义为：
$$
z := x_T \oplus \epsilon_T \oplus \dots \oplus \epsilon_1 \sim N(0,I), \\
x_{t-1} = \mu_T(x_t,t) + \sigma_t \odot \epsilon_t, t = T,\dots,1. \tag{4}
$$

图2展示了这两种类型的扩散模型概览。确定性DPMs潜在空间的一个优点是可以轻松反转映射 $G$，通过ODE形式从图像 $x$ 获取潜在代码 $z$，这使得确定性DPMs可以用于图像编辑应用[32, 7]。接下来，我们展示了如何使用随机DPMs从图像 $x$ 采样潜在代码 $z$。

### 3.3. DPM编码器
为了使用随机DPMs从图像 $x$ 采样潜在代码 $z$，我们提出了DPM编码器（DPM-Encoder）。给定一个预训练的随机DPM，DPM-Encoder不需要任何额外的训练，而是通过定义进行潜在代码采样。回顾一下，对于每个图像 $x := x_0$，随机DPMs定义了后验分布 $q(x_{1:T} | x_0)$ [9, 29]。基于 $q(x_{1:T} | x_0)$ 和公式（4），我们可以直接推导出 $z \sim \text{DPMEnc}(z|x,G)$，如下所示（详见附录A）：
$$
x_1,\dots,x_{T-1},x_T \sim q(x_{1:T} | x_0), \\
\epsilon_t = \frac{x_{t-1} - \mu_T(x_t,t)}{\sigma_t}, t = T,\dots,1, \\
z := x_T \oplus \epsilon_T \oplus \dots \oplus \epsilon_2 \oplus \epsilon_1. \tag{5}
$$

DPM-Encoder的一个特性是完美重建，即对于每个 $z \sim \text{Enc}(z|x,G)$，都有 $x = G(z)$。通过归纳法证明详见附录A。

### 3.4. 应用1：使用CycleDiffusion编辑图像
给定两个建模两个分布 $D_1$ 和 $D_2$ 的随机DPMs $G_1$ 和 $G_2$，许多研究人员和从业者发现，使用相同的“随机种子”采样会生成相似的图像[19]。基于这一发现，之前的研究[32, 7] 提出了使用确定性DPMs进行图像编辑。具体来说，可以使用ODE形式化反转映射 $G$，从图像 $x$ 获取潜在代码 $z$，然后使用 $z$ 生成新图像 $\hat{x} = G(z)$。编码和解码过程使用不同的模型检查点，或基于不同的文本提示进行条件解码。

**算法1：CycleDiffusion用于零样本图像编辑，使用文本引导的扩散模型**

**输入**: 源图像 $x := x_0$；源文本 $t$；目标文本 $\hat{t}$；编码步数 $T_{es} \leq T$

1. 采样噪声图像 $\hat{x}_{T_{es}} = x_{T_{es}} \sim q(x_{T_{es}} | x_0)$
2. 对于 $t = T_{es}, \dots, 1$，执行以下步骤：
    1. $x_{t-1} \sim q(x_{t-1} | x_t, x_0)$
    2. $\epsilon_t = \frac{x_{t-1} - \mu_T(x_t, t | t)}{\sigma_t}$
    3. $\hat{x}_{t-1} = \mu_T(\hat{x}_t, t | \hat{t}) + \sigma_t \odot \epsilon_t$
3. **输出**: $\hat{x} := \hat{x}_0$

在这项工作中，我们展示了我们的潜在空间定义和DPM编码器允许随机DPMs用于图像编辑。具体来说，我们提出了一种简单的无配对图像到图像翻译方法，称为CycleDiffusion。给定一个源图像 $x \in D_1$，我们使用DPM编码器将其编码为 $z$，然后将其解码为 $\hat{x} = G_2(z)$：
$$
z \sim \text{DPMEnc}(z | x, G_1), \hat{x} = G_2(z). \tag{6}
$$
我们还可以将CycleDiffusion应用于文本到图像的扩散模型，定义 $D_1$ 和 $D_2$ 为条件为两个文本的图像分布。设 $G_t$ 为一个条件为文本 $t$ 的文本到图像的扩散模型。给定一个源图像 $x$，用户编写两个文本：描述源图像 $x$ 的源文本 $t$ 和描述目标图像 $\hat{x}$ 的目标文本 $\hat{t}$。然后我们可以通过以下方式执行零样本图像到图像编辑（零样本意味着模型从未在图像编辑上进行训练）：
$$
z \sim \text{DPMEnc}(z | x, G_t), \hat{x} = G_{\hat{t}}(z). \tag{7}
$$
受SDEdit [17] 中现实-忠实度权衡的启发，我们可以将 $z$ 截断到指定的编码步数 $T_{es} \leq T$。CycleDiffusion的算法如算法1所示。

#### 固定 $z$ 时的图像相似性分析
我们分析了使用文本到图像扩散模型时的图像相似性。假设文本到图像模型具有以下两个属性：
1. 在相同文本条件下，类似的噪声图像会导致相似的均值预测。形式上，$\mu_T(x_t,t|t)$ 是 $K_t$-Lipschitz，即 $\|\mu_T(x_t,t|t) - \mu_T(\hat{x}_t,t|t)\| \leq K_t\|x_t - \hat{x}_t\|$。
2. 给定相同的图像，两个文本会导致相似的预测。形式上，$\|\mu_T(\hat{x}_t,t|t) - \mu_T(\hat{x}_t,t|\hat{t})\| \leq S_t$。直观上，$t$ 和 $\hat{t}$ 之间的差异越小，$S_t$ 越小。

设 $B_t$ 为使用相同潜在代码 $z$ 采样时 $t$ 时刻的上界（即 $x_0 = G_t(z)$ 和 $\hat{x}_0 = G_{\hat{t}}(z)$）。我们有 $B_T = 0$，因为 $\|x_T - \hat{x}_T\| = 0$，并且 $B_0$ 是生成图像 $\|x - \hat{x}\|$ 的上界。$B_t$ 可以从 $T$ 传播到 0。具体来说，通过结合上述两个属性，我们得到：
$$
B_{t-1} \leq (K_t + 1)B_t + S_t。 \tag{8}
$$

### 3.5. 应用2：即插即用引导
之前的研究表明，可以在潜在空间中为生成模型实现引导[18, 20, 35]。具体来说，给定一个条件 $C$，可以将引导图像分布定义为基于能量的模型（EBM）：$p(x|C) \propto p_x(x)e^{-\lambda_C E(x|C)}$。对于 $x \sim p(x|C)$ 的采样等价于 $z \sim p_z(z|C), x = G(z)$，其中 $p(z|C) \propto p_z(z)e^{-\lambda_C E(G(z)|C)}$。能量函数 $E(x|C)$ 的示例在第4.3节中提供。为了采样 $z \sim p(z|C)$，可以使用任何与模型无关的采样器。例如，Langevin动力学[34] 从 $z^{(0)} \sim N(0,I)$ 开始，并通过以下迭代方式采样 $z := z^{(n)}$：
$$
z^{(k+1)} = z^{(k)} + \sigma^2 \nabla_z \log p_z(z^{(k)}) - E(G(z^{(k)}) | C) + \sqrt{\sigma}\omega^{(k)}, \\
\omega^{(k)} \sim N(0,I)。 \tag{9}
$$

## 4. 实验
本节提供了对提出工作的实验验证。第4.1节展示了CycleDiffusion如何在无配对图像到图像翻译基准上取得竞争性结果。第4.2节提供了零样本图像编辑的协议；CycleDiffusion优于多个基线方法。第4.3节展示了在即插即用方式下控制确定性和随机DPMs的结果。

### 4.1. 无配对图像到图像翻译
给定两个不对齐的图像域，无配对图像到图像翻译将图像从一个域映射到另一个域。我们尽可能遵循之前工作的设置，具体如下。按照[21, 37]中的方法，我们在AFHQ [4] 的测试集上进行了实验，分辨率为256×256，用于猫 → 狗和野生动物 → 狗的翻译。对于每个源图像，每种方法都应生成一个变化最小的目标图像。在CycleDiffusion生成翻译后的图像后，我们使用SDEdit的Tsdedit步骤清理伪影。对于 $T = 1000$，我们设置猫 → 狗的 $T_{sdedit} = 100$，野生动物 → 狗的 $T_{sdedit} = 125$。

#### 评估指标：
为了评估生成图像的真实感，我们报告了生成图像与目标图像之间的Frechet Inception Distance (FID) [8] 和 Kernel Inception Distance (KID) [2]。为了评估图像的忠实度，我们报告了每个生成图像与其源图像之间的峰值信噪比 (PSNR) 和结构相似性指数 (SSIM) [33]。

#### 基线方法：
我们将CycleDiffusion与之前的最先进的无配对图像到图像翻译方法进行了比较，包括：CUT [21]、ILVR [3]、SDEdit [17] 和 EGSDE [37]。CUT基于GAN，而其他方法使用扩散模型。

#### 预训练的扩散模型：
基线方法ILVR、SDEdit和EGSDE只需要在目标域上训练的扩散模型，我们按照[3]中的方法使用预训练的狗模型。CycleDiffusion需要在两个域上训练的扩散模型，因此我们在猫和野生动物领域上训练了模型。

表1显示了实验结果。CycleDiffusion在真实感评估（即FID和KID）中表现最好。在忠实度指标（即PSNR和SSIM）之间存在不匹配，值得注意的是，SSIM与人类感知的相关性远高于PSNR [33]。在所有基于扩散模型的方法中，CycleDiffusion实现了最高的SSIM。图3展示了CycleDiffusion生成的一些图像示例，显示我们的方法在改变图像域的同时保留了局部细节，如背景、光照、姿态和动物的整体颜色。

### 4.2. 零样本图像编辑
本节提供了零样本图像编辑的实验。我们为该任务整理了一组150个三元组 $(x, t, \hat{t})$，其中 $x$ 是源图像，$t$ 是源文本（例如，图4右下方第二行中的“秋景的鸟瞰图”），$\hat{t}$ 是目标文本（例如，“冬景的鸟瞰图”）。生成的图像记为 $\hat{x}$。在本节的最后，我们还展示了CycleDiffusion可以与CrossAttentionControl [7] 结合以保留图像结构。

#### 评估指标：
为了评估生成图像对源图像的忠实度，我们报告了PSNR和SSIM。这些指标显示了生成图像与源图像的接近程度。为了评估生成图像对目标文本的真实性，我们报告了CLIP得分 $S_{\text{CLIP}} = \cos(\text{CLIP}_{\text{img}}(\hat{x}), \text{CLIP}_{\text{text}}(\hat{t}))$，其中CLIP嵌入经过归一化处理。我们注意到PSNR/SSIM与SCLIP之间存在权衡：通过完全复制源图像而不考虑文本，可以获得高PSNR/SSIM但低SCLIP；通过忽略源图像，直接基于目标文本生成图像，可以获得高SCLIP但低PSNR/SSIM。为了应对这种权衡，我们报告了方向性CLIP得分[22]（CLIP嵌入经过归一化处理）：
$$
SD-CLIP = \cos(\text{CLIP}_{\text{img}}(\hat{x}) - \text{CLIP}_{\text{img}}(x), \text{CLIP}_{\text{text}}(\hat{t}) - \text{CLIP}_{\text{text}}(t))\tag{10}
$$


#### 基线方法：
我们将CycleDiffusion与两个基线方法进行了比较：SDEdit [17] 和 DDIB [32]。SDEdit向源图像添加噪声，并基于目标文本去噪；DDIB使用基于ODE的确定性DPM将源图像编码为其潜在代码，然后在目标文本的条件下解码该潜在代码。我们尽可能为基线方法和CycleDiffusion使用相同的超参数（例如，扩散步骤数、分类器无引导的强度；详见附录B）。

#### 预训练的文本到图像扩散模型：
在我们的实验中，我们使用了以下文本到图像的扩散模型：(1) LDM-400M，一个具有1.45B参数的模型，训练于LAION-400M [28]；(2) Stable Diffusion v1-4，一个具有0.98B参数的模型，训练于LAION-5B [27]。

#### 结果：
表2显示了零样本图像到图像翻译的结果。CycleDiffusion在保持源图像的忠实度方面表现优异（即PSNR和SSIM）；相比之下，SDEdit和DDIB在与目标文本的真实性（即SCLIP）上具有相当的表现，但它们的输出在忠实度上要低得多。图4提供了CycleDiffusion的示例，展示了CycleDiffusion实现的有意义的编辑，包括：(1) 替换物体，(2) 添加物体，(3) 改变风格，(4) 修改属性。图5提供了零样本图像到图像翻译的定性比较。与DDIB和SDEdit相比，CycleDiffusion大大提高了源图像的忠实度。

#### 与交叉注意力控制的兼容性：

除了固定随机种子外，[7] 还表明，固定交叉注意力图（即交叉注意力控制，CAC）进一步提高了合成图像之间的相似性。CAC适用于CycleDiffusion：在算法1中，我们可以将 $\mu_T(x_t,t|t)$ 的注意力图应用于 $\mu_T(\hat{x}_t,t|\hat{t})$。然而，CAC不能应用于所有样本，因为它对 $t$ 和 $\hat{t}$ 之间的差异有要求。图6显示，当预期的结构变化较小时，CAC对CycleDiffusion有帮助。例如，当预期变化是颜色而不是形状时（左侧），CAC帮助CycleDiffusion保留背景；当预期变化是马 → 大象时，CAC使生成的大象在形状上更像马。

### 4.3. 即插即用引导

之前的扩散模型引导方法（如分类器引导）需要在噪声图像上训练引导模型[5, 16]。在本实验中，我们探索如何在不对CLIP模型进行噪声图像微调的情况下，从扩散模型中采样文本条件的图像。我们将第3.5节中的能量表示为 $E_{\text{CLIP}}(x|t) = \frac{1}{L} \sum_{l=1}^{L} \left(1 - \cos(\text{CLIP}_{\text{img}}^{\text{DiffAug}_l}(x), \text{CLIP}_{\text{text}}(t))\right)$，其中DiffAugl表示可微分增强[38]，用于减轻对抗性影响。我们使用Langevin动力学在公式（9）中采样，设 $n = 200, \sigma = 0.05$。

在图7中，我们展示了不同模型和不同文本提示（括号中显示）的几种未筛选样本。在这些方法中，SN-DDPM [1] 是一个随机DPM，LDM-DDIM [25] 是一个确定性DPM。除了这两种方法外，我们还展示了两种扩散模型的变体，去噪扩散GAN (DDGAN) [36] 和扩散自动编码器 (DiffAE) [23]。这两种模型不是典型的扩散模型，但它们也可以按照图2中的类似方式进行公式化。结果表明，即插即用引导是引导确定性和随机DPMs及其变体的有效方法。

## 5. 结论
本文提出了一种用于随机扩散模型的潜在空间。我们的想法受到了确定性扩散模型潜在空间的启发，该潜在空间允许诸如图像编辑等有趣的应用。我们展示了随机扩散模型的潜在空间可以在多个应用中与确定性扩散模型的潜在空间相同方式使用：（1）使用两个独立预训练的扩散模型（例如猫和狗）的无配对图像到图像翻译；（2）使用预训练的文本到图像扩散模型进行零样本图像编辑；（3）使用现成的图像理解模型（如CLIP）对预训练的（随机或确定性）扩散模型进行即插即用引导。我们希望这项工作能激发更多关于理解随机扩散模型及其潜在空间的研究。