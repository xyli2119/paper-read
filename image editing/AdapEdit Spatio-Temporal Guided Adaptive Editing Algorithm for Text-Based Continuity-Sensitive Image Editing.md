# AdapEdit：Spatio-Temporal Guided Adaptive Editing Algorithm for Text-Based Continuity-Sensitive Image Editing

## 摘要

随着文本条件扩散模型在创造性文本生成图像方面的巨大成功，基于文本驱动的图像编辑方法吸引了众多研究者的关注。然而，**以往的研究主要集中在对离散性敏感的指令，如添加、删除或替换特定的物体、背景元素或全局风格（即“硬编辑”）**，**而通常忽视了基于主体绑定但语义上细微变化的连续性敏感指令，如动作、姿势或形容词等（即“软编辑”）。**这限制了生成式AI生成用户定制的视觉内容。为缓解这一困境，我们提出了一种时空引导的自适应编辑算法AdapEdit。该算法通过**引入软注意力策略**，从**时间**和空间的角度动态调整编辑条件对视觉像素的引导程度，从而实现自适应图像编辑。值得注意的是，我们的方法在保持模型先验方面具有显著优势，并且不需要模型训练、微调、额外数据或优化。我们在各种原始图像和编辑指令上展示了我们的结果，表现出具有竞争力的性能，并表明其显著优于以往的方法。代码可访问：[https://github.com/AnonymousPony/adap-edit](https://github.com/AnonymousPony/adap-edit)。

## 1 引言

文本条件图像生成近年来取得了巨大的成功，产生了许多精彩且富有创造力的文本生成图像案例。其中，扩散概率模型（DPMs）（Ho, Jain, 和 Abbeel 2020；Nichol 和 Dhariwal 2021）作为核心网络，发挥了重要作用，并被开发用于各种文本生成图像的应用（Saharia 等 2022；Ramesh 等 2022；Ma 等 2023a）。然而，图像合成的可控性以及对用户自定义的支持仍然是具有挑战性的问题，并且吸引了越来越多研究人员的关注。为了解决这些问题，Stable Diffusion（Rombach 等 2022）和 ControlNet（Zhang 和 Agrawala 2023）作为两个领先的研究，提出了两个高效的文本条件学习框架用于训练，随后提出了一系列文本引导的图像编辑方法（Hertz 等 2022；Ruiz 等 2023；Wallace, Gokul 和 Naik 2023；Kawar 等 2023；Mokady 等 2023），这些方法通过编辑或修改原始图像以生成用户定制的图像输出，并在可控性方面取得了巨大的改进。

尽管取得了显著的进展，现有的基于文本的图像编辑方法仍然存在以下三大局限性。**（1）软编辑困境**。首先，如图1所示，我们注意到先前的方法主要集中在相对简单的编辑指令上，例如添加、删除或替换特定物体、背景元素或全局风格（即“硬编辑”），而通常忽略了与主体绑定但语义上细微变化的复杂指令，例如动作、姿势或形容词等（即“软编辑”），这阻碍了生成式AI生成用户自定义的图像。**（2）时空连续性困境**。其次，我们注意到先前的方法通常**在文本生成图像的扩散过程中对原始图像的每个像素（即引导目标）和文本条件的每个词语（即引导源）采用相同的引导程度，这可能导致意外区域出现伪影**，并且可能导致编辑对象周围的像素与周围环境的语义不连续。**（3）训练困境**。第三，为了提高模型在给定编辑指令下的可控生成能力，大多数先前的方法需要提供大量标注的图像-文本对作为训练数据，并且还需要设计额外的优化目标进行训练。然而，昂贵的训练通常会导致语言漂移现象，即在基于编辑指令训练时图像先验受到影响，而忽略了未编辑指令下的先验。

为了解决上述局限性，我们提出了AdapEdit，一种用于复杂连续性敏感图像编辑任务的时空引导自适应编辑算法。具体而言，为了解决第一个局限性，我们在算法中引入了软注意力策略，以自适应地引导从文本词语到视觉像素的注意力程度，从而实现可控的文本生成图像编辑。基于软注意力策略，我们进一步通过设计灵活的词级时间（FWT）调整模块和动态像素级空间（DPS）加权模块，解决了第二个连续性困境，从而通过空间引导尺度$s[V]$和时间引导尺度$\tau_c^*$实现时空引导自适应编辑。此外，我们引入了一个空间插值权重$\lambda_S$，它是一个超参数，用于在像素级精细控制编辑幅度。最后，整个AdapEdit算法只需一次前向扩散过程即可执行，并且不需要任何训练或优化，其算法时间开销仅为$O(n)$，自然解决了第三个训练困境。我们的主要贡献总结如下：

- **增强了软编辑能力**。支持更复杂的编辑任务，如软编辑任务，从而更好地支持用户定制图像生成场景。
- **改善了编辑的自然性和上下文连续性**。通过分配可变的时空引导尺度，自适应地引导从文本词语到视觉像素的注意力，显著提高了编辑图像的自然性和上下文语义一致性。
- **无需训练且不会破坏模型先验**。由于开发了确定性的自适应算法，它几乎不需要训练成本，如训练数据、注释、额外的优化目标和巨大的GPU开销。此外，更重要的是，我们的算法不会破坏DPMs自身的先验。

## 2 相关工作

**基于扩散的生成模型**。近年来，基于扩散的生成模型取得了显著的成功，得益于其在多样性和强大的生成能力方面的出色表现。以往的研究主要集中在采样过程（Song 等 2020；Liu 等 2022）、条件引导（Dhariwal 和 Nichol 2021；Nichol 等 2021）、似然最大化（Kingma 等 2021；Kim 等 2022）和泛化能力（Kawar 等 2022；Ma 等 2023b）等方面，从而实现了最先进的图像生成技术。

**文本引导的图像生成模型**。文本引导的图像生成旨在通过扩散模型中的噪声分布采样和预测，生成符合文本语义的图像。得益于引导扩散技术，最近的大规模文本生成图像模型如Imagen（Saharia 等 2022）、DALL-E2（Ramesh 等 2022）、Parti（Yu 等 2022）、CogView2（Ding 等 2022）和Stable Diffusion（Rombach 等 2022）生成了各种前所未有的图像，并展示了出色的表现。然而，值得注意的是，这些模型仅支持在给定文本提示的条件下从随机高斯分布中采样，无法通过直接编辑用户上传的图像来实现定制化图像生成，这限制了它们在更复杂和具有挑战性的图像定制任务中的应用。

**文本引导的图像编辑模型**。为了解决可控性问题并支持定制化，一系列基于扩散的图像编辑模型被提出。作为可控文本生成图像的领先研究，ControlNet（Zhang 和 Agrawala 2023）提出了一种端到端的神经网络架构，控制上述的大型扩散模型（如Stable Diffusion）学习特定任务输入条件，以高效支持条件引导的图像生成。基于这一有效的微调框架，一系列图像编辑方法被提出，推动了可控性研究。Prompt-to-Prompt（Hertz 等 2022）是其中最具代表性的方法之一，它通过在交叉注意力层的注意力图上执行一系列硬操作，如插入、替换和重加权，实现定向编辑。然而，它仍然受到无分类器引导累积误差的影响。为了解决这一问题，Null-Text-Inversion（Mokady 等 2023）引入了关键逆转技术以纠正累积误差，并提出了空文本优化方案以获得更好的性能。随后，为了实现主体绑定的编辑，DreamBooth（Ruiz 等 2023）提出引入一个唯一标识符用于主体特定绑定，并在预训练的扩散模型上进行微调，设定类别特定的先验保持目标。同时，Imagic 也利用了预训练的文本生成图像扩散模型，首先通过微调扩散模型找到输入图像的最优文本嵌入，然后采用线性插值技术获得语义上有意义的混合，以实现有效编辑。

尽管取得了显著成功，这些最新的图像编辑方法仍然存在前述的软编辑、时空连续性以及训练困境，阻碍了生成模型朝着更精细化控制和更广泛的定制化方向发展。为此，我们提出了AdapEdit算法，该算法可以应用于上述所有模型，以提高它们的编辑性能，并能够应对更复杂的编辑指令（即，连续性敏感的指令），如姿势、动作、形容词以及其他主体绑定但语义上细微变化的指令。

## 3 方法

### 3.1 预备知识

#### 扩散模型

扩散模型被定义为：
1. 从原始图像$x_0$到纯高斯分布$x_T \sim N(0,I)$的确定性前向噪声过程$q(x_t|x_{t-1}) = N(x_t; \sqrt{\alpha_t}x_{t-1}, (1-\alpha_t)I)$，其累积形式为：
$$
x_t = \sqrt{\bar{\alpha_t}}x_0 + \sqrt{1-\bar{\alpha_t}}\epsilon, \epsilon \sim N(0, I)
\tag{1}
$$
2. 迭代的可预测反向去噪过程$p_\theta(x_{t-1}|x_t) = N(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))$，通过合并$\mu_\theta$和$\Sigma_\theta$来预测噪声$\epsilon_\theta$，并通过简化的去噪目标$L_{simple}$进行训练：
$$
L_{simple} = E_{x_0,t,\epsilon \sim N(0,I)} [||\epsilon - \epsilon_\theta(x_t, t)||^2_2]
\tag{2}
$$
其中$t \sim U[1, T]$为时间参数，$U(·)$表示均匀分布。此外，在Stable Diffusion（Rombach等，2022）中，图像$x_t$通过编码器$E$被压缩为潜变量$z_t$，以实现更高效的训练，即$z_t = E(x_t)$，因此通常将这一目标定义为使$\epsilon_\theta(z_t, t)$尽可能接近$\epsilon \sim N(0,I)$。

#### 文本引导的扩散模型

文本引导的扩散模型的核心在于将文本条件$c$的语义集成到噪声预测模型$\epsilon_\theta(z_t, t)$中，以生成符合文本语义的视觉内容，即$\epsilon_\theta(z_t, t, c)$。最近，分类器自由引导技术被广泛应用于文本引导的图像生成中，表示为：
$$
\tilde{\epsilon_\theta}(z_t, t, c, \varnothing) = w \cdot \epsilon_\theta(z_t, t, c) + (1 - w) \cdot \epsilon_\theta(z_t, t, \varnothing)
\tag{3}
$$
其中$w = 7.5$为Stable Diffusion中无条件引导目标与条件引导目标的默认线性参数，$t$为时间输入，$c$为文本条件，$\varnothing$表示由零向量初始化的空文本嵌入，$\theta$为模型参数。注意，这些参数将在扩散模型的变体中单独或联合优化以实现控制的图像编辑。

### 3.2 AdapEdit算法框架

AdapEdit的流程如图3所示。为了实现用户友好的编辑，模型的输入包括两个文本条件$\{c, c^*\}$，与Prompt-to-Prompt（Hertz等，2022）相同，其中$c$为原始文本条件，$c^*$表示编辑条件。给定由$c$引导生成的图像$x$，AdapEdit的目标是操纵隐藏的注意力权重，将图像$x$修改为$x^*$，使用引导条件$c^*$。之前的方法主要集中于硬编辑指令，例如添加、移除或替换视觉内容，而基本忽略了更具挑战性的软编辑指令（即连续性敏感的提示）。为此，我们提出了AdapEdit，一种时空引导的自适应编辑算法。

具体而言，我们首先提出了一个灵活的词级时间（FWT）调整模块，支持根据编辑指令$c^*$中的每个词的影响自适应分配不同的引导尺度，以实现时间引导的编辑。

#### 灵活词级时间调整

**灵活词级时间（FWT）调整模块用于自适应地计算时间引导尺度$\tau_{c^*}$，以实现时间引导的编辑。**与使用相同的时间截断$\tau$应用于扩散模型中的关键字和一般词汇不同，我们首先提出了一种软注意力策略，灵活地计算每个词语的$\tau_{e^*_i}$，即$\{e^*_i\}_{i=1}^{N_{c^*}}$，以获得更新的注意力图$M_{e^*_i}^t$。形式上表示为：
$$
\overline{M}^{e^*_i}_t =
\begin{cases}
M^{e_i}_t & \text{如果} t > \tau_{e^*_i} \\
C(M^{e_i}_t, M^{e^*_i}_t) & \text{否则}
\end{cases}
\tag{4}
$$
其中，$e_i$和$e^*_i$分别表示$c$和$c^*$中的第$i$个词，$M_t$为它们对应的注意力图。当$t \geq \tau_{e^*_i}$时，更新后的注意力图$\overline{M}^{e^*_i}_t$等于$M^{e_i}_t$，以保留图像$x$的特征。而当$t < \tau_{e^*_i}$时，更新后的图$\overline{M}^{e^*_i}_t$由$M^{e_i}_t$和$M^{e^*_i}_t$的组合获得，其中$C(·)$表示插值操作，具体描述将在后续的DPS模块中进行。

接着，在运行至最后的扩散步骤时，我们基于32×32的分辨率计算所有层之间的交叉注意力图的平均值。随后，在给定每个单词的交叉注意力图$M^{e^*_i}_t$和视觉嵌入$E_{[V]} \in \mathbb{R}^{N_{[V]} \times d}$的情况下，每个$M^{e^*_i}_t \in \mathbb{R}^{1 \times N_{[V]}}$，其中$N_{[V]}$是视觉像素的数量。通过softmax归一化层，$M^{e^*_i}_t$与视觉嵌入$E_{[V]}$相乘，得到对应于编辑文本条件$c^*$的文本嵌入$E_{c^*} \in \mathbb{R}^{N_{c^*} \times d}$，公式如下：

$$
E_{c^*} = M^{c^*}_t \times E_{[V]} \tag5
$$
接着，通过平均池化对关键字嵌入进行融合，定义为$E_{e^*_k}$。在归一化嵌入之后，关键字与替换词嵌入$E_{e^*_k}$之间的相关向量$A_{c^*}$可以通过以下公式计算：

$$
A_{c^*} = E_{c^*} \times E_{e^*_k}, A_{c^*} \in \mathbb{R}^{1 \times N_{c^*}} \tag6
$$


最后，$\tau_{e^*_i}$的计算可以形式化定义为：

$$
\tau_{e^*_i} = \begin{cases} 
0 & e^*_i \in c^*_k, \\
\lambda_{\tau_{c^*}}[1 - \exp(A_{e^*_i} - 1)] & \text{otherwise}.
\end{cases}\tag7
$$


为了方便控制$\tau_{c^*}$的范围，我们使用了一个超参数$\lambda_{\tau_{c^*}}$来调整每个$\tau_{e^*_i} \in \tau_{c^*}$。在实际操作中，我们观察到通过上述过程计算的$\tau_{e^*_i}$与预期结果存在一定偏差，原因是背景区域的影响。以图4(a)为例，“sitting”的背景区域与“grass”的前景区域有大量重叠。背景区域中的大量非零像素会干扰$A_{grass}$。因此，我们采用了一种简单但有效的策略来为每个$M_{e^*_i}^t$获取一个掩码。当$M_{e^*_i}^t$中的值小于$\alpha_m$时，我们将其设为0。$\alpha_m$的经验值设为0.03。最终，我们可以获得区分性嵌入来计算时间引导尺度$\tau_{c^*}$。

#### 动态像素级空间加权

动态像素级空间（DPS）加权模块用于自适应地计算与视觉内容[V]相对应的空间引导尺度$S_{[V]}$，以实现空间引导的编辑。$S_{[V]}$表示视觉像素与$c^*$中的编辑词之间的相似度矩阵。所有$e^*_i \in c^*$共享相同的$S_{[V]}$，如果没有混淆，接下来的讨论中将省略下标$i$，注意力图将统一表示为$M_c^t$和$M_{c^*}^t$。为自适应地整合原始特征，我们引入了超参数$\lambda_S$，用于对$M_c^t$和$M_{c^*}^t$进行加权，并随后使用插值操作将它们整合：
$$
C(M_c^t, M_{c^*}^t) = \lambda_S [S_{[V]} \cdot M_{c^*}^t + (1 - S_{[V]}) \cdot M_c^t] + (1 - \lambda_S)M_c^t
\tag{8}
$$
在归一化$E_{e^*_k}$（经过$\alpha_m$过滤）和$E_{[V]}$后，$S_{[V]}$的相似度矩阵计算为：
$$
S_{[V]} = \lambda_{S_{[V]}}E_{e^*_k} \cdot E_{[V]}
\tag{9}
$$
类似于$\lambda_{\tau_{c^*}}$，我们使用$\lambda_{S_{[V]}}$自适应调整$S_{[V]}$中的值范围。综上所述，基于FWT和DPS两个模块，AdapEdit可以在不进行训练的情况下，通过时空引导尺度$\tau_{c^*}$和$S_{[V]}$执行连续性敏感的编辑任务。

### 算法1：AdapEdit算法

**输入**：文本条件$c$，编辑指令$c∗$  
**输出**：图像$x$，编辑后的图像$x∗$

1. 初始化：随机种子和参数
2. 函数FWT：
   3. 如果是最后一次扩散步骤
   4. 计算交叉注意力图$Mc∗_t$的平均值；
   5. 计算$Multiply(Mc∗_t, Ev) → Ec∗$（见方程5）；
   6. 计算$Average Pooling(Ee∗_i, i ∈ c∗_k) → Ee∗_k$；
   7. 计算$Multiply(Ec∗, Ee∗_k) → Ac∗$（见方程6）；
   8. 对于$i ∈ Nc∗$
   9. 更新$τe∗_i$（见方程7）；
   10. 结束循环
11. 结束条件判断
12. 函数DPS：
   13. 对于$t = T, T − 1, ..., 1$
   14. 计算$Multiply(Ee∗_k, Ev) → S[V]$（见方程9）；
   15. 更新$C(Mc_t, Mc∗_t)$（见方程8）；
   16. 更新$Mc∗_t$（见方程4）；
   17. 结束循环
18. 生成{$x, x∗$}；
19. 返回{$x, x∗$}。



## 4 实验

### 4.1 实验设置

#### 实现细节

我们采用Stable Diffusion v1.4版本（SD-v1.4）（Rombach等，2022）作为所有实验的骨干网络。此外，我们按照(Song, Meng, 和 Ermon 2020)将默认去噪迭代次数设置为50。分类器自由引导尺度设置为7.5，文本指令的最大长度设置为77。所有实验都在2个NVIDIA RTX3090 GPU上通过PyTorch进行。

#### 基准

我们将AdapEdit与两组最先进的图像编辑方法进行比较。1）第一组是基于真实图像操作的方法，包括SDEdit（Meng等，2021）和InstructPix2Pix（Brooks, Holynski, 和 Efros 2023）。在这两个基准中，目标文本条件$c∗$和原始真实图像$x$作为共同输入。为了与它们保持一致，我们使用Prompt-to-Prompt（Hertz等，2022）生成原始图像进行比较。2）第二组是基于文本生成的方法，包括Prompt-to-Prompt（Hertz等，2022）和MasaCtrl（Cao等，2023）。它们都使用原始文本条件$c$和目标文本条件$c∗$作为输入，生成原始图像$x$和编辑图像$x∗$。我们保持相同的设置。在实验中，我们固定了相同的随机种子以获得相同的原始图像。

### 4.2 定性评估

定性评估结果如图5所示。从图5中，我们得出以下三个观察结果。1）对于从“蛋糕”到“巧克力蛋糕”的硬编辑指令，所有方法都表现出色，这对于基于扩散的方法来说相对容易实现。2）进一步地，当编辑图5第二行中的“猫”的姿势以及图5第三行中的“桃子”数量时，Prompt-to-Prompt、SDEdit和InstructP2P都未能完成姿势或数量导向的编辑任务。得益于FWT和DPS模块的自适应编辑能力，我们的方法有效地处理了所有硬编辑或软编辑场景。此外，如图2所示，AdapEdit还可以进行自适应编辑，控制“两个苹果”具有不同的大小，展现出一定的推理能力。3）MasaCtrl在对象编辑性能方面表现出色，但未能保留原始图像的更多细节。值得注意的是，AdapEdit在这两个方面都处理得很好，证明了其有效性。

### 4.3 定量评估

根据Wang等（2023）的研究，我们进一步通过LPIPS、CLIP分数和CLIP方向性度量评估了我们的方法。具体来说，我们使用了来自InstructPix2Pix（Brooks, Holynski, 和 Efros 2023）的数据集，该数据集中包含700个人工标注的样本和GPT-3生成的454,445条编辑指令。为了避免机器生成指令中可能存在的噪音，我们随机选择了700个人工标注的样本进行评估。从表1可以看出，AdapEdit在CLIP分数和CLIP方向相似性方面表现最佳，而其LPIPS仅略高于InstructPix2Pix。与其他方法（如MasaCtrl）相比，它在CLIP分数和CLIP方向相似性方面表现良好，但在编辑结果的一致性方面存在不足。InstructPix2Pix的LPIPS值较低，但在CLIP分数方面表现不佳。上述现象表明，我们的方法在编辑的连续性和一致性之间达到了良好的平衡。

### 4.4 消融研究

在这一部分中，我们进行了消融实验，以评估每个组件的有效性。我们重点关注四个关键设置：1）w/o FWT表示移除了灵活的词级时间（FWT）调整模块；2）w/o DPS表示移除了动态像素级空间（DPS）加权模块；3）w/o DPS∗表示仅移除了DPS中提到的像素级掩蔽策略。图6展示了在不同设置下的可视化案例。从图6可以观察到，移除任何组件都会导致性能下降。特别是，当移除第2列中的DPS或DPS*模块时，AdapEdit将难以移除盘子上的“草莓”，这验证了DPS模块的有效性。此外，当移除DPS中的像素级掩蔽策略（即w/o DPS*）时，算法会出现轻微的不一致问题，意外地改变了原始特征，这表明提出的像素级掩蔽策略可以有效地感知与目标编辑指令相关的关键区域，并减弱无关区域的影响。此外，当在第3列中移除FWT模块时，站在草地上的鸟将难以编辑成“展翅”，这证明了FWT模块在处理连续性敏感编辑任务中的有效性。

### 4.5 超参数分析

为了分析时间尺度τc∗和空间尺度S[V]的影响，我们对λτc∗、λS[V]和λS进行了超参数实验。注意，λS是空间插值权重。实验结果如图7所示。如图7上排所示，时间超参数λτc∗揭示了时间连续性编辑过程。具体来说，随着λτc∗逐渐减小，马的姿势和动作逐渐接近“奔跑的马”的语义。此外，图7下排的第二张和第三张编辑后的图像中可以观察到，当保持λS = 0.9不变，仅将λS[V]从0.8变化到1.0时，桌子上的“汉堡”显示出显著的变化，顶部的“蛋糕”奶油出现了。此外，当λS达到0.98且λS[V]达到1.0时，图片中的“汉堡”已明显被编辑为“蛋糕”，这验证了自适应空间编辑的有效性，并表明选择合适的超参数对于可控的时空引导是必要的。

## 5 结论

在这项工作中，我们首先提出了关注连续性敏感的“软编辑”任务，并基于该任务提出了一种时空引导的自适应编辑算法，简称为AdapEdit。具体而言，该算法主要包括两个精心设计的模块，灵活的词级时间（FWT）调整模块和动态像素级空间（DPS）加权模块，以实现时空引导的自适应编辑。基于采用可变引导程度进入前向扩散过程的有前途的洞见，我们采用了软注意力策略，以实现具有挑战性的连续性敏感“软编辑”任务。此外，我们还引入了空间插值权重，以自适应地保留原始图像的特征。对真实图像编辑数据集的实验结果表明，AdapEdit在定性和定量评估中都表现出了其有效性和优越的性能。

