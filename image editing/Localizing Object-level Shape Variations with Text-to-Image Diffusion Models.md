# Localizing Object-level Shape Variations with Text-to-Image Diffusion Models

## 摘要
文本到图像模型引发了通常从探索步骤开始的工作流程，在此过程中，用户会筛选大量生成的图像。文本到图像生成过程的全局性阻碍了用户将探索范围缩小到图像中的特定对象。**在本文中，我们提出了一种生成图像集合的技术，该集合展示了特定对象形状的变体**，从而实现了对象级别的形状探索过程。生成合理的变体具有挑战性，因为它需要在控制生成对象形状的同时保持语义一致性。在生成对象变体时，一个特别的挑战是准确定位应用于对象形状的操作。我们引入了一种提示混合技术，该技术在去噪过程中切换提示，以获得多样化的形状选择。为了在图像空间中定位操作，我们提出了两种结合自注意力层与交叉注意力层的技术。此外，我们展示了这些定位技术在生成对象变体之外的领域同样通用且有效。大量的结果和对比实验表明了我们方法在生成对象变体上的有效性，以及定位技术的能力。

## 1. 引言
文本到图像扩散模型最近展示了前所未有的图像质量和多样性【36, 42, 40】，并开启了图像合成的新纪元。然而，生成图像的控制仍然有限，导致用户需要进行繁琐的选择过程，从中采样多个初始噪声，才能选择一个偏好的图像。从相同文本提示下的不同初始噪声生成的图像共享语义，但生成的形状在形状、外观和位置上可能有很大差异。由于此类采样过程引发的无控制的全局变化，用户无法与生成的图像进行交互，无法缩小其开放式的探索过程。特别是，用户对生成图像缺乏对象级的控制，阻碍了用户在探索过程中专注于改进特定对象的能力。

一种可能的与生成图像交互的方法是使用文本引导的图像修复【40】或SDEdit【30】，作为探索图像中特定对象的形状和外观的手段。这些方法主要在改变对象纹理方面表现出色，因此更适合纹理的探索。然而，改变对象的形状，尤其是在需要保持图像其他区域不变的情况下，挑战性显著增加，这些方法很难在不影响整个图像的情况下实现这一点。图2展示了上述方法在形状变体上的不足之处。

**图2**. 修补和SDEdit在实现篮子的显著形状变化时表现困难。修补受限于掩码，而SDEdit在局部化更改时表现不佳，请参见香蕉的部分。

本文我们探讨了对象级的形状探索，用户可以获得包含图像中特定对象形状变体的图库，无需提供任何额外输入。具体而言，鉴于以往工作在几何操控方面的困难，我们专注于自动生成的对象级形状变体，并展示给用户，参见图1。我们引入了一种提示混合技术，在去噪过程中使用不同提示的混合。这种方法建立在去噪过程是一个从粗到精的合成过程的前提上，该过程大致分为三个阶段。在第一阶段，绘制出一般的配置或布局。在第二阶段，形成对象的形状。最后，在第三阶段生成对象的细视觉细节。

**图1**. 我们的方法生成对象的形状变体。在这里，给定文本提示后，我们生成了篮子的变体。选择一个偏好的篮子后，我们生成了杯子的变体。我们开发了用于局部化修改的通用技术。

文本编辑技术中常见的挑战是如何对对象的修改进行局部化。因此，我们开发了两种新方法来局部化编辑。首先，为了保留其他对象的形状，**我们将原始图像中的局部自注意力图注入到新生成的图像中**。这种注入操作导致了两个图像之间的粗略对齐。接着，为了进一步保留外观（例如图像背景），我们自动提取了原始图像和生成图像的标记分割图。分割图允许在选定的局部区域中应用编辑。最后，在去噪的最后几个步骤中，我们将所有区域无缝地融合在一起。

我们展示了无需任何昂贵的优化过程，我们的方法能够为用户生成对象级的形状变体，同时保持生成图像或真实图像的忠实性。此外，我们证明了我们的局部化技术不仅对对象形状变体有益，还对通用的局部图像编辑方法有效。我们通过将我们的局部化技术整合到现有的图像编辑方法中，展示了改进的效果。通过大量实验，我们证明了我们的方法可以在形状变化更大且内容保留更好的情况下，生成更多样化的结果，与替代方法相比优势明显。

## 2. 相关工作
### 2.1. 文本引导的图像生成
文本到图像的合成是计算机视觉和计算机图形学中的一个长期问题。早期的工作基于生成对抗网络(GAN)【50, 38, 39, 52】，并在小规模数据集上进行训练，通常是单一类别的数据集。最近，随着扩散模型【46, 22, 15】、自回归模型【51, 16, 17, 9】的快速进展以及巨型文本-图像数据集【44】的可用性，大规模文本到图像模型【32, 40, 37, 42, 43, 5, 24】的性能得到了巨大提升。我们的工作使用了基于潜在扩散模型（Latent Diffusion Models）的公开可用的Stable Diffusion模型【40】。

大规模文本到图像模型允许用户为给定的文本提示生成一个图像库。然而，对于生成图像的控制是有限的，图像的构图、对象的形状、颜色和纹理等属性会根据随机采样的初始噪声而发生变化。因此，最近的工作向模型引入了额外的空间条件，例如分割图【3, 7】、边界框【27, 40】、关键点【27】和其他视觉条件【48, 53, 23】。这些条件提供了空间控制，但没有对象级的控制。为了获得对象级的控制，最近开发了许多文本引导的图像编辑方法。

### 2.2. 文本引导的图像编辑
生成模型是图像编辑的强大工具【1, 45, 34, 20, 6】。随着文本到图像扩散模型性能的提升，许多方法【30, 21, 10】利用它们进行文本引导的图像编辑。一种简单的方法是在输入图像上添加噪声，然后使用引导提示对其进行去噪【4, 2, 30, 13】。为了局部化编辑，用户需要定义一个掩码【40】。另一种方法是在生成过程中操纵模型的内部表示（如注意力图）【21, 47, 33】，以保持图像布局。其他方法则在文本编码器的潜在空间中操作【18】，可能会微调生成器【41, 25, 19】，或在图像对上训练模型【8】。

最近，有研究表明，可以通过在去噪过程中切换文本提示来改变对象的语义【28】。这种方法与我们的方法有相似之处，因为它们也混合提示。然而，他们的方法仅限于通过外观修改来改变语义，并显式地保留对象的形状。相比之下，我们的工作专注于对象几何修改。需要注意的是，与上述所有编辑方法不同，生成对象形状变体并不是一种编辑任务，因为它旨在为给定图像生成多个对象级的变体，同时保持对象的语义。此外，我们提出的编辑局部化技术是对上述编辑方法的补充，后续我们将展示这一点。

## 3. 预备知识
### 潜在扩散模型
我们展示了将我们的方法应用于公开可用的Stable Diffusion模型，该模型基于潜在扩散模型(LDM)架构【40】。在LDM中，扩散模型在预训练的自编码器的潜在空间中运行。去噪网络被实现为一个UNET，包含自注意力层和交叉注意力层。在每个时间步$t$，噪声化的空间编码$z_t$作为输入传递给去噪网络。网络的中间特征，表示为$\phi(z_t)$，从自注意力层和交叉注意力层中接收信息。注意力机制由三个主要组成部分组成：Keys (K)，Queries (Q) 和 Values (V)。Keys和Queries一起形成注意力图，并与Values相乘。在这项工作中，我们利用了自注意力层和交叉注意力层的注意力图。

### LDM中的交叉注意力层
在LDM中，**文本引导通过交叉注意力机制完成**。具体来说，令文本编码为$c$，通过学习的线性层$f_Q$、$f_K$、$f_V$获得$Q = f_Q(\phi(z_t))$，$K = f_K(c)$，$V = f_V(c)$。文本提示中的每个token对应于由Queries和Keys形成的注意力图，该图与每个token的Values相乘。因此，直观地说，Keys和Queries控制每个token的位置，而Values控制其形状和外观，正如我们在补充材料中展示的那样。然而，值得注意的是，这些组件并没有完全解耦。

根据交叉注意力机制的定义，可以观察到文本提示的编码仅输入到$f_K$和$f_V$，因此Keys和Values是唯一直接受文本提示影响的组件。

### LDM中的自注意力层
自注意力层建模了每个像素与所有其他像素之间的关系。在LDM中，每个像素对应于最终生成图像中的一个patch。先前的研究【21, 47】表明，自注意力严格控制图像的布局及其中对象的形状，因此在图像编辑中非常有用，可以保留输入图像的结构。在我们的工作中，我们的目标是修改感兴趣的对象，并保留图像的其余部分。因此，注入整个自注意力图不允许感兴趣对象的形状变化，正如上述工作所展示的那样。

## 4. 提示混合

为了生成对象变体，我们提出了一种在推理过程中操作的方法，不需要任何优化或模型训练。给定一个文本提示$P$和一个感兴趣的对象，由一个单词$w$表示，我们操纵去噪过程以获得对象级别的变体。

生成对象形状变体的关键是我们的提示混合技术。在提示混合中，不同的提示在去噪过程的不同时间间隔中使用。具体来说，我们定义了三个时间步区间$[T, T_3]$，$[T_3, T_2]$，$[T_2, 0]$，并在每个区间中使用不同的提示来引导去噪过程。我们用$P[t,t']$表示在区间$[t, t']$中使用的提示。此技术基于去噪过程从粗到细的性质，这一见解在【5, 10, 48, 14, 11】中也有所提及，我们在第4.1节中进一步分析了这些见解。

需要注意的是，与图像编辑技术相同，局部化操作区域对于成功的结果至关重要。在第5节中，我们介绍了两种实现局部化编辑的通用技术，并在我们的完整对象变体流水线中使用它们，如图3所示。

**图3**. 给定参考图像及其对应的去噪过程，我们的完整流程由三个主要组成部分组成。我们在时间戳区间$[T, T_3]$、$[T_3, T_2]$、$[T_2, 0]$中使用提示$P(w)$进行混合匹配。例如，在区间$[T, T_3]$、$[T_2, 0]$期间，我们设定$w = “篮子”$，而在区间$[T_3, T_2]$期间，我们设定$w = “托盘”$。在去噪过程中，我们应用基于注意力的形状局部化技术以保留其他对象的结构（这里为“桌子”）。我们通过选择性注入参考去噪过程中的自注意力图来实现这一点。在$t = T_1$时，我们通过分割参考图像和新生成的图像应用可控的背景保留技术，混合它们后继续去噪过程。

### 4.1. 去噪扩散过程的阶段

**图4**. 提示混合。对于提示语 “Two ⟨w⟩ on the street”（街上有两个⟨w⟩），右侧图像下方的彩色条表示在去噪过程中使用的对应单词。

我们分析了上面定义的时间步区间，以展示每个区间控制的图像属性类型。该分析展示了去噪过程的从粗到细的性质。图4中展示了分析的结果。在每行中，最左边的三个图像是使用提示$P(w)$生成的，形式为“两只⟨w⟩在街上”，其中$w$在每张图像中表示不同的单词。所有图像都使用相同的初始噪声生成。在每行的最右边两个图像中，我们应用了提示混合。具体来说，我们在每个时间区间中改变了输入提示中的$w$。如前所述，输入提示被输入到交叉注意力层，并直接影响Keys和Values。我们使用改变后的提示$P(w')$来计算Values，同时使用原始提示$P(w)$来计算Keys。该设计选择在补充材料中有详细解释。

在每行的第四列中，我们使用$P[T ,T_3](w_1)$和$P[T_3,0](w_2)$。如图所示，我们得到了一个包含$w_2$（金字塔、杯子）的图像，其布局和背景来自包含球体的图像（$w_1$）。在第五列中，我们使用$P[T ,T_3](w_1)$，$P[T_3,T_2](w_2)$和$P[T_2,0](w_3)$。可以看到，我们得到了一个包含$w_2$（金字塔、杯子）的图像，其布局和背景来自包含球体的图像（$w_1$），而$w_3$（毛绒物、金属）的细视觉细节（例如纹理）也被保留。**我们得出结论，第一区间$[T, T_3]$控制图像布局，第二区间$[T_3, T_2]$控制对象的形状，第三区间$[T_2, 0]$控制细节视觉（例如纹理）**。更多示例见补充材料。

### 4.2. 对象变体
#### 混合匹配
设$P(w)$表示原始图像的提示，其中$w$表示对应于感兴趣对象的单词。为了生成感兴趣对象的形状变体，我们执行提示混合，其中$P[T ,T_3](w) = P[T_2,0](w)$。这是提示混合的一种特殊情况，我们称之为“混合匹配”，因为我们在第二个区间进行混合，并在第一个和第三个区间匹配提示。形式上，我们通过使用$P[T ,T_3](w)$，$P[T_3,T_2](w')$和$P[T_2,0](w)$，其中$w'$是“代理”词（下文解释），来实现形状变体。

混合匹配允许保持在第一个区间形成的原始图像布局，第二个区间中设置的$w'$的形状，以及第三个区间中由$w$表示的原始对象的细视觉细节。

#### 代理词
在这里，我们描述确定代理词的方案。直观地说，代理词表示一个与感兴趣对象语义相近但形状不同的对象。受Stable Diffusion中使用CLIP【35】提取文本编码的启发，我们使用CLIP的文本空间来寻找代理词集合。给定一个单词$w$，我们寻找与$w$最相似的$k$个token$\{w'_1, ..., w'_k\}$。为此，我们考虑CLIP的tokenizer中的所有token，并使用形式为$P_{sim}(t) = “一张⟨t⟩的照片”$的提示将每个token嵌入到CLIP嵌入空间中。然后，我们选取与$P_{sim}(w)$编码距离最小的$k$个token，该提示代表我们的感兴趣对象。这为我们提供了与$w$语义最接近的$k$个token。为了考虑输入提示的上下文，我们根据$P(t)$和$P(w)$之间的CLIP距离对这$k$个token进行排序。最后，我们将排名前$m$个token定义为代理词。对于每个代理词，我们执行混合匹配以获得感兴趣对象的变体图像。

需要注意的是，由于我们在token级别考虑嵌入，有些代理词本身可能没有语义意义。然而，我们观察到它们在我们的混合匹配技术中仍然提供了有意义的变体，因为它们在CLIP的嵌入空间中距离很近。

## 5. 编辑局部化
如上所述，在改变对象的形状时，局部化编辑尤其具有挑战性。为此，我们提出了两种有助于从不同方面进行编辑局部化的技术。这些局部化技术对于成功生成对象形状变体至关重要。正如我们将展示的那样，其他已知的编辑方法也可以通过集成这些技术来受益，从而实现更好的局部操作。

### 5.1. 基于注意力的形状局部化
为了保留图像中对象的形状，我们引入了一种基于自注意力图注入的形状局部化技术。在对象变体流水线中，我们将此技术应用于我们希望保留的对象。即使仅注入几步的完整自注意力图，也能准确保留原始图像的结构，但同时也会阻止我们希望更改的对象发生显著的形状变化。

**图5.** 基于注意力的形状局部化。详细信息请参见第5.1节。

我们的技术如图5所示，围绕选择性注入自注意力图展开。考虑去噪网络中的一个特定自注意力层$l$，该层接收维度为$N \times N$的特征，并形成的注意力图为$S_t^{(l)}$，其维度为$N^2 \times N^2$。图中的值$S_t^{(l)}[i, j]$表示像素$j$对像素$i$的影响程度。换句话说，图的第$i$行显示每个像素对像素$i$的影响程度，而第$j$列显示像素$j$对图像中所有其他像素的影响程度。为了保留对象的形状，我们注入对应于包含感兴趣对象的像素的自注意力图的行和列。具体来说，给定一个去噪时间步$t$、自注意力层$l$和自注意力图$S_t^{(l)}$，我们定义对应的掩码$M_t^{(l)}$为：

$$
M_t^{(l)}[i, j] = \begin{cases} 
1 & i \in O_t^{(l)} \text{ 或 } j \in O_t^{(l)} \\
0 & \text{其他情况}
\end{cases} \tag{1}
$$

其中，$O_t^{(l)}$是对应于我们希望保留的对象的像素集。稍后我们将解释如何找到$O_t^{(l)}$。

定义掩码$M_t^{(l)}$后，新生成图像中的自注意力图变为：

$$
S_t^{*(l)} \leftarrow M_t^{(l)} \cdot S_t^{(l)} + (1 - M_t^{(l)}) S_t^{*(l)}, \tag{2}
$$

其中，$S_t^{(l)}$和$S_t^{*(l)}$分别是原始图像和新生成图像的自注意力图。更多的掩码控制在补充材料中有介绍。

为了找到对象所在的像素（即定义像素集$O_t^{(l)}$），我们利用交叉注意力图。这些图建模了图像中每个像素与提示中每个token之间的关系。对于我们希望保留的对象，我们考虑提示中对应token的交叉注意力图。然后，我们通过设置固定阈值，将交叉注意力图中激活值较高的像素定义为对象的像素集$O_t^{(l)}$。

### 5.2. 可控的背景保留
如之前的研究【47, 21】所示，自注意力注入主要保留结构。因此，我们引入了一种可控的背景保留技术，该技术可以保留背景的外观以及可能由用户定义的对象，这些对象通过输入提示中的对应名词指定。我们为用户提供了设置要保留的用户定义对象的控制权，因为不同的图像可能需要不同的配置。例如，在图3中，如果篮子的尺寸合适，用户可能希望保留橙子，而在其他情况下，篮子的尺寸发生变化时，可能希望更改橙子以适应修改后的篮子大小。

为了保留所需区域的外观，在$t = T_1$时，我们将原始图像和生成的图像进行混合，从生成的图像中获取变化的区域（例如感兴趣的对象），从原始图像中获取未变化的区域（例如背景）。接下来，我们展示我们新颖的分割方法，并描述混合过程。

#### 自分割
我们在有噪声的潜在编码上进行分割，因此无法应用现成的语义分割方法。因此，我们引入了一种基于自注意力图的分割方法，并通过交叉注意力图为每个分割区域进行标注。该方法基于这样一个前提：生成模型的内部特征编码了分割所需的信息【12, 55】。

在$t = T_1$时，我们对整个去噪过程中322 × 322的自注意力图进行平均。我们获得了一个大小为322 × 322的注意力图，将其重塑为32 × 32 × 1024，并使用K-Means算法对深度像素进行聚类，其中每个像素由聚合后的自注意力图的1024个通道表示。每个生成的聚类对应于生成图像的一个语义分割区域。图6中展示了几种分割结果。

提取语义分割区域后，我们将每个分割区域与输入提示中的一个名词匹配。对于每个分割区域，我们考虑提示名词的归一化聚合交叉注意力图，并通过以下方法将每个分割区域匹配到一个名词。对于对应于二进制掩码$M_i$的分割区域$i$，以及对应于归一化聚合交叉注意力图$A_n$的提示名词$n$，我们计算得分$s(i, n) = \frac{P(M_i \cdot A_n)}{P(M_i)}$。如果$\max_n s(i, n) > \sigma$，我们将分割区域$i$标记为$\arg\max_n s(i, n)$，否则将其标记为背景。阈值$\sigma$在所有实验中是固定的。

#### 混合原始图像和生成图像
我们使用分割图和相应的标注来覆盖新生成图像中的相关区域。只有当像素在原始图像和新图像中都被标记为背景或用户定义的对象时，我们才保留来自原始图像的像素。这种方法有助于克服感兴趣对象的形状修改，如图3中的篮子示例所示，篮子的手柄区域取自新生成的图像。混合潜在图像后，我们继续进行去噪过程。

## 6. 实验
### 6.1. 对象形状变体
我们进行了实验以评估我们方法在生成对象级形状变体方面的有效性，并与其他现有方法进行了比较。具体来说，我们将我们的方法与通过更改种子直接为图像提供变体的方法（如修补【40】、SDEdit【30】）以及最近的文本引导图像编辑基线（P2P【21】、I-Pix2Pix【8】、PnP【47】、零样本图像到图像翻译【33】、Imagic【25】）进行了比较。

需要注意的是，我们的方法在两个关键方面与图像编辑方法不同。首先，大多数编辑方法将对象更改为不同类别的对象，而我们的方法保持类别不变，并为同一对象提供替代选项。其次，编辑方法需要用户提供有关他们希望获得的确切对象的具体说明，而我们的方法允许开放式探索，无需依赖此类说明。因此，当我们将我们的方法与图像编辑方法进行比较时，采用了两种不同的方式：(i) 我们优化了输入提示，(ii) 我们用代理词替换了表示感兴趣对象的词。第二种方法（使用代理词）也可以看作是对混合匹配技术的消融研究。

我们分别评估了我们方法的三个重要方面。首先，我们的目标是实现感兴趣对象形状的高度多样性。其次，对象类别应保持不变。我们将这一评估目标称为对象忠实性。第三，除感兴趣对象以外的图像区域应保持不变。

#### 定性实验
在图7中，我们展示了一个图像库。更多结果在补充材料中提供。请注意我们方法的结果中的形状多样性以及原始图像的保留情况。在图8中，我们通过采样不同的种子与修补【40】和SDEdit【30】方法进行比较以获得变体。对于修补方法，我们使用了通过我们的分割技术获得的掩码。与图2类似，我们观察到应用修补主要导致纹理变化，而SDEdit进行了小范围的形状更改，但未能保留背景和其他对象（例如，猫）。

在图9中展示了与文本引导图像编辑方法的比较。对于每种方法，我们使用优化的提示（如“蛋椅”、“凳子”）和我们的自动代理词（如“购物车”、“床”、“凳子”）进行引导。需要注意的是，优化后的提示是手动选择的椅子类型。值得注意的是，对于没有子类型的对象（例如篮子），优化提示更具挑战性。在P2P【21】中，我们展示了两种版本的结果，区别在于自注意力注入步骤的比例（10%、40%）。关于每种方法的配置和更多方法的比较，详见补充材料。

图9中的我们方法的多样化结果保持了椅子的类别，并保留了图像的其余部分。不出所料，当提示中指定了不同的对象时，编辑方法在保持椅子方面遇到了困难。例如，当将椅子替换为购物车时，添加了车轮。在我们的方法中，得益于混合匹配技术，我们保留了车轮的形状，但椅子的细节视觉元素保持不变。

如图9所示，在P2P【21】中，将自注意力图注入40%的去噪步骤会阻止感兴趣对象（此处为椅子）的变化。相反，当将自注意力图注入10%的去噪步骤时，P2P在保持狗和背景方面遇到了困难。Instruct-Pix2Pix【8】的结果多样性较高，但在图像保留（蛋椅）和忠实性（床、凳子）方面不如我们的方法。Plug-and-Play【47】在执行形状变化时遇到了困难，因为它在整个去噪过程中注入了完整的自注意力图。与零样本图像到图像翻译【33】相比，该方法需要使用代理词进行1000次提示，而我们的方法在保持狗的颜色方面表现更好，并且允许生成更多样化的形状（参见凳子）。

我们还将我们的方法与使用Imagen【42】的Imagic【25】进行了比较。虽然Imagic产生了高质量的结果，但我们的方法在保留背景和保持椅子类别方面具有优势。此外，我们的方法比Imagic的基于优化的方法更高效。

#### 定量实验
给定一组图像的对象级变体，我们按照以下方式衡量每个目标。对于形状多样性，我们使用CLIPSeg【29】提取感兴趣对象的掩码，并计算集合中每对掩码的IoU均值。我们将多样性定义为$1 - \text{IoU}$。我们通过使用CLIP【35】计算包含感兴趣对象的图像的平均嵌入与集合中每张图像的余弦相似性来衡量忠实性。为了量化图像的保留，我们使用了LPIPS【54】。

我们创建了一个包含150张不同提示图像的数据集，并使用每种方法生成20个感兴趣对象的变体。有关数据集构建的更多细节在补充材料中提供。我们测试了使用代理词（所有方法）、随机种子（修补、SDEdit）和提示优化（I-Pix2Pix、P2P）的其他方法。

在图10中，我们展示了定量结果。我们的方法在多样性、忠实性和图像保留之间达到了良好的平衡。多样性得分高于我们的方法的其他方法没有保留原始图像，并且不忠实于感兴趣的对象，正如定性比较中所示。如图所示，保留或忠实性得分优于我们的方法的那些方法几乎没有改变对象的形状。

#### 消融研究
我们对生成给定图像的对象级形状变体的完整流程进行了消融实验，展示了每个部分的必要性。结果如图11所示。在第一行，我们展示了没有局部化技术的混合匹配结果。可以看到，单独使用混合匹配无法保留狗和背景。添加基于注意力的形状局部化技术后，我们为自注意力图基于“狗”一词创建了一个掩码，从而保留了狗。最后，添加可控的背景保留技术可以保留原始图像的背景。

### 6.2. 编辑局部化
我们将我们的局部化技术与现有的文本到图像方法结合，以展示在使用这些方法时结合我们的技术可以提高结果。

#### 基于注意力的形状局部化
之前的方法【21, 47】通过注入整个自注意力图来保留原始图像的形状。为了展示我们基于注意力的形状局部化技术的有效性，我们将其与P2P【21】结合，并在图12中展示了结果，我们的目标是将狗的帽子变成王冠。该图显示了左侧的原始生成图像，接着是我们在去噪步骤的40%和20%过程中注入了整个自注意力图的P2P结果，以及使用我们局部化技术的P2P结果。如图所示，使用P2P需要在准确地将帽子变为王冠和保留原始狗之间进行权衡。通过结合我们的方法，我们能够选择性地仅注入与狗对应的行和列，从而在保持狗的原始形状的同时实现帽子向王冠的转换。

#### 可控的背景保留
我们使用PnP【47】、P2P【21】和SDEdit【30】测试了我们的背景保留技术，并在图13中展示了结果。对于P2P，我们使用了其局部混合功能。为了将每种方法与我们的技术结合，我们首先对每张图像进行反转【31】。然后，我们使用我们的技术对图像的每个分割区域进行分割和标注，并创建图像中主要对象的掩码。在去噪过程的$t = 35$时，我们在编辑后的图像和原始图像之间进行混合，从原始图像中获取背景，并从编辑后的图像中获取对象。

如图所示，我们的方法即使对反转图像也能生成合理的分割图。对于PnP和SDEdit，我们的方法允许在保留原始图像背景的同时编辑对象。在P2P中，我们看到我们的方法更好地局部化了编辑，移除了图像中的猫尾巴和脚。

## 7. 讨论与结论
我们提出了一种用于探索图像中对象级形状变体的方法，解决了两个主要技术挑战：改变对象的形状以及局部化更改。为实现这一目标，我们引入了混合匹配技术来生成形状变体，并基于自注意力图注入来保留原始图像结构，同时允许更改感兴趣的对象。此外，我们展示了如何利用自注意力图中编码的几何信息进行图像分割，从而引导背景的保留。虽然我们的方法能够生成合理的图像变体，但我们也承认自动代理词有时可能会失效。未来，我们希望开发一种方法来探索连续的词汇空间，而非当前离散的词汇空间。