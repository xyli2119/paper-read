# High-Resolution Image Synthesis with Latent Diffusion Models

## 摘要

通过将图像生成过程分解为去噪自动编码器的顺序应用，扩散模型（DMs）在图像数据及其他领域的合成结果上达到了最先进的水平。此外，它们的公式允许通过引导机制来控制图像生成过程，而无需重新训练。然而，由于这些模型通常直接在像素空间中操作，优化强大的DMs通常需要数百个GPU天的计算资源，并且由于需要顺序评估，推理过程也非常昂贵。为了在有限的计算资源上实现DM的训练，同时保持其质量和灵活性，我们将它们应用于强大的预训练自动编码器的潜在空间中。与之前的工作相比，在这种表示上训练扩散模型首次能够在复杂度降低和细节保留之间达到接近最优的平衡，大大提高了视觉保真度。

通过在模型架构中引入交叉注意力层，我们将扩散模型转变为强大且灵活的生成器，能够接受通用的条件输入，如文本或边界框，并且卷积方式下的高分辨率合成成为可能。我们的潜在扩散模型（LDMs）在图像修复和类条件图像合成方面取得了新的最先进得分，并且在多项任务上表现出高度竞争力，包括文本到图像合成、无条件图像生成和超分辨率，同时显著减少了与像素级DM相比的计算需求。

## 1. 引言

图像合成是计算机视觉领域中发展最为显著的领域之一，但同时也是计算需求最大的领域之一。尤其是复杂自然场景的高分辨率合成，目前主要依赖于扩大基于似然的模型规模，这些模型可能包含数十亿个参数，如自回归（AR）变压器 [66, 67]。相比之下，GAN [3, 27, 40] 的令人印象深刻的结果已被揭示为主要局限于具有相对有限多样性的数据集，因为其对抗学习过程难以扩展到复杂的多模态分布建模。最近，基于去噪自动编码器层次结构的扩散模型 [82] 在图像合成 [30, 85] 及其他领域 [7, 45, 48, 57] 中取得了令人瞩目的成果，并在类条件图像合成 [15, 31] 和超分辨率 [72] 方面定义了最新的技术水平。此外，与其他类型的生成模型 [19, 46, 69] 不同，即使是无条件扩散模型也可以轻松应用于诸如修复和上色 [85] 或基于笔触的合成 [53] 等任务。作为基于似然的模型，它们不像GAN那样表现出模式崩溃和训练不稳定性，并且通过大量利用参数共享，它们可以在不涉及数十亿参数（如AR模型 [67]）的情况下建模自然图像的高度复杂分布。

### 民主化高分辨率图像合成
扩散模型属于基于似然的模型类别，其模式覆盖行为使得它们在建模数据的细节时容易消耗过多的容量（因此也是计算资源）[16, 73]。尽管重新加权的变分目标 [30] 试图通过对初始去噪步骤进行欠采样来解决这一问题，但扩散模型仍然计算量巨大，因为训练和评估此类模型需要在RGB图像的高维空间中进行反复的函数评估（以及梯度计算）。例如，训练最强大的扩散模型通常需要数百个GPU天（例如，在 [15] 中为150至1000个V100天），并且在输入空间的噪声版本上进行反复评估也使得推理代价高昂，因此生成50k样本大约需要5天 [15]，在一台A100 GPU上完成。这对研究界和用户来说有两个重要影响：首先，训练这样的模型需要大量的计算资源，这仅为少数领域的研究者所能获得，并且会留下巨大的碳足迹 [65, 86]。其次，评估已训练好的模型也需要大量的时间和内存，因为相同的模型架构必须在大量步骤中连续运行（例如，在 [15] 中为25至1000步）。

为了提高这一强大模型类别的可访问性，同时减少其显著的资源消耗，需要一种能够减少训练和采样计算复杂性的方法。因此，在不损害扩散模型性能的前提下，降低其计算需求是增强其可访问性的关键。

### 进入潜在空间

我们的方法首先从分析已经在像素空间训练的扩散模型开始：图2展示了已训练模型的率失真权衡。与任何基于似然的模型一样，学习过程大致可以分为两个阶段：第一阶段是感知压缩阶段，去除了高频细节但仍然学习到一些语义变化。第二阶段，实际的生成模型学习数据的语义和概念组成（语义压缩）。因此，我们的目标是找到一个在感知上等效但在计算上更合适的空间，并在该空间中训练扩散模型以实现高分辨率图像合成。

按照常见的做法 [11, 23, 66, 67, 96]，我们将训练分为两个独立的阶段：首先，我们训练一个自动编码器，该编码器提供了一个较低维度（因此更高效）的表示空间，该空间在感知上等同于数据空间。重要的是，与以往工作不同 [23,66]，我们不需要依赖过度的空间压缩，因为我们在学习的潜在空间中训练扩散模型，该空间在空间维度上的扩展性更好。简化的复杂性也使得能够通过单次网络传递从潜在空间高效生成图像。我们将所得的模型类别称为**潜在扩散模型（LDMs）**。

这种方法的显著优势在于，我们只需要训练一次通用的自动编码阶段，并且可以将其重复使用于多个扩散模型的训练中，或用于探索可能完全不同的任务 [81]。这使得能够高效地探索大量的扩散模型，用于各种图像到图像和文本到图像的任务。对于后者，我们设计了一种架构，将transformer与扩散模型的UNet主干相连接，并支持任意类型的基于token的条件机制，详见第3.3节。

总之，我们的工作做出了以下贡献：
1. 与纯transformer-based方法相比 [23, 66]，我们的方法在扩展到更高维度数据时表现更加优雅，因此：(a) 在压缩级别上工作时能够比以往工作提供更忠实和详细的重建（见图1），并且(b) 能够高效地应用于百万像素图像的高分辨率合成。
2. 我们在多个任务（无条件图像合成、图像修复、随机超分辨率）和数据集上实现了竞争性的性能，同时显著降低了计算成本。与基于像素的扩散方法相比，我们也大幅减少了推理成本。
3. 与以往工作不同 [93]，后者同时学习编码器/解码器架构和基于分数的先验，我们的方法不需要精细权衡重建和生成能力。这确保了极其忠实的重建，并且对潜在空间的正则化需求极少。
4. 我们发现，对于超分辨率、修复和语义合成等密集条件任务，我们的模型可以通过卷积方式应用，并渲染出大而一致的图像，尺寸可达∼1024²像素。
5. 此外，我们设计了一种基于交叉注意力的通用条件机制，支持多模态训练。我们利用它来训练类条件、文本到图像以及布局到图像的模型。
6. 最后，我们发布了预训练的潜在扩散和自动编码模型，网址为：https://github.com/CompVis/latent-diffusion，这些模型可以用于除训练扩散模型之外的各种任务 [81]。

## 2. 相关工作

### 图像合成的生成模型  
图像的高维特性为生成建模带来了独特的挑战。生成对抗网络（GAN）[27]允许高效生成具有良好感知质量的高分辨率图像 [3, 42]，但难以优化 [2, 28, 54]，并且难以捕捉完整的数据分布 [55]。相比之下，基于似然的方法注重良好的密度估计，使得优化过程更加平稳。变分自编码器（VAE）[46]和基于流的方法 [18, 19] 实现了高分辨率图像的高效合成 [9, 44, 92]，但其样本质量不及GAN。自回归模型（ARM）[6, 10, 94, 95] 在密度估计上表现强劲，但其计算量大且采样过程是顺序进行的，限制了它们只能生成低分辨率的图像。由于图像的像素表示中包含了几乎无法感知的高频细节 [16,73]，最大似然训练将大量容量花费在这些细节的建模上，导致训练时间长。为了解决高分辨率问题，一些两阶段方法 [23,67,101,103] 使用ARM来建模压缩后的潜在图像空间，而不是直接处理原始像素。

最近，扩散概率模型（DM）[82]在密度估计 [45] 和样本质量 [15] 方面取得了最先进的成果。这些模型的生成能力源于它们与图像数据的归纳偏差相契合，尤其是在其底层神经骨干网络由UNet实现时 [15, 30, 71, 85]。通常，使用重新加权目标 [30] 进行训练可以实现最佳的合成质量。在这种情况下，DM相当于一种有损压缩器，能够在图像质量和压缩能力之间进行权衡。然而，在像素空间中评估和优化这些模型的缺点是推理速度较慢且训练成本极高。虽然可以通过先进的采样策略 [47, 75, 84] 和层次方法 [31, 93] 部分解决推理速度问题，但在高分辨率图像数据上进行训练仍然需要计算昂贵的梯度。我们提出的潜在扩散模型（LDMs）在更低维度的压缩潜在空间上工作，从而降低了训练成本并加快了推理速度，同时几乎不降低合成质量（见图1）。

### 两阶段图像合成  
为了弥补个别生成方法的不足，许多研究 [11, 23, 67, 70, 101, 103] 通过两阶段方法将不同方法的优点结合到更高效和更具表现力的模型中。VQ-VAEs [67, 101] 使用自回归模型在离散化的潜在空间上学习表达性先验。VQGANs [23, 103] 扩展了这一方法，使用对抗和感知目标来扩展自回归transformers以生成更大的图像。然而，为了实现可行的自回归模型训练，所需的高压缩率引入了数十亿个可训练参数 [23, 66]，这限制了这些方法的整体性能，而降低压缩率则意味着更高的计算成本 [23, 66]。我们的工作避免了这些权衡，因为我们提出的LDMs通过卷积骨干更优雅地扩展到更高维度的潜在空间。因此，我们可以自由选择最佳的压缩水平，从而既保证生成扩散模型学习强大的第一阶段，又避免将过多的感知压缩留给扩散模型，同时确保高保真重建（见图1）。

尽管存在一些联合 [93] 或分别 [80] 学习编码/解码模型与基于分数的先验的方法，但前者仍然需要在重建和生成能力之间进行权衡 [11]，并且在性能上被我们的方法超越（见第4节），而后者则专注于高度结构化的图像，如人脸。

### 3. 方法  
为降低扩散模型训练对高分辨率图像合成的计算需求，我们观察到，尽管扩散模型允许通过对相应损失项进行欠采样来忽略感知上无关的细节 [30]，但它们仍然需要在像素空间中进行高昂的函数评估，从而造成巨大的计算时间和能源资源需求。

我们提出通过引入压缩学习阶段与生成学习阶段的显式分离来规避这一缺点（见图2）。为此，我们采用一种自动编码模型，该模型学习到的空间在感知上与图像空间等效，但计算复杂度显著降低。

这种方法带来了几个优势：(i) 通过离开高维图像空间，我们获得了计算上更加高效的扩散模型，因为采样是在低维空间上进行的。(ii) 我们利用了扩散模型从其UNet架构继承的归纳偏差 [71]，这使得它们在具有空间结构的数据上特别有效，因此缓解了之前方法所需的激进的、降低质量的压缩水平 [23, 66]。(iii) 最后，我们获得了通用的压缩模型，其潜在空间可用于训练多个生成模型，还可以用于其他下游应用，如单图像CLIP引导的合成 [25]。

### 3.1 感知图像压缩

我们的感知压缩模型基于之前的工作 [23]，由结合感知损失 [106] 和基于 patch 的对抗目标 [20, 23, 103] 训练的自动编码器组成。这确保了重建结果被限制在图像流形内，通过强制局部真实感避免了仅依赖像素空间损失（如L2或L1目标）引入的模糊。

更具体地说，给定一个RGB空间的图像 $x \in \mathbb{R}^{H \times W \times 3}$，编码器 $E$ 将 $x$ 编码为潜在表示 $z = E(x)$，解码器 $D$ 从潜在表示重建图像，生成 $\tilde{x} = D(z) = D(E(x))$，其中 $z \in \mathbb{R}^{h \times w \times c}$。重要的是，编码器将图像下采样了一个因子 $f = H/h = W/w$，我们研究了不同的下采样因子 $f = 2^m$，其中 $m \in \mathbb{N}$。

为了避免潜在空间的任意高方差，我们尝试了两种不同的正则化方法。第一种变体 KL 正则化（KL-reg.），类似于 VAE [46, 69]，对学习的潜在空间施加了向标准正态分布的轻微 KL 惩罚，而 VQ 正则化（VQ-reg.）则在解码器中使用了向量量化层 [96]。该模型可以解释为 VQGAN [23]，但量化层被吸收到解码器中。由于我们后续的扩散模型（DM）旨在处理学习到的二维结构潜在空间 $z = E(x)$，我们可以使用相对温和的压缩率，并实现非常好的重建效果。这与之前的工作 [23, 66] 形成了对比，后者依赖于对学习空间 $z$ 进行任意的1D排序以自回归地建模其分布，忽略了 $z$ 的固有结构。因此，我们的压缩模型能够更好地保留图像 $x$ 的细节（见表8）。完整的目标函数和训练细节可以在补充材料中找到。

### 3.2 潜在扩散模型

扩散模型（DM）[82] 是一种概率模型，旨在通过逐步去噪一个服从正态分布的变量来学习数据分布 $p(x)$，这对应于学习一个长度为 $T$ 的固定马尔可夫链的逆过程。对于图像合成，最成功的模型 [15, 30, 72] 依赖于一个重新加权的变分下界，该下界反映了去噪分数匹配 [85]。这些模型可以解释为等权重的去噪自动编码器序列 $\epsilon_{\theta}(x_t, t); t = 1, \dots, T$，这些编码器被训练来预测输入 $x_t$ 的去噪版本，其中 $x_t$ 是输入图像 $x$ 的噪声版本。相应的目标函数可以简化为（见补充材料B）：

$$
LDM = \mathbb{E}_{x,\epsilon \sim \mathcal{N}(0, 1), t} \left[ \|\epsilon - \epsilon_{\theta}(x_t, t)\|_2^2 \right] 
$$

其中 $t$ 从 $1, \dots, T$ 均匀采样。

#### 潜在表示的生成建模

通过我们训练的感知压缩模型（包含 $E$ 和 $D$），我们现在可以获得一个高效的、低维的潜在空间，其中高频的、不可察觉的细节被抽象掉了。与高维像素空间相比，这个空间更适合基于似然的生成模型，因为它们现在可以：(i) 专注于数据中的重要语义信息；(ii) 在低维且计算上更高效的空间中进行训练。

与之前依赖于高度压缩的离散潜在空间中的自回归注意力变换模型的工作 [23, 66, 103] 不同，我们可以利用我们的模型提供的图像特定的归纳偏差。这包括构建基于2D卷积层的底层UNet，并进一步将目标集中在感知上最相关的信息上，使用重新加权的下界，其形式为：

$$
LLDM := \mathbb{E}_{E(x),\epsilon \sim \mathcal{N}(0,1), t} \left[ \|\epsilon - \epsilon_{\theta}(z_t, t)\|_2^2 \right]
$$

模型的神经骨干 $\epsilon_{\theta}(\cdot, t)$ 通过时间条件的UNet实现。由于前向过程是固定的，$z_t$ 可以在训练期间从 $E$ 高效获取，并且从 $p(z)$ 采样的样本可以通过 $D$ 的单次传递解码到图像空间。

### 3.3 条件机制

与其他类型的生成模型类似 [56, 83]，扩散模型原则上能够建模形如 $p(z|y)$ 的条件分布。这可以通过条件去噪自动编码器 $\epsilon_{\theta}(z_t, t, y)$ 实现，从而为通过输入 $y$（如文本 [68]、语义图 [33, 61] 或其他图像到图像的转换任务 [34]）控制合成过程铺平了道路。

在图像合成的背景下，将扩散模型的生成能力与类标签 [15] 或输入图像的模糊版本 [72] 之外的其他类型的条件结合，迄今仍是一个研究不足的领域。

我们通过将交叉注意力机制 [97] 引入扩散模型的底层UNet骨干，增强了它们作为更灵活的条件图像生成器的能力。交叉注意力机制在学习多种输入模态的注意力模型时非常有效 [35, 36]。为了对来自不同模态的 $y$ 进行预处理（如语言提示），我们引入了一个领域特定的编码器 $\tau_{\theta}$，它将 $y$ 投射到一个中间表示 $\tau_{\theta}(y) \in \mathbb{R}^{M \times d_{\tau}}$，然后通过实现交叉注意力层的方式将其映射到UNet的中间层中：

$$
Attention(Q, K, V) = \text{softmax} \left( \frac{QK^T}{\sqrt{d}} \right) \cdot V
$$

其中，$Q = W(Q_i) \cdot \phi_i(z_t)$，$K = W(K_i) \cdot \tau_{\theta}(y)$，$V = W(V_i) \cdot \tau_{\theta}(y)$。这里 $\phi_i(z_t) \in \mathbb{R}^{N \times d_i}$ 表示UNet的一个（展平的）中间表示，$W(V_i) \in \mathbb{R}^{d \times d_i}$，$W(Q_i) \in \mathbb{R}^{d \times d_{\tau}}$ 和 $W(K_i) \in \mathbb{R}^{d \times d_{\tau}}$ 是可学习的投射矩阵 [36, 97]。图3给出了一个视觉示例。

基于图像-条件对，我们通过下式学习条件LDM：

$$
LLDM := \mathbb{E}_{E(x), y, \epsilon \sim \mathcal{N}(0,1), t} \left[ \|\epsilon - \epsilon_{\theta}(z_t, t, \tau_{\theta}(y))\|_2^2 \right]
$$

其中，$\tau_{\theta}$ 和 $\epsilon_{\theta}$ 通过式(3) 联合优化。该条件机制非常灵活，因为 $\tau_{\theta}$ 可以通过领域特定的专家进行参数化，例如当 $y$ 为文本提示时可以使用（未遮蔽的）transformers [97]（见第4.3.1节）。

## 4. 实验

LDM 提供了一种灵活且计算上可行的基于扩散的图像合成方法，适用于各种图像模态。我们通过以下实验实证展示了这一点。首先，我们分析了 LDM 相对于基于像素的扩散模型在训练和推理中的优势。有趣的是，我们发现尽管 VQ 正则化的第一阶段模型的重建能力略微落后于其连续对应物，但在 VQ 正则化的潜在空间中训练的 LDM 有时能实现更好的样本质量（参见表 8）。关于第一阶段正则化方案对 LDM 训练的影响及其对高于 256² 分辨率的泛化能力的视觉比较，见附录 D.1。E.2 中列出了所有结果的架构、实现、训练和评估细节。

### 4.1 感知压缩的权衡

本节分析了具有不同下采样因子 $f \in \{1, 2, 4, 8, 16, 32\}$ 的 LDM 的行为（缩写为 LDM-f，其中 LDM-1 对应于基于像素的扩散模型）。为了获得可比的测试环境，我们将本节所有实验的计算资源固定为单个 NVIDIA A100，并在相同的步数和参数数量下训练所有模型。表 8 显示了用于本节比较的 LDM 第一阶段模型的超参数和重建性能。图 6 展示了类条件模型在 ImageNet [12] 数据集上训练 200 万步的样本质量随训练进展的变化情况。我们观察到：

1. 小的下采样因子（LDM-{1,2}）导致训练进展缓慢；
2. 过大的下采样因子（如 LDM-32）在较少训练步数后导致保真度停滞。

结合前面的分析（图 1 和 2），我们认为这主要归因于：
1. 将大部分感知压缩留给了扩散模型；
2. 第一阶段的过强压缩导致信息丢失，限制了可实现的质量。

LDM-{4-16} 实现了效率和感知上真实结果之间的良好平衡，经过 200 万训练步，像素扩散模型（LDM-1）与 LDM-8 之间的 FID 差距高达 38。

在图 7 中，我们对比了在 CelebA-HQ [39] 和 ImageNet 上训练的模型，在使用 DDIM 采样器 [84] 进行不同的去噪步数时的采样速度，并将其与 FID 分数 [29] 作比较。LDM-{4-8} 在感知和概念压缩比例不合适的模型中表现更佳。特别是与基于像素的 LDM-1 相比，它们在显著提高样本吞吐量的同时获得了更低的 FID 分数。对于复杂的数据集如 ImageNet，减少压缩率是避免质量下降的关键。总的来说，LDM-4 和 LDM-8 为实现高质量合成结果提供了最佳条件。

### 4.2 使用潜在扩散进行图像生成

我们在 CelebA-HQ [39]、FFHQ [41]、LSUN-Churches 和 LSUN-Bedrooms [102] 上训练了 256² 图像的无条件模型，并通过以下方式评估了这些模型：
1. 样本质量；
2. 数据流形的覆盖度，使用 FID [29] 和 Precision-and-Recall [50] 进行评估。

表 1 总结了我们的结果。在 CelebA-HQ 上，我们报告了新的最先进的 FID 分数 5.11，超越了之前的基于似然的模型以及 GANs。我们还超越了 LSGM [93]，后者是将潜在扩散模型与第一阶段联合训练的模型。相比之下，我们在固定空间中训练扩散模型，避免了在重建质量与学习潜在空间的先验之间进行权衡的难题（见图 1 和 2）。

我们在除 LSUN-Bedrooms 数据集以外的所有数据集上都超越了之前基于扩散的方法，尽管我们仅使用了 ADM [15] 一半的参数，并且训练资源减少了四倍（见附录 E.3.5）。此外，LDM 在 Precision 和 Recall 上持续超越基于 GAN 的方法，进一步证明了其模式覆盖的基于似然的训练目标相比对抗性方法的优势。图 4 展示了我们在每个数据集上的定性结果。

### 4.3 条件潜在扩散

#### 4.3.1 LDM 的 Transformer 编码器

通过将基于交叉注意力的条件机制引入 LDM，我们将其扩展到以前未探索的多种条件模态。对于文本到图像的建模，我们在 LAION-400M 数据集上训练了一个由语言提示条件的 1.45B 参数 KL 正则化 LDM。我们采用 BERT-tokenizer [14] 并使用 Transformer [97] 实现 $τθ$，以推断潜在编码，该编码通过（多头）交叉注意力映射到 UNet 中（参见 3.3 节）。这种结合了领域特定专家的语言表示学习和视觉合成的模型表现出了强大的能力，在处理复杂的用户定义文本提示时具有良好的泛化能力，参见图 8 和图 5。

在定量分析中，我们遵循之前的工作，并在 MS-COCO [51] 验证集上评估文本到图像生成。我们的模型超越了强大的 AR [17, 66] 和 GAN [109] 基础的方法，参见表 2。我们注意到，应用无分类器的扩散引导 [32] 极大提升了样本质量，因此引导的 LDM-KL-8-G 在文本到图像合成方面与最新的 AR [26] 和扩散模型 [59] 相当，同时显著减少了参数数量。为了进一步分析基于交叉注意力的条件机制的灵活性，我们还在 OpenImages [49] 上训练了基于语义布局合成图像的模型，并在 COCO [4] 上进行微调，见图 8。有关定量评估和实现细节，请参阅 D.3 节。

最后，遵循之前的工作 [3, 15, 21, 23]，我们在表 3、图 4 和 D.4 节中评估了我们最好的类条件 ImageNet 模型（来自 4.1 节的 $f \in \{4, 8\}$）。在这里，我们超越了最新的扩散模型 ADM [15]，同时显著减少了计算需求和参数数量，参见表 18。

#### 4.3.2 超过 256² 的卷积采样

通过将空间对齐的条件信息连接到 $ϵθ$ 的输入，LDM 可以作为高效的通用图像到图像翻译模型。我们利用这一点训练了语义合成、超分辨率（参见 4.4 节）和图像修复（参见 4.5 节）的模型。对于语义合成，我们使用了配对有语义地图的风景图像 [23, 61]，并将下采样后的语义地图与 $f = 4$ 模型（VQ-reg.，参见表 8）的潜在图像表示相结合。我们在 256²（从 384² 裁剪）分辨率上进行训练，但发现我们的模型可以推广到更大的分辨率，并在卷积方式下生成高达百万像素的图像（参见图 9）。我们利用这一特性，在 4.4 节的超分辨率模型和 4.5 节的图像修复模型中生成 512² 至 1024² 的大图像。对于此应用，信噪比（由潜在空间的尺度引起）显著影响结果。我们在 D.1 节中对此进行了说明，当在 $f = 4$ 模型提供的潜在空间上学习 LDM（KL-reg.，参见表 8）时，并结合无分类器引导机制 [32]，还可以直接合成超过 256² 的图像，参见图 13。

### 4.4 使用潜在扩散的超分辨率

LDM 可以通过直接连接低分辨率图像来进行超分辨率训练（参见 3.3 节）。在第一次实验中，我们遵循 SR3 [72]，将图像降解固定为 4× 下采样的双三次插值，并在 ImageNet 上按照 SR3 的数据处理流程进行训练。我们使用在 OpenImages 上预训练的 $f = 4$ 自动编码模型（VQ-reg.，参见表 8），并将低分辨率条件 $y$ 与 UNet 的输入相结合，即 $τθ$ 是恒等映射。我们的定性和定量结果（参见图 10 和表 5）显示了具有竞争力的性能，LDM-SR 在 FID 上超越了 SR3，而 SR3 在 IS 上表现更好。简单的图像回归模型在 PSNR 和 SSIM 得分上表现最好；然而，这些指标与人类感知不一致 [106]，并偏向于模糊而不是高频细节对齐不完美的图像 [72]。此外，我们还进行了一项用户研究，将像素基准与 LDM-SR 进行比较。我们遵循 SR3 [72]，让受试者在两张高分辨率图像之间展示一张低分辨率图像，并询问他们的偏好。表 4 的结果证明了 LDM-SR 的优异性能。可以通过使用后处理引导机制 [15] 来提高 PSNR 和 SSIM，我们通过感知损失实现了这个基于图像的引导，参见 D.6 节。

由于双三次降解过程无法很好地泛化到未经过这种预处理的图像，我们还通过使用更多样化的降解过程训练了一个通用模型 LDM-BSR。结果见 D.6.1 节。

### 4.5 使用潜在扩散进行图像修复

图像修复是通过填充图像中被遮挡的区域来生成新内容的任务，通常用于修复损坏的图像或替换图像中不需要的内容。我们评估了我们的通用条件图像生成方法在这一任务中的表现，并与当前最先进的专门方法进行了比较。我们的评估遵循了 LaMa [88] 的协议，LaMa 是一个最近的图像修复模型，依赖于快速傅里叶卷积 [8] 的特殊架构。Places 数据集上的具体训练和评估协议见 Sec. E.2.2。

我们首先分析了第一阶段的不同设计选择对修复效率的影响。特别地，我们比较了 LDM-1（即基于像素的条件扩散模型）与 LDM-4，以及 KL 和 VQ 正则化的差异，另外还包括在第一阶段没有任何注意力机制的 VQ-LDM-4（见表 8），后者在高分辨率解码时减少了 GPU 内存消耗。为了可比性，我们为所有模型固定了参数数量。表 6 报告了 256² 和 512² 分辨率下的训练和采样吞吐量、每轮训练时间（小时）以及在验证集上六轮后的 FID 分数。总体而言，我们观察到，基于潜在空间的扩散模型在加速方面至少比像素扩散模型快 2.7 倍，同时 FID 分数至少提升了 1.6 倍。

表 7 中与其他修复方法的对比显示，带有注意力机制的模型在 FID 分数上优于 [88] 的方法。我们的样本与未遮挡图像的 LPIPS 指标稍高于 [88]，我们认为这是因为 [88] 仅生成一个结果，通常恢复了更平均的图像，而我们的 LDM 生成了更多样化的结果（参见图 21）。此外，在用户研究中（见表 4），受试者更倾向于选择我们的结果，而不是 [88] 的结果。

基于这些初步结果，我们还在 VQ 正则化的第一阶段潜在空间中训练了一个更大的扩散模型（表 7 中的 big）。参考 [15]，该扩散模型的 UNet 在其特征层次的三个层级上使用了注意力层，并采用 BigGAN [3] 残差模块进行上采样和下采样，模型参数为 387M 而不是 215M。训练后，我们注意到 256² 和 512² 分辨率下生成样本的质量存在差异，我们推测这是由附加的注意力模块引起的。然而，在 512² 分辨率下对模型进行半个 epoch 的微调，能够让模型调整到新的特征统计，并在图像修复任务上设立了新的 FID 记录（big, w/o attn, w/ ft，见表 7 和图 11）。

## 5. 限制与社会影响
### 限制
与基于像素的扩散模型相比，LDMs 显著减少了计算需求，但它们的顺序采样过程仍然比 GANs 慢。此外，当高精度要求时，LDMs 的使用可能会受到质疑：尽管在我们的 f = 4 自编码模型（见图 1）中图像质量的损失非常小，但它们的重建能力可能会成为需要像素空间精确任务的瓶颈。我们认为，我们的超分辨率模型（见 Sec. 4.4）在这方面可能已经受到一定的限制。

### 社会影响
生成模型（如图像生成）是一把双刃剑：一方面，它们支持各种创意应用，尤其是像我们的方法可以降低训练和推理成本，具有潜力促进技术的普及和探索的民主化。另一方面，它也意味着制造和传播操纵数据或传播虚假信息和垃圾信息变得更加容易。特别是，图像的故意操纵（如“深度伪造”）在这个背景下是一个常见问题，而女性尤其受到这一问题的影响 [13, 24]。

生成模型还可能泄露其训练数据 [5, 90]，这在数据包含敏感或个人信息并且未经明确同意收集时，尤为令人担忧。然而，目前尚不清楚这种问题在图像扩散模型中的程度。

最后，深度学习模型倾向于再现或加剧数据中已经存在的偏见 [22, 38, 91]。虽然扩散模型在覆盖数据分布方面比 GAN 方法表现更好，但我们使用结合对抗训练和基于似然的目标的两阶段方法在多大程度上会错误表示数据仍是一个重要的研究问题。

对于深度生成模型的伦理问题的更广泛讨论，可以参考 [13]。

## 6. 结论
我们提出了潜在扩散模型（LDMs），这是一种简单且高效的方法，显著提高了去噪扩散模型的训练和采样效率，而不会降低它们的质量。基于此以及我们的交叉注意力条件机制，我们的实验表明，与当前最先进的方法相比，LDMs 在广泛的条件图像生成任务中表现出色，而不需要任务特定的架构。