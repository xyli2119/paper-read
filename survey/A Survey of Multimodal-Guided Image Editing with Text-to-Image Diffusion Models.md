# 基于文本到图像扩散模型的多模态引导图像编辑综述

摘要—图像编辑旨在根据用户的特定需求，对给定的合成图像或真实图像进行编辑。近年来，作为人工智能生成内容（AIGC）中一个有前途且充满挑战的领域，图像编辑受到了广泛的研究。该领域最近的重要进展基于文本到图像（T2I）扩散模型的发展，这些模型可以根据文本提示生成图像。这些模型展示了显著的生成能力，已成为图像编辑中广泛使用的工具。基于T2I的图像编辑方法显著提高了编辑性能，并提供了一个用户友好的界面，用于在多模态输入的指导下修改内容。在这篇综述中，我们对利用T2I扩散模型的多模态引导图像编辑技术进行了全面的回顾。首先，我们从整体角度定义了图像编辑的范围，并详细介绍了各种控制信号和编辑场景。然后，我们提出了一个统一的框架来形式化编辑过程，并将其分为两大算法类别。该框架为用户实现特定目标提供了设计空间。接着，我们对该框架中的每个组件进行了深入分析，研究了不同组合的特征及其适用场景。鉴于基于训练的方法学习在用户指导下直接将源图像映射到目标图像，我们对此类方法进行了单独讨论，并介绍了在不同场景中注入源图像的方案。此外，我们回顾了将2D技术应用于视频编辑的情况，重点介绍了解决帧间不一致性的问题。最后，我们讨论了该领域的开放性挑战，并提出了未来可能的研究方向。我们在https://github.com/xinchengshuai/Awesome-Image-Editing上持续追踪相关工作。

**关键词**—图像编辑, AIGC, 多模态, 文本到图像扩散模型, 综述

## 1. Introduction

随着跨模态数据集 [1], [2], [3], [4], [5], [6], [7] 和生成框架 [8], [9], [10], [11], [12] 的发展，新兴的大规模文本到图像（T2I）模型 [13], [14], [15] 使人们能够创建所需的图像，从而引领了计算机视觉中的人工智能生成内容（AIGC）时代。这些工作大多基于**扩散模型** [12]，一种广受研究的流行生成框架。近年来，大量研究探索了这些基于扩散模型在其他领域的应用，例如图像编辑 [16], [17], [18], [19], [20], [21]，3D生成/编辑 [22], [23], [24]，视频生成/编辑 [25], [26], [27], [28] 等等。与图像生成不同，**编辑旨在进行二次创作，修改源图像中的目标元素，并保留与语义无关的内容**。图像编辑在质量和适用性方面仍有进一步改进的空间，使其成为一个充满前景和挑战的任务。在本工作中，我们对利用T2I扩散模型的多模态引导图像编辑技术进行了全面的综述。

有一些综述 [174], [175], [176], [177], [178] 从不同角度回顾了基于扩散的最新方法，如图像恢复 [179]，超分辨率 [176]，医学图像分析 [177] 等。与这些综述相比，我们专注于图像编辑领域的技术。有两项与我们综述相关的同期工作 [175], [178]。其中，**[178] 介绍了扩散模型在图像编辑中的应用，并根据其学习策略对相关论文进行了分类。**

与其相比，我们从一个新颖且整体的角度讨论了这一主题，并**提出了一个统一的框架来形式化编辑过程**。我们发现，**先前文献中的编辑解释是有限且不完整的** [16], [32], [66], [178]。这些工作限制了保留概念的范围，并试图从源图像中重建尽可能多的细节。然而，这种常见的设置排除了某些高级语义的维护，如**身份、风格**等。为了解决这个问题，我们首先提供了一个严格且全面的编辑定义，并在本综述中纳入了更多相关研究，如 [37], [38], [61], [146]。图1展示了符合我们定义的各种场景。值得注意的是，一些生成任务，如定制化 [41], [54] 和基于图像引导的条件生成 [37], [134]，都符合我们的讨论范围。这些任务在另一项专注于可控生成的同期工作 [175] 中有所讨论。其次，我们将回顾的方法整合到一个统一的框架中，该框架将编辑过程分为两个算法类别，即**反演算法和编辑算法**。在 [178] 中，提出了一个类似的框架，以统一不需要训练或测试时微调的方法。不同的是，我们的框架更适用于广泛的编辑场景讨论。同时，该框架为用户提供了一个设计空间，可以根据其特定目的组合适当的技术。本综述中的实验展示了不同组合的特征及其适用场景。此外，我们还研究了2D方法 [32], [180] 在视频编辑中的扩展 [165], [173]，并专注于它们解决时间不一致性问题的方法，补充了这一研究领域中缺失的部分。

<center>图1：符合我们定义的编辑任务。我们将编辑任务分为内容感知组和内容自由组，并为每种场景列举了几个源-目标对及其对应的控制信号。示例图像来自 [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40]。</center>

我们对三百多篇论文进行了广泛的调查，分析了现有方法的本质和内部逻辑。本综述主要聚焦于基于T2I扩散模型的研究 [13], [14], [181]。在第2节中，介绍了扩散模型及T2I生成中的技术，提供了基本的理论背景。在第3节中，我们给出了图像编辑的定义，并讨论了几个重要方面，如**不同模态的用户引导、编辑场景**，以及一些用于评估的定性和定量指标。同时，我们形式化了提出的统一框架，以整合现有的方法。接下来，在第4节和第5节中分别讨论了我们框架的主要组成部分。**反演算法从源图像中捕捉需要保留的概念，而编辑算法则旨在根据用户的引导再现视觉元素，实现内容一致性和语义保真度**。在第6节中，我们研究了反演算法和编辑算法的不同组合，探讨了它们的特点和适用场景，从而引导用户为不同目标选择适当的方法。由于**基于训练的方法 [20], [119], [122], [182] 学习直接将源图像转化为目标图像**，我们在第7节讨论了这些工作，并详细说明了在不同任务中注入源图像的方案。第8节介绍了图像编辑在视频领域的扩展。由于**视频数据的稀缺，直接应用图像领域的方法常常会导致帧间不一致性**。本节讨论了现有研究中的几种解决方案 [158], [164], [166], [171]。最后，我们在**第9节中讨论了尚未解决的挑战**，并提出了未来可能的研究方向。图2展示了我们工作的组织结构，并分类列出了每节中回顾的论文。

## 2.预备知识

在此，我们介绍扩散模型和文本到图像生成的一些基本方面，提供基础理论背景，以帮助理解本综述的内容。

### 2.1 去噪扩散概率模型

最近，由于其在建模复杂分布方面的强大能力，**去噪扩散概率模型（DDPMs）**[12] 在图像生成及相关任务中得到了广泛的探索 [13], [17], [24], [66], [119], [166], [183], [184]。DDPMs由两个T步的马尔可夫过程组成。在0到T的前向过程中，模型迭代地向图像中引入独立的高斯噪声。随着迭代次数接近T，噪声图像逐渐变为纯高斯噪声。反向过程中，模型将噪声逆向转化为干净的图像。从T到0，扩散模型预测当前步骤中注入的噪声并进行去噪，以推断下一步的潜在变量。

- **DDPM中的前向与反向过程**。给定输入图像$z_0$和步骤$1 \leq t \leq T$，DDPM的前向过程 $F_{fw}^{DP}$ 形式化为：

$$
z_t = F_{fw}^{DP} (z_{t-1}, t - 1) = \sqrt{1 - \beta_t}z_{t-1} + \sqrt{\beta_t}\epsilon_t,(1)
$$

其中，$\beta_t$ 是由方差调度器指定的超参数，且 $\epsilon_t \sim N(0, I)$ 是注入的噪声。此外，$z_t$ 也可以直接从 $z_0$ 推导得到：

$$
z_t = \sqrt{\bar{\alpha_t}}z_0 + \sqrt{1 - \bar{\alpha_t}}\epsilon^0_t,(2)
$$

其中，$\bar{\alpha_t} = \prod_{i=1}^t \alpha_i$ 且 $\alpha_t = 1 − \beta_t$。$\epsilon^0_t$ 是 $t$ 步中引入的高斯噪声。反向过程 $F_{bw}^{DP}$ 将潜在变量去噪为干净图像，其形式化为：

$$
z_{t-1} = F_{bw}^{DP} (z_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( z_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha_t}}} \epsilon_\theta (z_t, t) \right) + \sigma_t \hat{\epsilon_t},(3)
$$

其中，$epsilon_\theta (z_t, t)$ 是对注入噪声的估计值，$\theta$ 代表网络参数。$\sigma_t$ 和 $\hat{\epsilon_t}$ 分别是反向过程中的超参数和随机成分。

对于噪声估计网络 $\epsilon_\theta$ 的训练，研究 [12] 提出通过最小化负对数似然的变分下界 [9] 来实现，其目标是解决以下问题：

$$
\arg \min_\theta \mathbb{E}_{z_0 \sim p_{\text{data}}, t \sim U(1,T), \epsilon \sim N(0,I)} 
\left[ \lambda_t \|\epsilon_\theta (z_t, t) - \epsilon\|^2 \right],(4)
$$

其中，$p_{\text{data}}$ 是训练数据的分布。$z_t$ 通过公式2获得，$\lambda_t$ 是时间变化的加权因子。

- **DDIM中的反演与采样过程**。**去噪扩散隐式模型（DDIM）**[185] 加速了DDPM中的采样过程，其采样过程 $F_{bw}^{DI}$ 给出如下公式：

$$
z_{t-1} = F_{bw}^{DI} (z_t, t) = \sqrt{\bar{\alpha}_{t-1}} \cdot z_t - \frac{\sqrt{1 - \bar{\alpha}_t} \cdot \epsilon_\theta(z_t, t)}{\sqrt{\bar{\alpha}_t}} + \sqrt{1 - \bar{\alpha}_{t-1}} \cdot \epsilon_\theta(z_t, t).(5)
$$

重新排列公式5可以得出DDIM的反演过程 $F_{fw}^{DI}$：

$$
z_t = F_{fw}^{DI} (z_{t-1}, t - 1) = \frac{\sqrt{\bar{\alpha}_t} \cdot z_{t-1} - \sqrt{1 - \bar{\alpha}_{t-1}} \cdot \epsilon_\theta(z_t, t)}{\sqrt{\bar{\alpha}_{t-1}}} + \sqrt{1 - \bar{\alpha}_t} \cdot \epsilon_\theta(z_t, t),(6)
$$

该**过程广泛用于将真实图像反演到潜在空间中。**

- **条件生成**。对于可控生成，分类器引导 [186] 引入了一个与噪声相关的分类器来引导反向过程。无分类器引导 [187] 提供了一种不需要辅助分类器的解决方案。它在反向过程中结合了条件噪声估计 $\epsilon_\theta(z_t, t, C)$ 和无条件项 $\epsilon_\theta(z_t, t, \emptyset)$，其中$C$ 和 $\emptyset$ 分别表示有条件和无条件状态。在无分类器引导中，预测的噪声表达为：

$$
\tilde{\epsilon}_\theta (z_t, t, C, \emptyset, \omega) = \omega \cdot \epsilon_\theta (z_t, t, C) + (1 - \omega) \cdot \epsilon_\theta (z_t, t, \emptyset),(7)
$$

其中，$\omega$ 是引导比例，$\omega$ 越大，条件$C$ 对最终结果的影响越大。

- **得分函数**。从基于得分的角度来看 [188], [189]，扩散过程 [12] 可以被推广为随机微分方程（SDE）的表示形式，其形式化如下：
  $$
  dz = f(z, t) + g(z)dw,(8)
  $$
  其中，$f(z, t)$ 和 $g(z)$ 分别是漂移系数和扩散系数，$w$ 表示布朗运动。方程 (8) 的反向过程表示为：

  $$
  dz = (f(z, t) - g(z)^2 \nabla_z \log p_t(z)) dt + g(z) dw,(9)
  $$

  其中，$\nabla_z \log p_t(z)$ 是 $p_t(z)$ 的得分函数，定义为带噪声的边缘分布概率的对数的梯度。此外，条件得分函数定义为：

  $$
  \nabla_z \log p_t(z | C) = \nabla_z \log \left( \frac{p_t(C | z)}{p_t(C)} \right) \propto \nabla_z \log p_t(z) + \nabla_z \log p_t(C | z),(10)
  $$
  其中，$\log p_t(C | z)$ 表示 $C$ 的后验概率。根据相关文献 [186], [190]，在 DDPMs 中估计的噪声可以表示为以下等价形式：

  $$
  \epsilon_\theta(z_t, t, C) = -\sqrt{1 - \bar{\alpha}_t} \nabla_{z_t} \log p_t(z_t | C),(11)
  $$
  这有助于得分函数和预测噪声之间的相互转换。

### 2.2 文到图生成

最近，许多大模型 [13], [14], [15], [181], [191] 被开发出来用于实现文本驱动的生成，这些模型也赋能了其他相关领域 [19], [25], [35], [168], [192], [193], [194], [195]。为了便于讨论，我们在下文中介绍了T2I（文本到图像）生成的一些基本方面。

- **文本编码器**。文本编码器 [196], [197], [198] 是当前T2I模型中不可或缺的组件之一，用于**从文本输入中提取富有表现力的语义特征**。在这些模型中，CLIP [196] 通过训练文本编码器和图像编码器以及4亿对文本-图像对，将文本和图像嵌入到一个多模态空间中。对于大型语言模型（LLM），T5 [197] 和 BERT [198] 在仅包含文本的语料库上进行训练，并具备强大的理解和推理能力。文献 [14] 表明，这些模型在图像生成方面表现出不同的效果。为了处理文本输入，文本编码器将每个离散的单词转换为唯一的token，并用于在学习的查找表中检索token嵌入。最终，这些词级特征会进一步处理，得到包含整体语义的文本嵌入。

- **注意力机制**。Transformer块 [199] 是现有大模型中的默认内置模块，因为它具有良好的扩展性。具体来说，交叉注意力（cross-attention）使图像特征与文本嵌入之间能够进行通信，从而促进文本引导的生成，而自注意力（self-attention）负责捕捉图像块的空间相关性。注意力机制的过程形式化为：

$$
\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V = AV,(12)
$$
其中，$Q$、$K$ 和 $V$ 分别是查询、键和值，$d$ 表示隐藏维度，$A$ 是注意力图。在自注意力中，这些变量都是通过相应的投影权重从图像特征中计算出来的。而在交叉注意力中，$K$ 和 $V$ 是从文本特征中获取的，用于跨模态计算。

- **文本到图像模型**。受益于复杂的文本编码器 [196], [197]，存在多种T2I模型，如StableDiffusion [13]、Imagen [14]、DALLE-2 [15]、GLIDE [191] 等等 [181], [200]。与方程 (4) 略有不同，这些模型的训练目标表示为：

$$
\mathbb{E}_{(z_0,C) \sim p_{\text{data}}, t \sim U(1,T), \epsilon \sim \mathcal{N}(0,I)} \left[ \lambda_t \|\epsilon_\theta (z_t, t, C) - \epsilon\|^2 \right],(13)
$$
其中，$C$ 被指定为文本条件。在这些模型中，由于**StableDiffusion的代码是开源**的，它在后续的研究中被广泛研究 [22], [32], [171]。StableDiffusion基于潜在扩散模型（LDM） [13] 构建，该模型使用自动编码器 [9], [201], [202] 将输入图像投射到低维空间中，以提高计算效率。同时，它采用U-Net [203] 作为噪声估计器的架构，并在潜在空间中运行。

### 2.3 符号表示

我们在表1中列出了常用的符号，以便于理解本综述。在后续部分中，如果有必要，我们将在公式中标明条件 $C$，以表示包含条件输入，例如 $F_{bw}^{DI}(z_t, t, C)$。同时，在不需要区分的情况下，我们将DDPM和DDIM的正向（反演）/反向（采样）过程统一表示为 $F_{fw}$ / $F_{bw}$。

## 3 问题表述
在研究高级方法之前，我们首先给出我们对图像编辑的定义，并介绍几个关键方面，如在我们主题中涉及的多模态用户引导和编辑场景。此外，我们对所提出的统一框架进行了详细介绍。表2展示了这些代表性方法的元素。

表2：代表性工作的总结。 我们使用以“T”开头的缩写来表示编辑场景：对象操作（TOM）/ 属性操作（TAM）/ 空间变换（TST）/ 图像修复（TI）/ 风格转换（TSC）/ 图像转换（TIT）/ 主题驱动定制（TSDC）/ 属性驱动定制（TADC）。对于条件，我们使用以“G”开头的缩写：静态文本（GST）/ 指令（GI）/ 自然图像（GNI）/ 掩码（GM）/ 用户界面（GUI）。对于免训练方法，我们使用以“I”和“E”开头的缩写分别表示反演和编辑算法：基于微调的反演（IT）/ 基于前向的反演（IF），普通编辑（EN）/ 基于注意力的编辑（EA）/ 基于混合的编辑（EB）/ 基于得分的编辑（ES）/ 基于优化的编辑（EO）。对于基于训练的方法，我们使用以“S”开头的缩写来表示注入方案：图像拼接（SIC）/ 潜在混合（SLB）/ 图像适配器（SIA）/ 文本空间适配器（STSA）/ 潜在空间适配器（SLSA）。

### 3.1 多模态引导的图像编辑定义

给定源图像集 $SI$ 和多模态引导集 $G$，图像编辑旨在识别在特定场景中需要保留的视觉元素，并生成编辑后的图像 $z_{e0}$，该图像既保留了 $SI$ 中的期望内容，又反映了 $G$ 中涉及的编辑目标。在我们看来，保留的概念不仅指低级语义，例如与编辑无关区域的像素，还包括一些高级语义，例如身份或其他属性。图1展示了低级语义的示例（第1-6行）和高级语义的示例（第7-8行）。

### 3.2 多模态用户引导
对于由 $G$ 引导的可控编辑，我们列出了以下几种常用的不同模态的控制信号。

- **自然语言**。自然语言对于人类来说是一种方便且灵活的描述特定目的的方式，近年来被广泛应用于研究中 [16], [32], [38], [41], [66]。文本引导有几种形式，常见的是静态文本，通过完整的描述来表示目标。例如，“一只蓝色的狗”表明用户想要将狗的毛发变成蓝色。对于一些工作 [66], [80]，需要一对描述来说明编辑前后的差异。相比之下，指令更为灵活，通过一个命令式句子来描述目的，例如“将狗变成蓝色”。
- **图像**。图像是一种直观的表示，传达了难以用语言表达的视觉内容的语义。常见形式是自然图像 [31], [33]，可以通过普遍存在的设备捕捉，或由生成模型 [14], [181] 合成。例如，如果有人希望将他的绘画风格转移到源图像中，他只需要将一定数量的作品输入编辑方法。另一种形式是掩码 [246], [247]，本质上是一种二值图像，用于指示要修改的感兴趣区域。掩码通常用作实现局部编辑的辅助条件。
- **用户界面**。用户界面，如鼠标操作（例如点击和拖动）[35], [109]，滑动条和输入框 [34] 等，提供了一种交互方式来操作图像。算法负责将特定的用户操作转换为可识别的数值参数。例如，用户可以将对象拖动到目标位置，编辑方法会将鼠标操作转换为点坐标 [35]。
- **控制信号的组合**。由于用户引导的丰富性，一些方法 [81], [92], [109], [110], [153] 同时接受不同模态的多个控制信号。例如，一些工作 [92], [127] 使用文本提示和掩码来完成填充任务，以语义内容填充区域。

### 3.3 编辑场景
我们列举了在所审查的方法中涉及的大多数编辑场景/任务，并将其分为两组。

- **内容感知编辑**。属于这一组的任务旨在保持与编辑无关的低级语义，同时实现 $G$ 中所指示的目标。
  1) 局部编辑。该子组在局部区域修改图像。
    1. 对象操作。此任务指的是指定对象的添加/删除/替换 **[32]**, [66]。
    2. 属性操作。此类别旨在增强/减弱/改变对象的内在属性，如颜色、纹理、姿势和动作等 [17], [33], [69], [82]。
    3. 空间变换。此任务改变对象的空间属性 [35], [109], [110]，如平移、缩放和局部变形。
    4. 图像修复。此类别在源图像中的感兴趣区域填充一致的内容 [92], [93]。
  2) 全局编辑。此子组中的任务修改源图像的全局语义。
    1. 风格转换。此任务将源图像的风格更改为另一种风格 [33], [248]。
    2. 图像转换。该任务将图像从源域转换到目标域，例如从深度图到自然图像 [37], [134]。

- **内容无关编辑**。与内容感知编辑任务相反，这一组任务旨在保留高级语义，并在由 $G$ 引导的全新上下文中重现概念。
  1) 主题驱动定制：此任务 [57], [146] 生成目标主体的新图像，同时保持其身份。
  2) 属性驱动定制：与学习整体概念不同，此任务 [48], [50] 提取解耦的属性，如形状、风格、纹理和动作等，这些属性可以分配给其他对象。

特别是，**内容感知编辑是当前工作中讨论的主要主题** [16], [32], [178]。与这一常见设置不同，我们的定义扩展了讨论范围，其中保留的概念涉及更多方面，如身份、风格等。因此，满足我们定义的几项生成任务也包括在本综述中，如定制 [38], [41], [46], [54] 和图像引导下的条件生成 [37], [133], [147]。我们在图1中展示了不同控制信号下的各种编辑场景。值得注意的是，由于文本和图像条件都难以描述空间变换中的编辑目的，该类别中的大多数任务通过用户界面（如鼠标操作）获得指导。

### 3.4 图像编辑的评估
无论编辑任务是什么，都有两个基本指标分别用于评估对源图像和引导的保真度：

1) **内容一致性**：编辑后的图像 $z_{e0}$ 必须保留 $SI$ 中期望的视觉元素。几个定量指标可以衡量源图像和编辑图像的一致性，如 CLIP-I [196]、LPIPS [249] 和 FID [250]，其中 **CLIP-I 计算 CLIP 图像嵌入的余弦相似度**。
2) **语义保真度**：编辑后的图像 $z_{e0}$ 必须反映 $G$ 中涉及的编辑目标。对于定量评估，**CLIP-T [196] 被用来衡量对文本提示的保真度**，它计算来自 CLIP 的图像和文本嵌入的余弦相似度。此外，方向性 CLIP 相似度（directional CLIP similarity） [251] 接收图像对及其源描述和目标描述，以指示编辑方向的准确性。

### 3.5 统一框架
当前的方法可以整合到一个统一的框架中，其中编辑过程分为两个主要的算法家族。

- **反演算法**。反演算法 $F_{\text{inv}}$ 将源图像编码为称为反演线索的 $\Phi_I$，该线索在编辑阶段用于重建期望的内容。该过程表示为：

$$
\Phi_I = F_{\text{inv}}(SI , CI ),(14)
$$

其中，$CI$ 是标识原始内容的源提示词。我们在图3的顶部展示了不同的算法。值得注意的是，方程 (14) 也适用于图像条件，因为用户希望在 $z_{e0}$ 中保留参考图像中的某些内容。

- **编辑算法**。通过根据不同的反演方法将 $\Phi_I$ 融入基础模型，编辑算法随后使用 $G$ 中的 $CI$ 变体版本来重建保留的内容并实现目标，例如图3中的“狗”和“戴眼镜的狗”。正式来说，编辑算法 $F_{\text{edit}}$ 的目标是生成编辑后的图像：

$$
z_{e0} = F_{\text{edit}}(\Phi_I , G),(15)
$$

具体来说，$F_{\text{edit}}$ 介入反向过程，如下所示：

$$
F_{\text{bw}}(z_t, t, C | \Phi_I , G)\{ · \},(16)
$$

其中，$F_{\text{bw}}(z_t, t, C | \Phi_I , G)$ 表明反向过程是基于 $\Phi_I$ 和 $G$ 进行的。同时，$\{ · \}$ 代表 $F_{\text{edit}}$ 的操作，增强了内容一致性和语义保真度。值得注意的是，像掩码或来自用户界面的数值参数这样的控制信号仅在 $\{ · \}$ 中处理，因为它们无法被T2I模型识别。此外，我们还将 $F_{\text{edit}}$ 的常规版本称为 $F_{\text{edit}}^{\text{Norm}}$：

$$
F_{\text{bw}}(z_t, t, C | \Phi_I , G),(17)
$$

这表明没有干预。

- **反演与编辑算法的协作**。我们通过图3中的一个简单案例来说明反演和编辑算法的协作。如图3所示，其他编辑算法在保留狗的外观方面优于 $F_{\text{edit}}^{\text{Norm}}$。例如，基于混合的算法将 $CI$ 的文本嵌入与目标嵌入融合，并使用混合特征来引导反向过程，以实现对源图像和文本引导的保真度。基于注意力的算法将源图像的注意力图注入，以保留狗的细节。

通过我们提出的框架，用户能够结合适当的方法来实现特定的目的。我们在表2中展示了审查方法的反演和编辑算法。值得注意的是，许多研究 [32], [88], [102] 同时采用了多种编辑算法。为了简化，我们仅根据他们使用的主要技术对其进行分类。图4展示了几个代表性工作，以说明我们框架在多任务中的适用性。

## 4 反演算法
在基于GAN的方法的文献中 [252], [253]，**反演是指将自然图像嵌入到潜在空间的过程**，**然后将该表示输入到GAN中进行重建**。在本节中，我们研究了基于扩散模型的反演算法 [38], [41], [66], [80]，并将其分为**基于微调的反演和基于前向的反演**，分别表示为 $F_{\text{inv}}^{T}$ 和 $F_{\text{inv}}^{F}$。图3展示了它们的思想。

### 4.1 基于微调的反演
为了重新创建源内容，一大类研究 [36], [38], [41] 利用扩散模型的原始训练过程将源图像植入生成分布。在这一范式下，$F_{\text{inv}}^{T}$ 的目标是解决以下问题：

$$
\text{arg min}_{\theta_I} \mathbb{E}_{z_0^s \sim p(SI), t, \epsilon} \left[ \lambda_t \|\epsilon_{\theta \cup \theta_I}(z_t^s, t, CI) - \epsilon_t\|^2 \right]，(18)
$$
其中，$z_0^s$ 是从 $SI$ 中采样的源图像。$\theta_I$ 表示要更新的参数，$\cup$ 表示并集运算。其他项与方程 (13) 相同。在这一范式下，$\Phi_I = \theta_I$。在编辑时，这些方法将 $\theta_I$ 加载到基础模型中，并使用 $CI$ 的变体来重建保留的概念。

根据方程 (18) 和图3的左上角，基于微调的反演算法优化所有潜在的去噪路径，以从任意高斯噪声中重建源图像。$F_{\text{inv}}^{T}$ 具有以下特点：

1. 由于 $F_{\text{inv}}^{T}$ **优化了大量的去噪轨迹**，它具有相当强的重建能力。
2. 由于基于 $F_{\text{inv}}^{T}$ 的方法从随机噪声中进行采样，因此它们在图像布局方面具有很高的灵活性。然而，这伴随着较长的微调时间，并且由于单次或少量微调，可能会丧失一定程度的生成能力。

因此，**$F_{\text{inv}}^{T}$ 主要用于内容无关的编辑任务** [38], [41], [55], [60], [205]，在内容感知的场景中也有一些应用 [19], [81], [101], [107]，因为前者需要更高的灵活性，对保持图像结构和低级语义的要求较低。我们主要讨论内容无关编辑任务中的挑战及相应的解决方案，同时根据其微调空间组织这些方法。

#### 4.1.1 文本空间
对于T2I模型 [13], [14], [181]，文本编码器 [196], [197], [254] 被用来从文本提示中提取代表性特征，促进跨模态计算以生成创意内容。因此，**有一条研究路线旨在优化文本嵌入 ($\theta_I$) 以完成不同的编辑任务** [41], [42], [43], [44]。然而，**由于参数规模的限制，在文本空间中进行微调往往导致较差的重建性能**。文献 [47], [48], [255] 表明，文本嵌入往往捕捉全局语义，而**忽视细节**。因此，一些研究 [45], [46] 扩展了普通空间以解决这个问题。相反，其他方法 [40], [48], [50] 利用这一特性，尝试从图像中解构纠缠的属性。

- **文本空间的扩展**。一组方法 [41], [42], [43], [44] 探索了文本空间在不同编辑场景中的有效性，而其他研究 [45], [46] 则致力于通过空间扩展来增强重建能力。对于定制化，Textual-Inversion (TI) [41] 引入了一个可学习的词嵌入来表示主体，并将一个稀有的token分配给它作为标识符，例如“sks”。通过构建包含该稀有token的 $CI$（例如“一张<rare_token>的照片”）并将方程 (18) 应用于文本-图像对，TI 保留了主体的身份。此外，DreamArtist [42] 联合优化正负嵌入 [186] 以缓解在单一图像上进行微调时的过拟合问题。具体来说，负嵌入用于纠正正嵌入所犯的错误。其他工作 [43], [44] 则在内容感知的编辑任务中应用了这一技术。UniTune [43] 优化了 $CI$ 的嵌入。在编辑阶段，它将 $CI$ 与目标提示词连接起来以保留基本内容。不同的是，HiPer [44] 优化了文本嵌入的尾部以获得更好的一致性，并用目标嵌入替换初始部分。

由于参数数量的限制，这些方法在保持细节方面表现不足。基于StableDiffusion [13]，XTI [45] 引入了扩展的文本条件空间 $P^+$ 以缓解欠拟合问题。与TI [41] 不同，XTI 为每个U-Net层 [13] 学习了独立的向量。层级嵌入有助于提取多级特征以实现更好的重建。NeTI [46] 进一步沿时间维度扩展了 $P^+$。具体来说，它训练了一个投影网络，将当前的去噪步骤和层号映射到词嵌入。同时，该方法在最后一层线性层添加了嵌套降噪层 [256]，通过调整截断值来平衡重建能力和可编辑性。

- **属性的提取**。根据相关研究 [47], [255]，文本空间倾向于捕捉域内特征。一些方法 [39], [47], [48] 利用这一特性从整体概念中解构纠缠的属性。由于**每个去噪阶段负责不同的语义**，ProSpect [47] 引入了一个超网络来推断时间上的嵌入。这些特征捕捉了不同的属性，如材料和形状等。此外，MATTE [39] 通过更精细的方法实现了这一目标。与NeTI [46] 类似，它在层级和时间维度上扩展了文本空间。在编辑时，该方法使用嵌入的子集来组合不同的属性。概念分解 [48] 提供了一个更直观的解决方案。它使用二叉树来表示不同语义层次的概念。在每次迭代中，该方法通过在组合的子标识符和父嵌入生成的图像上应用方程 (18)，分解当前节点指示的概念。

与具体的视觉内容相比，从有限的图像中提取隐性属性更具挑战性。在这种情况下，单纯使用像素级扩散损失是不足的。一些方法 [40], [49] 旨在从源图像中学习动作。由于自然语言中的介词表示了不同主体的关联，Reversion [40] 利用对比损失将动作嵌入与采样的介词嵌入对齐，同时将其与其他词类词（POE）区分开来。从另一个角度来看，Action-Disentangled Identifier (ADI) [49] 通过计算源图像和辅助图像之间的梯度差异，寻找与动作相关的词嵌入通道，并仅对这些关键参数应用梯度下降。此外，Lego [50] 旨在从示例图像中提取更一般的概念。它首先为主体和要学习的概念分别分配不同的token和词嵌入。在微调过程中，主体图像的 $CI$ 只包含“<subject_token>”，而其他图像在此基础上添加“<concept_token>”以指示新属性的注入。同时，受Reversion的启发，该方法在文本空间中使用对比损失将这些嵌入分离开来。

#### 4.1.2 模型空间
一大类方法 [38], [53], [61] 通过更新基础模型中的模块 ($\theta_I$) 来提高重建能力。例如，一些工作 [19], [35], [101] 在非刚性编辑或局部变形后，为了保持内容一致性，优化了文本嵌入和模型参数。其他方法 [38], [54] 则在定制化中增强了细节。然而，**更新大量参数会引发语言漂移 [257], [258] 和灾难性遗忘问题**。也就是说，模型在微调后会忘记先前的知识。此外，它们在**解耦无关内容（如背景）方面也存在困难**，这削弱了编辑对象的身份。这些过拟合现象在不同程度上削弱了编辑能力。针对这些问题，提出了以下几种解决方案。

- **先验知识保持正则化**。一组工作 [38], [51], [52], [53] 通过正则化微调过程来保持先验知识。作为先驱工作，DreamBooth [38] 引入了类别特定的正则化数据集，其中包含通过原始模型生成的若干先验图像。通过在正则化数据和源图像上联合微调，它缓解了过拟合问题。文献 [51] 通过构建复杂的提示模板以生成更丰富的数据，进一步增强了正则化数据集。DCO [52] 提出了在没有正则化数据集的情况下，减少Kullback-Leibler散度（用于衡量两个概率分布的相似性的度量指标）（KLD）的方法。借鉴直接偏好优化（DPO）[259] 的思想，DCO通过保持先验知识来微调基础模型。此外，为了保持个性化主体的内在特性，FaceChain-SuDe [53] 通过增加条件概率 $p(C_{\text{cate}}|z_t, t, CI)$ 的正则化来保留定制主体的超类别 $C_{\text{cate}}$。这一新的正则化项鼓励方法保留来自超类别的私人属性。
- **高效微调**。受到参数高效微调（PEFT）[58] 的启发，**一条研究路线 [54], [57], [59], [205] 优化了一小部分参数以加速反演过程**，同时缓解过拟合问题。一些研究 [54], [56] 基于既定指标识别关键参数。Custom Diffusion [54] 验证了大多数参数在微调后相对变化很小，**除了交叉注意力模块。因此，该方法更新了键值的投影权重，同时冻结了其他层**。另一方面，Cones [56] 通过缩小其值并检查扩散损失是否减少，确定了重要的神经元。其他工作 [57], [204] 在特定层中引入了额外的参数以进行高效微调。SVDiff [57] 利用奇异值分解（SVD）学习权重偏移 $\Delta W = U \Delta \Sigma V^T$，其中 $\Delta \Sigma$ 是奇异值矩阵的变化。LoRA [58] 将权重偏移分解为两个低秩矩阵的乘积 $\Delta W = DU$，并寻找最优的下降矩阵 $D$ 和上升矩阵 $U$。此外，ANOVA [59] 提出了一个设计空间，用于更高效地注入LoRA模块。
- **无关语义的解耦（降低系统中不同部分之间的依赖性）**。特定概念的定制通常会受到无关内容（如背景或其他对象）的影响，这会**由于概念耦合而削弱编辑能力**。为了解决这一挑战，一些工作通过精心设计的模板 [51] 或多模态模型估计的描述性标题 [101], [260] 来增强 $CI$，以包含图像中指示的各种概念。基本上，这些研究假设生成模型能够在语义空间中学习文本token与图像特征之间的关联，同时解耦不同的概念。其他方法 [61], [62] 需要二值掩码来分离图像空间中的视觉元素，这些掩码由用户提供或通过分割网络 [213], [261], [262] 预测。对于同时从单一图像中定制多个主体的情况，Break-a-Scene [61] 只在掩码区域内应用方程 (18)，排除无关内容的影响。同时，该方法通过最小化交叉注意力图与相应掩码之间的差异，使每个主体的交叉注意力图与相应掩码对齐，从而防止注意力泄漏。借鉴这一思想，Clic [62] 旨在从整体概念中学习局部概念。具体来说，它进一步考虑了掩码外区域，以整合上下文信息，并在软掩码内应用扩散损失。

#### 4.1.3 图像适配器空间
除了基础模型中的内在参数外，另一类研究 [36], [63], [65] 通过调优附加的适配器网络 ($\theta_I$) 来进行反演，该网络将源图像的特征整合到反向过程中。由于来自CLIP [196] 的图像和文本特征在联合空间中是对齐的，一些方法 [36], [63], [64] 引入了投影网络，将图像嵌入映射到文本空间，并通过交叉注意力模块将特征注入扩散模型。为了将参考图像的风格转移到目标图像上，InST [36] 提出了基于多层注意力的辅助适配器，以提取风格图像的全局语义。其他方法 [63], [64] 使用适配器来解耦不同的视觉组件。DisenBooth [63] 引入了一个多层感知器（MLP），从CLIP图像嵌入中提取与背景相关的特征，而主体身份则在原始文本空间中学习。此外，方法 [64] 进一步学习与姿势相关的特征，以实现更细致的解耦。与使用CLIP编码器不同，ViCo [65] 利用本地噪声估计器从源图像中提取更细致的细节。具体来说，该方法将可学习的注意力层注入基础模型，其中查询由生成的图像计算，键和值来自源图像。像素级的相互计算使得ViCo在身份保留方面表现出色。

### 4.2 基于前向的反演
扩散模型中的前向过程自然地将图像反演到噪声空间，而反向过程则基于这个初始点重新创建输入图像。在没有繁琐的测试时微调的情况下，这一理念在许多研究中得到了广泛的研究和改进 [66], [70], [71], [75], [86], [115]，以适应各种任务。具体来说，这些方法**旨在逆转由前向过程生成的固定轨迹/路径 [12], [185] 以重现源图像**。形式上，$F_{\text{inv}}^{F}$ 中反演路径和重建路径的中间噪声潜变量表示为：

$$
[z_{t+1}^{s,*}]_{t=0}^{T-1} = [F_{\text{fw}}(z_t^{s,*}, t, C^*)]_{t=0}^{T-1}, \text{使得} z_0^{s,*} = z_0^{s},\\
[z_t^{s,\prime}]_{t=0}^{T-1} = [F_{\text{bw}}(z_{t+1}^{s,\prime}, t + 1, CI | \theta_F)]_{t=0}^{T-1}, \text{使得} z_T^{s,\prime} = z_T^{s,*},(19)
$$
其中，$z_0^{s}$ 是来自 $SI$ 的源图像。上标 * 和 ′ 分别表示反演路径和重建路径中的变量。$\theta_F$ 表示与方法相关的参数 [66], [70], [80]，用于对齐 $z_t^{s,*}$ 和 $z_t^{s,\prime}$。例如，NTI [66] 优化“空文本”嵌入 ($\theta_F$) 以最小化 $\|z_t^{s,*} - z_t^{s,\prime}\|^2$。$F_{\text{inv}}^{F}$ 的目标表示为：

$$
\text{arg min}_{\theta_F} \|z_t^{s,\prime}|\theta_F - z_t^{s,*}\|^2, \text{使得} \forall t \in [0, ..., T - 1], z_T^{s,\prime} = z_T^{s,*},(20)
$$
其中，$z_t^{s,\prime}|\theta_F$ 表示 $z_t^{s,\prime}$ 与 $\theta_F$ 相关。具体来说，$F_{\text{inv}}^{F}$ 将源图像编码为 $z_T^{s,*}$ 和 $\theta_F$，即 $\Phi_I = (z_T^{s,*}, \theta_F)$。在编辑过程中，方法使用 $z_T^{s,*}$ 作为初始点，同时结合 $\theta_F$ 来保留最终结果中的基本内容。

根据方程 (20) 和图3的右上部分，本组方法仅重建单个去噪路径。$F_{\text{inv}}^{F}$ 具有以下几个特点：1. 由于源图像编码在初始潜变量中，因此在采样过程中保留了图像的基本内容。2. 与 $F_{\text{inv}}^{T}$ 相比，$F_{\text{inv}}^{F}$ 不改变输出分布，从而保持了基础模型的生成能力。因此，这一类方法通常用于内容感知的编辑任务，因为这些任务需要保留低级语义。在本节中，我们根据它们的前向过程 [12], [185] 对方法进行了分类。

#### 4.2.1 DDIM反演
早期的研究 [32], [185], [187] 利用DDIM [185] 的确定性采样特性来反演真实图像，其中方程 (20) 中的 $F_{\text{fw}}$ 和 $F_{\text{bw}}$ 分别对应 $F_{\text{fw}}^{DI}$ 和 $F_{\text{bw}}^{DI}$。如方程 (6) 所示，由于在DDIM反演过程中 $z_t^{s,*}$ 是不可访问的，常见的解决方案是使用 $\epsilon_\theta(z_t^{s,*}, t)$，假设步长无限小时，这种方法几乎是正确的。然而，根据文献 [66], [70], [74]，当与大规模的指导系数 $\omega$ 一起应用无分类器指导 [186] 时，**上述近似常常会导致显著的累计误差**，导致较差的重建效果。此外，由于噪声图的元素彼此之间并不独立，这与扩散模型的训练阶段并不完全相同，普通的DDIM反演往往会导致可编辑性的下降。对于广泛使用的无分类器指导策略，这些问题是不可接受的。在本节中，我们重点讨论这些问题的解决方案。

- **反演轨迹的近似**。为了减轻累计误差，一些方法 [66], [70] 试图近似反演轨迹。一组研究 [66], [67], [68], [69] 引入了可学习的参数 ($\theta_F$) 来减少 $z_t^{s,*}$ 和 $z_t^{s,\prime}$ 之间的差异。Null-Text Inversion (NTI) [66] 是第一个考虑真实图像编辑问题的方法。为了解决由高 $\omega$ 引起的可编辑性下降，反演轨迹通过在无分类器指导中设置 $\omega = 1$ 来生成。为了防止重建轨迹偏离反演路径，该方法通过最小化 $||z_t^{s,*} - z_t^{s,\prime}||^2$ 来优化每一步的“空文本”嵌入 $\emptyset$，其中 $z_t^{s,\prime}$ 是通过较大 $\omega$ 获得的。相比之下，Prompt Tuning Inversion (PTI) [67] 优化了文本嵌入，该嵌入在编辑时与目标嵌入进行插值。与这些方法不同，StyleDiffusion [68] 将误差编码到交叉注意力模块的值矩阵中，以获得更好的一致性。对于非刚性编辑，KV Inversion [69] 优化了自注意力模块中的键和值的特征偏移以及相应的权重因子。在编辑时，生成图像的键和值与学习到的键和值相加，以保持基本内容。

此外，一些方法 [70], [71], [108] 旨在避免上述方法 [66], [67], [69] 中耗时的优化过程，同时继承它们的重建能力。Negative-Prompt Inversion (NPI) [70] 假设模型在相邻步骤中具有相似的噪声预测。在这一假设下，该方法验证了在NTI [66] 中优化的“空文本”嵌入在每一步中等效于源嵌入。受益于这一近似，该方法绕过了繁琐的优化过程，节省了推理时间。基于NPI，ProxEdit [71] 进一步约束了语义无关区域中的条件和无条件项之间的差异，以获得更好的一致性。

其他研究 [72], [73] 通过在收缩假设下解决 $z_t^{s,*}$ 作为前向函数的固定点来解决这个问题：$z_t^{s,*} = f(z_t^{s,*})$，其中 $f$ 是方程 (6) 的右侧项。在每个步骤中，Fixed-point Inversion (FPI) [72] 执行 $F_{\text{fw}}^{DI}$ $N$ 次，作为 $z_t^{i+1} = f(z_t^i)$，$i = 0, 1, .., N$，其中 $z_t^0 = z_{t-1}^N$，为了简洁，省略了上标 $s, *$。当 $z_t^{i+1}$ 和 $z_t^i$ 之间的差异足够小时或迭代次数达到上限时，操作停止。此外，加速迭代扩散迭代 (AIDI) [73] 通过提出的Anderson加速算法的变体进一步加快了收敛速度。

- **精确的DDIM反演**。另一条研究路线 [29], [74], [75] 并非模拟前向轨迹，而是建立精确的DDIM反演。受归一化流模型 [263], [264] 的启发，EDICT [74] 重新定义了DDIM过程，并在每一步的反演过程中跟踪两个相关的噪声变量，这些变量在采样时可以精确地从彼此中推导出来。双向积分近似 (BDIA) [75] 通过减少神经函数评估（NFE）的数量进一步加快了这一过程。具体来说，该方法在反向步骤中建立了当前噪声潜变量与若干前述变量之间的关系，并反转方程以获得精确的反演过程。从更直观的角度来看，PnP Inversion [76] 计算并存储每个步骤 $z_t^{s,\prime}$ 和 $z_t^{s,*}$ 之间的差异，并在编辑时将其添加到噪声变量中。
- **可编辑性的提升**。对于普通的DDIM反演轨迹，中间步骤中估计的噪声图不符合纯高斯噪声的统计特性，导致可编辑性较差。为了解决这一挑战，pix2pix-zero [18] 及其后续工作 [77] 引入了自相关损失来指导反演过程，该损失正则化预测的噪声以防止偏离分布。从另一个角度来看，研究 [68], [78] 通过修正文本token与图像特征在交叉注意力图中的对应关系来纠正不准确的语义对齐。基于NTI [66]，DPL [78] 在每个去噪步骤中引入了额外的损失项。具体来说，这些约束旨在减少来自不同视觉组件的交叉注意力图的余弦相似性，从而缓解注意力泄漏并增强可编辑性。

#### 4.2.2 DDPM反演
另一类方法 [30], [79], [80], [105], [180] 探索了DDPM反演空间的特性。这些方法仅使用 $F_{\text{fw}}^{DP}$ 注入随机性到图像中，而无需进行噪声估计。由于每个步骤的噪声图符合高斯分布，这些方法在可编辑性方面表现出色，同时节省了计算资源。由于DDPM过程的不确定性，一些研究 [79], [80] 努力抵抗由于注入的随机性导致的重建能力下降。为了解决这一问题，DDPM反演 [80] 将 $z_t^{s,\prime}$ 和 $z_t^{s,*}$ 之间的差异编码为方程 (3) 中给出的 $F_{\text{bw}}^{DP}$ 的随机性项。由于随机性图（$\theta_F$）包含丰富的视觉信息，该方法在修改过程中可以忠实地再现源内容，同时提供较高的编辑灵活性。

## 5 编辑算法
作为所提出统一框架的剩余部分，编辑算法 $F_{\text{edit}}$ 旨在基于 $\Phi_I$ 和 $G$ 生成最终的编辑图像。方程 (17) 中所示的常规解决方案直接将 $\Phi_I$ 融入基础模型，并在 $G$ 的指导下执行采样过程 [12], [185]。这是早期工作中的一种常见做法 [41], [43], [44], [180]，尤其是在内容无关的编辑任务中 [40], [50]，如表2所示。然而，根据图3，$F_{\text{edit}}^{\text{Norm}}$ 缺乏**对最终图像的细粒度控制，难以同时实现内容一致性和语义保真度**。此外，一些模态如掩码和来自用户界面的数值输入，对于T2I模型来说难以处理。为了解决这些挑战，许多研究 [19], [32], [110], [116] 干预了普通的反向过程，如方程 (16) 中所述。我们将当前的方法分为四类：基于注意力的编辑、基于混合的编辑、基于得分的编辑和基于优化的编辑，分别表示为 $F_{\text{edit}}^{\text{Attn}}$、$F_{\text{edit}}^{\text{Blend}}$、$F_{\text{edit}}^{\text{Score}}$ 和 $F_{\text{edit}}^{\text{Optim}}$。这些方法的原理在图3的底部进行了说明。值得注意的是，用户可以结合使用这些方法，以利用它们的不同特点，从而获得更好的性能。在本节中，我们主要讨论这些思想的机制和特性，为各种编辑任务中的算法选择提供参考。

### 5.1 基于注意力的编辑
一大类方法 [16], [17], [32], [33] 通过操控注意力机制来实现对最终结果的更精细控制。形式上，$F_{\text{edit}}^{\text{Attn}}$ 的过程表示为：

$$
z_{t-1} = F_{\text{bw}}(z_t, t, C | \Phi_I , G)\{A_t, V_t \leftarrow \hat{A}_t, \hat{V}_t\}，(21)
$$
其中，大括号内的项表示将注意力图 $A_t$ 和值 $V_t$ 替换为更改后的 $\hat{A}_t$ 和 $\hat{V}_t$。我们在图5中展示了几项代表性工作，以说明它们在不同场景中的操控方法。由于交叉注意力和自注意力的性质不同，我们在下文中分别讨论它们。

#### 5.1.1 在交叉注意力机制中的操作
对于T2I模型 [13], [14]，交叉注意力机制使图像特征和文本嵌入之间能够进行通信，弥合自然语言与视觉内容之间的差距。具体来说，在交叉注意力中，注意力图表示文本token与图像区域的关联，而值则标识语义内容。接下来，我们介绍几种操作方案。

- **注入交叉注意力特征**。一组研究 [32], [81] 旨在将来自其他来源的交叉注意力图或值注入到反向过程中，以实现语义布局的细粒度可控性。Prompt-to-Prompt (P2P) [32] 使用来自重建路径的注意力图（$\hat{A}_t$）来操控编辑路径中的相应部分，以实现各种目的。图3和图5中展示了一些对象操作的示例。通过注入，同时保留编辑提示词中的值，该方法保持了源图像的基本结构。在P2P的基础上，Custom-Edit [81] 结合了基于微调 [54] 和基于前向反演 [66] 方法的思想，用示例图像中的参考对象替换原对象。具体来说，该方法采用了与P2P相同的注入策略，同时在目标提示词中使用个性化对象的标识符token来实现对象替换。Object-Shape Variation [82] 提出了提示词混合的方法，以调整对象的形状，同时保持其他元素不变。具体来说，它在中间去噪阶段注入来自另一个提示词的值（$\hat{V}_t$），该值负责修改与形状相关的语义。

- **调整注意力得分**。在交叉注意力图中，**激活幅度反映了每个文本token的影响程度**。一组方法 [32], [83] 利用这一特性来调整目标词在编辑图像中的影响。例如，P2P**重新加权对象token的注意力得分，以控制其存在性**。由于在无关区域中的高注意力得分通常会降低质量，FoI [83] 利用了这一思想来防止过度编辑问题。

#### 5.1.2 在自注意力机制中的操作
**自注意力在注意力图中捕捉空间相关性**，并增强属于相同语义的图像token之间的通信，从而加速完成整体概念的过程。类似于交叉注意力的注入方案，一些方法 [16], [84], [85], [86] 利用来自其他轨迹的注意力图或值，以更精细的方式进行编辑。对于对象替换，Plug-and-Play (PnP) [16] **使用来自重建路径的自注意力图（$\hat{A}_t$）替换原来的自注意力图**，以保持图像布局。Free-Prompt Editing (FPE) [84] 研究了自注意力在文本引导的图像编辑任务中的有效性，并通过注意力注入在多个场景中提升了性能。为了保留定制主体的细节，DreamMatcher [86] 使用来自参考路径的扭曲值（$\hat{V}_t$）在空间上对齐来自不同来源的自注意力图和值。

此外，其他方法 [17], [34], [87], [88] 局部操作自注意力图以细化空间对应关系。其中，TF-ICON [87] 合并了输入图像的自注意力图，以实现和谐的图像合成。HD-Painter [88] 调整注意力得分，以防止无关像素对修复区域的影响。类似地，对于分层编辑，DesignEdit [34] 通过在对象区域内约束激活值的方式，利用这一思想来获得干净的背景。

另一条研究路线 [17], [33], [89], [265] 是查询其他上下文以控制外观。如图5所示，MasaCtrl [17] 引入了互相关自注意力机制，以实现非刚性编辑，其中键和值来自重建路径。通过查询参考上下文，MasaCtrl 在保持对象外观的同时，遵循非刚性引导，如姿势或动作的变化。此外，由于普通DDIM反演空间的不完美重建 [66], [70]，TIC [89] 通过直接利用来自反演轨迹的特征来提升性能。受互相关计算的启发，文献 [33] 提出了跨图像注意力，用于在具有相似语义的主体（如斑马和马）之间进行外观转移，其中键和值由外观图像计算。同样，其他研究 [90], [91] 利用这一思想进行风格转换，并从风格图像中获取键和值。

### 5.2 基于混合的编辑
许多方法 [19], [87], [92], [98], [103] 在同一空间中表示源图像和编辑目标，同时寻找最佳的中间状态，以同时保留所需内容并反映编辑目的。$F_{\text{edit}}^{\text{Blend}}$ 的形式化表达为：

$$
z_{t-1} = F_{\text{bw}}(z_t, t, C | \Phi_I , G)\{f_t \leftarrow \alpha f_{t}^{\text{src}} + \beta f_{t}^{\text{tgt}}\}，(22)
$$
其中，大括号内的项表示使用相应的混合因子 $\alpha$ 和 $\beta$ 将 $f_t^{\text{src}}$ 和 $f_t^{\text{tgt}}$ 进行融合。$f_t$ 是方法中指定的混合特征 [19], [92] 或参数 [103]。在本节中，我们根据混合空间对方法进行了分类，并在图6中展示了几项代表性工作 [92], [101]

#### 5.2.1 在空间层级的混合
在**空间层级进行混合**是将视觉元素合成为单个图像的常见做法，它将**源图像和生成图像的内容组合用于局部编辑**。基于多步骤的反向过程，一组工作 [93], [99], [100] 探索了混合扩散特征以获得和谐的结果。

- **在噪声潜变量空间中的混合**。根据文献 [16], [17]，噪声潜变量编码了生成图像的空间特性，如外观和结构。在这种情况下，**$f_t^{\text{src}}$ 和 $f_t^{\text{tgt}}$ 分别代表来自源图像和生成图像的中间噪声变量**。一些方法 [87], [92], [96] 使用用户提供的掩码 [92], [93] 或估计的掩码 [32], [82], [95], [96], [98] 进行混合。对于图像修复任务，用户需要提供掩码条件以指示感兴趣的区域。借鉴之前工作的思路 [246]，混合潜变量扩散 (BLD) [92] 在编辑路径和源图像的DDPM前向轨迹中混合噪声潜变量，如图6顶部所示。此外，高分辨率混合扩散 [93] 通过超分辨率模型进一步提高了质量。同时，类似于RePaint [247]，它在每个去噪步骤中迭代地应用混合过程和方程 (1)，从而获得更加和谐一致的结果。此外，差异扩散 [94] 允许用户提供变化图，以表示每个位置的编辑强度，从而实现平滑的图像修复。具体来说，该方法不是使用固定掩码，而是根据当前步骤和控制图计算动态混合因子，调整融合的平滑度以获得更高的连贯性。DreamEdit [206] 利用这一思想将定制化主体填充到指定区域中。

最近，一些方法 [34], [95] 混合来自多个轨迹的噪声潜变量，以组合不同的内容。由于每个目标的最佳反演步骤不同，OIR [95] 相应地构建了多个编辑路径并将它们融合，以在最终图像中整合它们的效果。

其他方法 [17], [32], [82], [96], [98] 利用这一思想防止方法修改与语义无关的内容。其中一些工作 [17], [32] 使用交叉注意力图来估计编辑区域。另一条研究路线 [71], [96], [97] 基于噪声图的差异进行预测，其中低于某个阈值的元素代表要保留的区域。为了避免在对象替换中的过度编辑问题，DiffEdit [96] 计算由源提示词和目标提示词分别引导的估计噪声的差异，以推断混合图。由于背景像素在噪声值方面的变化较小，该图粗略地指示了编辑区域。不同的是，其他方法 [71], [97] 计算条件项和无条件项之间的差异以确定融合因子。

- **在层级特征空间中的混合**。另一组方法 [17], [82], [86], [87], [99], [100] 在层级特征空间中进行混合，以获得更无缝且连贯的结果。PFB-Diff [99] 引入了混合模块，以融合来自特定transformer块的特征进行对象替换。其他一些方法 [17], [82] 融合了自注意力特征进行局部修改。例如，MasaCtrl [17] 混合了编辑路径和重建路径的特征，以保留背景内容。对于零样本定制，Tuning-Free Image Customization [100] 使用时间变化的混合因子混合来自不同轨迹的自注意力模块的输出，以结合场景图像和个性化主体。

#### 5.2.2 在语义空间中的混合
另一条研究路线 [19], [67], [101], [103] 将 $f_t^{\text{src}}$ 和 $f_t^{\text{tgt}}$ 表示在语义空间中，以实现高度灵活的编辑。

- **在文本空间中的混合**。由于文本嵌入包含丰富的信息，一些方法 [19], [67], [101] 将源图像编码到文本空间，并将其与目标嵌入混合。对于非刚性编辑，Imagic [19] 首先通过微调目标提示词的嵌入在有限步数内反转输入。这防止了目标嵌入偏离其原始值，从而保持线性特性。然后，该方法优化骨干网络中的参数以提高一致性，这是在第一步中未能满足的。在编辑时，Imagic 插值更新后的目标嵌入及其原始值以获得最终结果。相比之下，Forgedit [101] 为提高训练效率，联合优化了源嵌入和基础模型。如图6所示，通过合并源嵌入和目标嵌入，Forgedit 在内容一致性和语义保真度之间表现出良好的平衡。借鉴NTI [66] 的思路，PTI [67] 优化源嵌入以修正不完美的DDIM反演空间 [66]，并在编辑时与目标嵌入进行插值。从另一个角度来看，由于去噪阶段负责不同的视觉内容 [32], [47], [82]，文献 [102] 通过在编辑图像上应用语义和内容损失 [251], [266]，在每一步中寻找最佳混合因子，平衡保留和修改内容的比例。

- **在参数空间中的混合**。少数研究 [101], [103] 在参数空间中进行操作。为了减轻在单个图像上微调引起的过拟合问题，Forgedit 将部分模块替换为原始模型中的对应模块，以保留生成能力。受反事实推理框架的启发，双重反事实推理 (DAC) [103] 优化了 LoRA [204] 模块以编码源图像和编辑目标。通过调整权重因子，DAC 在修改和保留源内容之间取得平衡。

### 5.3 基于得分的编辑
对于图像生成，条件输入赋予模型对生成内容的可控性 [186], [187]。从基于得分的角度来看，在独立性假设下，条件得分函数可以扩展为以下方程：

$$
\begin{aligned}
\triangledown_{z_t} \log p_t(z_t | C_1, C_2, ..., C_{N_c}) &\propto \triangledown_{z_t} \log p_t(z_t) + \sum_{i=1}^{N_c} \triangledown_{z_t} \log p_t(C_i | z_t),(23)
\end{aligned}
$$
其中，$N_c$ 是条件的数量，$\log p_t(C_i | z_t)$ 可以通过分类器引导 [187] 或无分类器引导 [186] 来建模。基于得分的观点启发了许多研究 [104], [107], [110] 从辅助分布中收集信息，为不同的编辑目标提供定向指导。$F_{\text{edit}}^{\text{Score}}$ 的形式化表达为：

$$
z_{t-1} = F_{\text{bw}}(z_t, t, C | \Phi_I , G)\{\epsilon_\theta \leftarrow -\sqrt{1 - \bar{\alpha}_t}\left(\triangledown_{z_t} \log p_t(z_t) + \sum_{i=1}^{N_c} \omega_i \triangledown_{z_t} \log p_t(C_i | z_t)\right)\},
$$

其中，大括号内的项表示通过聚合多个条件的效果来计算噪声图，并通过方程 (11) 将得分转换为噪声。$\omega_i$ 是引导系数。我们根据 $\triangledown_{z_t} \log p_t(C_i | z_t)$ 的来源对方法进行了分类，并在图7中展示了几项代表性工作。

- **多噪声引导**。一组工作 [30], [104], [105], [106] 通过聚合来自不同文本提示的多个噪声估计来实现多目标编辑，其中 $\triangledown_{z_t} \log p_t(C_i | z_t)$ 是通过方程 (11) 和贝叶斯规则计算的 $\epsilon_\theta(z_t, t, C_i) - \epsilon_\theta(z_t, t, \emptyset)$。对于每个编辑目标，SEGA [104] 及其后续工作 [30], [105] 计算条件项，以在源图像中注入或移除指定的概念。该过程在图7的顶部进行了演示。此外，由于图像块对每个编辑目标的响应不同，这些方法相应地计算了块级引导系数。通过多噪声引导，它们在单一反向过程中实现了多个目标。
- **多专家引导**。由于在不同数据集上微调的扩散模型具有不同的领域知识，一些方法 [52], [107] 旨在继承多个专家的生成能力。为了避免在单个图像上微调时的过拟合问题，SINE [107] 使用了两个生成模型。如图7中间所示，它首先执行基于微调的算法 [38] 来反演源图像，然后整合从微调模型和原始模型估计的条件项，以同时实现一致性和语义保真度。同样，为了缓解定制化中的语言漂移和灾难性遗忘问题，DCO [52] 引入了奖励引导，以结合基础模型和定制专家的噪声图。
- **能量函数引导**。借鉴能量模型 [11] 的思想，另一条研究路线 [88], [108], [109], [111], [112], [113] 通过建立的能量函数模拟 $\log p_t(C_i | z_t)$ 并引导编辑方向以降低系统能量。例如，NMG [108] 设计了能量函数，作为反演和编辑轨迹中噪声潜变量的差异，以防止过度编辑问题。受益于扩散特征中编码的结构和外观信息，几项研究 [109], [110], [111], [112] 旨在提供空间引导。对于空间变换，Self-Guidance [109] 精心设计了每个场景的能量函数。图7底部展示了一个平移的示例。具体来说，该方法通过基于交叉注意力图和局部特征（分别负责位置和外观）对齐变换前后的对象特征来最小化能量。同样，DragonDiffusion [110] 最大化了变换相关区域内图像特征的余弦相似性。此外，MagicRemover [111] 在对象移除任务中改进了这一思路。通过约束交叉注意力图的激活，它从源图像中抹除了目标。FreeControl [112] 在图像转换中利用了这一思想。它为结构和外观引导构建了能量函数，以在生成更精细细节的同时保持源图像的布局。























